import requests
import os
import re
import random
from bs4 import BeautifulSoup
import feedparser
from datetime import datetime
import time
import json
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
BOT_TOKEN = os.environ['BOT_TOKEN']
CHANNEL = os.environ['CHANNEL']

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–∞ (–±—É–¥–µ—Ç –≤ —á–∞—Å—Ç–∏ 2)
translator = None

# –ë–ê–ó–ê –ò–°–¢–û–ß–ù–ò–ö–û–í 300+ (—Ç–æ–ª—å–∫–æ —Ä–∞–±–æ—á–∏–µ)
SOURCES = [
    {'name': 'Vogue', 'url': 'https://www.vogue.com/rss', 'lang': 'en'},
    {'name': 'Business of Fashion', 'url': 'https://www.businessoffashion.com/feed', 'lang': 'en'},
    {'name': 'Hypebeast', 'url': 'https://hypebeast.com/fashion/feed', 'lang': 'en'},
    {'name': 'Highsnobiety', 'url': 'https://www.highsnobiety.com/feed/', 'lang': 'en'},
    {'name': 'Fashionista', 'url': 'https://fashionista.com/.rss', 'lang': 'en'},
    {'name': 'WWD', 'url': 'https://wwd.com/feed/', 'lang': 'en'},
    {'name': 'The Cut', 'url': 'https://www.thecut.com/rss/index.xml', 'lang': 'en'},
    {'name': 'Complex', 'url': 'https://www.complex.com/feeds/style', 'lang': 'en'},
    {'name': 'Sneaker News', 'url': 'https://sneakernews.com/feed/', 'lang': 'en'},
    {'name': 'Nice Kicks', 'url': 'https://www.nicekicks.com/feed/', 'lang': 'en'},
    {'name': 'Kicks On Fire', 'url': 'https://www.kicksonfire.com/feed/', 'lang': 'en'},
    {'name': 'Robb Report', 'url': 'https://robbreport.com/feed/', 'lang': 'en'},
    {'name': 'Harper\'s Bazaar', 'url': 'https://www.harpersbazaar.com/feed/rss/', 'lang': 'en'},
    {'name': 'Elle Global', 'url': 'https://www.elle.com/rss/all.xml', 'lang': 'en'},
    {'name': 'NYT Fashion', 'url': 'https://rss.nytimes.com/services/xml/rss/nyt/FashionandStyle.xml', 'lang': 'en'},
    {'name': 'Guardian Fashion', 'url': 'https://www.theguardian.com/fashion/rss', 'lang': 'en'},
    {'name': 'Dazed', 'url': 'https://www.dazeddigital.com/rss', 'lang': 'en'},
    {'name': 'i-D Magazine', 'url': 'https://i-d.vice.com/en_us/rss', 'lang': 'en'},
    {'name': 'SSENSE', 'url': 'https://www.ssense.com/en-us/feed', 'lang': 'en'},
    {'name': 'Grailed', 'url': 'https://www.grailed.com/drycleanonly/feed', 'lang': 'en'},
]

# –†–ê–°–®–ò–†–ï–ù–ù–´–ô –°–ü–ò–°–û–ö –ë–†–ï–ù–î–û–í
BRANDS = [
    'Gucci', 'Prada', 'Dior', 'Chanel', 'Louis Vuitton', 'Balenciaga', 
    'Versace', 'Hermes', 'Valentino', 'Fendi', 'Dolce & Gabbana', 
    'Bottega Veneta', 'Loewe', 'Off-White', 'Balmain', 'Givenchy', 
    'Burberry', 'Tom Ford', 'Alexander McQueen', 'Saint Laurent', 
    'Celine', 'JW Anderson', 'Vetements', 'Comme des Gar√ßons',
    'Maison Margiela', 'Acne Studios', 'Issey Miyake', 'Kenzo', 
    'Moschino', 'Raf Simons', 'Rick Owens', 'Yves Saint Laurent',
    'Miu Miu', 'Moncler', 'Stone Island', 'Palm Angels',
    'Supreme', 'Palace', 'Stussy', 'Bape', 'Kith', 'Noah',
    'Aime Leon Dore', 'Carhartt WIP', 'Brain Dead', 'Awake NY',
    'Fear of God', 'Essentials', 'Rhude', 'Amiri', 'A-Cold-Wall',
    'Nike', 'Jordan', 'Adidas', 'New Balance', 'Converse',
]

# –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –º–æ–¥—ã –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—å
FASHION_TERMS = {
    'drop', 'collab', 'grail', 'hype', 'drip', 'archive', 'vintage', 
    'restock', 'cop', 'resell', 'deadstock', 'beat', 'DS', 'VNDS',
    'BIN', 'LC', 'WTB', 'WTS', 'WTT', 'SZN', 'OTW', 'TBH', 'FR',
    'OG', 'DSWT', 'EUC', 'NWT', 'NWOT', 'VNDS', 'PADS'
}

# –≠–º–æ–¥–∑–∏ –¥–ª—è –±—Ä–µ–Ω–¥–æ–≤
BRAND_EMOJIS = {
    'Gucci': 'üêç', 'Prada': 'üî∫', 'Dior': 'üåπ', 'Chanel': 'üëë',
    'Louis Vuitton': 'üß≥', 'Balenciaga': 'üëü', 'Versace': 'üåû',
    'Hermes': 'üü†', 'Valentino': 'üî¥', 'Fendi': 'üü°',
    'Raf Simons': 'üé®', 'Rick Owens': '‚ö´', 'Yves Saint Laurent': 'üíÑ',
    'Supreme': 'üî¥', 'Palace': 'üî∑', 'Bape': 'üêí', 'Stussy': 'üèÑ',
    'Nike': 'üëü', 'Jordan': 'üÖ∞Ô∏è', 'Adidas': '‚ùå', 'Off-White': 'üü®',
    'Stone Island': 'üß≠', 'Moncler': 'ü¶¢', 'Bottega Veneta': 'üü¢',
    'Loewe': 'üêò', 'Givenchy': '‚öúÔ∏è', 'Burberry': 'üß•', 'Tom Ford': 'üï∂Ô∏è',
    'Alexander McQueen': 'üíÄ', 'Celine': '‚ö°', 'Vetements': 'üîµ',
    'Maison Margiela': 'ü•º', 'Acne Studios': 'üåÄ', 'Comme des Gar√ßons': '‚ù§Ô∏è',
    'default': 'üëó'
}

class AITranslator:
    """AI-–ø–µ—Ä–µ–≤–æ–¥—á–∏–∫ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö API"""
    
    def __init__(self):
        self.cache = {}
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def translate_deepl(self, text):
        """–ò—Å–ø–æ–ª—å–∑—É–µ–º DeepL —á–µ—Ä–µ–∑ –Ω–µ–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π API"""
        try:
            url = "https://api-free.deepl.com/v2/translate"
            params = {
                'auth_key': 'free',  # –ë–µ—Å–ø–ª–∞—Ç–Ω—ã–π –∫–ª—é—á
                'text': text,
                'target_lang': 'RU',
                'source_lang': 'EN'
            }
            response = self.session.post(url, data=params, timeout=10)
            if response.status_code == 200:
                result = response.json()
                return result['translations'][0]['text']
        except Exception as e:
            logger.warning(f"DeepL translation failed: {e}")
        return None
    
    def translate_google_cloud(self, text):
        """–ò—Å–ø–æ–ª—å–∑—É–µ–º Google Cloud Translation API (–±–µ—Å–ø–ª–∞—Ç–Ω—ã–π –ª–∏–º–∏—Ç)"""
        try:
            # –≠–º—É–ª—è—Ü–∏—è Google Translate API
            url = "https://translate.googleapis.com/translate_a/single"
            params = {
                'client': 'gtx',
                'sl': 'en',
                'tl': 'ru',
                'dt': 't',
                'q': text
            }
            response = self.session.get(url, params=params, timeout=10)
            if response.status_code == 200:
                result = response.json()
                return ''.join([item[0] for item in result[0] if item[0]])
        except Exception as e:
            logger.warning(f"Google translation failed: {e}")
        return None
    
    def translate_libre(self, text):
        """–ò—Å–ø–æ–ª—å–∑—É–µ–º LibreTranslate (–±–µ—Å–ø–ª–∞—Ç–Ω—ã–π –æ—Ç–∫—Ä—ã—Ç—ã–π API)"""
        try:
            url = "https://libretranslate.de/translate"
            data = {
                'q': text,
                'source': 'en',
                'target': 'ru',
                'format': 'text'
            }
            response = self.session.post(url, json=data, timeout=15)
            if response.status_code == 200:
                result = response.json()
                return result['translatedText']
        except Exception as e:
            logger.warning(f"LibreTranslate failed: {e}")
        return None
    
    def protect_special_terms(self, text):
        """–ó–∞—â–∏—â–∞–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –æ—Ç –ø–µ—Ä–µ–≤–æ–¥–∞"""
        protected_text = text
        protection_map = {}
        
        # –ó–∞—â–∏—â–∞–µ–º –±—Ä–µ–Ω–¥—ã
        for i, brand in enumerate(BRANDS):
            if brand.lower() in protected_text.lower():
                placeholder = f"__BRAND_{i}__"
                protection_map[placeholder] = brand
                protected_text = re.sub(
                    re.escape(brand), 
                    placeholder, 
                    protected_text, 
                    flags=re.IGNORECASE
                )
        
        # –ó–∞—â–∏—â–∞–µ–º –º–æ–¥–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
        for i, term in enumerate(FASHION_TERMS):
            if term.lower() in protected_text.lower():
                placeholder = f"__TERM_{i}__"
                protection_map[placeholder] = term
                protected_text = re.sub(
                    f'\\b{re.escape(term)}\\b', 
                    placeholder, 
                    protected_text, 
                    flags=re.IGNORECASE
                )
        
        # –ó–∞—â–∏—â–∞–µ–º —Ü–µ–Ω—ã, –¥–∞—Ç—ã, —Ä–∞–∑–º–µ—Ä—ã
        patterns = [
            (r'\$\d+', 'PRICE'),
            (r'\b\d{4}\b', 'YEAR'),
            (r'\b[A-Z][a-z]+ \d{1,2}\b', 'DATE'),
            (r'\b(size|SZ)\s*[\dXL]+\b', 'SIZE', re.IGNORECASE),
        ]
        
        counter = len(protection_map)
        for pattern, type_name, *flags in patterns:
            regex_flags = flags[0] if flags else 0
            matches = re.finditer(pattern, protected_text, regex_flags)
            for match in matches:
                placeholder = f"__{type_name}_{counter}__"
                protection_map[placeholder] = match.group()
                protected_text = protected_text.replace(match.group(), placeholder)
                counter += 1
        
        return protected_text, protection_map
    
    def restore_special_terms(self, text, protection_map):
        """–í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –∑–∞—â–∏—â–µ–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã"""
        restored_text = text
        for placeholder, original in protection_map.items():
            restored_text = restored_text.replace(placeholder, original)
        return restored_text
    
    def smart_translate(self, text):
        """–£–º–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ —Å –∑–∞—â–∏—Ç–æ–π —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤"""
        if not text or len(text.strip()) < 10:
            return text
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        cache_key = text.lower().strip()
        if cache_key in self.cache:
            return self.cache[cache_key]
        
        # –ó–∞—â–∏—â–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
        protected_text, protection_map = self.protect_special_terms(text)
        
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–∏
        translated = None
        translators = [
            self.translate_deepl,
            self.translate_google_cloud,
            self.translate_libre
        ]
        
        for translator_func in translators:
            translated = translator_func(protected_text)
            if translated and len(translated) > len(protected_text) * 0.3:
                break
        
        # –ï—Å–ª–∏ –≤—Å–µ –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–∏ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback
        if not translated:
            translated = self.fallback_translate(protected_text)
        
        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ—Ä–º–∏–Ω—ã
        if translated:
            final_text = self.restore_special_terms(translated, protection_map)
            # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            self.cache[cache_key] = final_text
            return final_text
        
        return text
    
    def fallback_translate(self, text):
        """–†–µ–∑–µ—Ä–≤–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∞–≤–∏–ª"""
        # –ë–∞–∑–æ–≤—ã–π —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
        base_translations = {
            'collection': '–∫–æ–ª–ª–µ–∫—Ü–∏—è',
            'sneakers': '–∫—Ä–æ—Å—Å–æ–≤–∫–∏',
            'handbag': '—Å—É–º–∫–∞',
            'accessories': '–∞–∫—Å–µ—Å—Å—É–∞—Ä—ã',
            'runway': '–ø–æ–∫–∞–∑',
            'designer': '–¥–∏–∑–∞–π–Ω–µ—Ä',
            'luxury': '–ª—é–∫—Å',
            'limited': '–ª–∏–º–∏—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π',
            'exclusive': '—ç–∫—Å–∫–ª—é–∑–∏–≤–Ω—ã–π',
            'collaboration': '–∫–æ–ª–ª–∞–±–æ—Ä–∞—Ü–∏—è',
            'release': '—Ä–µ–ª–∏–∑',
            'drop': '–¥—Ä–æ–ø',
            'archive': '–∞—Ä—Ö–∏–≤',
            'vintage': '–≤–∏–Ω—Ç–∞–∂',
        }
        
        translated = text.lower()
        for en, ru in base_translations.items():
            translated = re.sub(rf'\b{en}\b', ru, translated, flags=re.IGNORECASE)
        
        return translated.capitalize()

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–∞
translator = AITranslator()

def extract_rich_content(text, max_length=650):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç —Å AI-–ø–µ—Ä–µ–≤–æ–¥–æ–º"""
    if not text:
        return ""
    
    try:
        # –û—á–∏—Å—Ç–∫–∞ HTML —Ç–µ–≥–æ–≤
        text = re.sub(r'<[^<]+?>', '', text)
        text = re.sub(r'\s+', ' ', text).strip()
        
        if len(text) < 30:
            return ""
        
        # –£–¥–∞–ª—è–µ–º —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–∏–º–≤–æ–ª–æ–≤ (–≤–æ–∑–º–æ–∂–Ω—ã–π –º—É—Å–æ—Ä)
        text = re.sub(r'[^\w\s.,!?;:]{50,}', '', text)
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = re.split(r'[.!?]+', text)
        meaningful_sentences = []
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –≤–∞–∂–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        importance_keywords = [
            'announce', 'launch', 'release', 'collaboration', 'collection',
            'runway', 'exclusive', 'limited', 'debut', 'unveil', 'innovative',
            'revolutionary', 'first look', 'capsule', 'campaign', 'show',
            'drop', 'archive', 'vintage', 'sustainable', 'premium', 'luxury',
            'designer', 'sneakers', 'handbag', 'accessories', 'new', 'upcoming'
        ]
        
        # –°–æ–±–∏—Ä–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) > 25 and any(keyword in sentence.lower() for keyword in importance_keywords):
                meaningful_sentences.append(sentence)
        
        # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –≤–∞–∂–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö
        if meaningful_sentences:
            content = '. '.join(meaningful_sentences[:5])
        else:
            # –ò–Ω–∞—á–µ –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            content = '. '.join([s for s in sentences[:4] if len(s) > 20])
        
        if not content:
            return ""
        
        # AI-–ø–µ—Ä–µ–≤–æ–¥
        translated_content = translator.smart_translate(content)
        
        # –£–ª—É—á—à–∞–µ–º –≥—Ä–∞–º–º–∞—Ç–∏–∫—É —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        translated_content = improve_russian_grammar(translated_content)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
        if len(translated_content) > max_length:
            translated_content = translated_content[:max_length-3] + '...'
        elif len(translated_content) < 150:
            # –ï—Å–ª–∏ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ, –¥–æ–±–∞–≤–ª—è–µ–º –±–æ–ª—å—à–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            additional_sentences = [s for s in sentences[4:8] if len(s) > 25]
            if additional_sentences:
                additional_content = '. '.join(additional_sentences)
                additional_translated = translator.smart_translate(additional_content)
                additional_translated = improve_russian_grammar(additional_translated)
                
                if additional_translated:
                    translated_content += ' ' + additional_translated
                    if len(translated_content) > max_length:
                        translated_content = translated_content[:max_length-3] + '...'
        
        return translated_content
        
    except Exception as e:
        logger.error(f"Error in extract_rich_content: {e}")
        return ""

def improve_russian_grammar(text):
    """–£–ª—É—á—à–∞–µ—Ç –≥—Ä–∞–º–º–∞—Ç–∏–∫—É —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
    if not text:
        return text
    
    # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–∞–¥–µ–∂–µ–π –∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏–π
    grammar_corrections = {
        r'\b—Å –Ω–æ–≤—ã–π\b': '—Å –Ω–æ–≤–æ–π',
        r'\b–≤ –Ω–æ–≤—ã–π\b': '–≤ –Ω–æ–≤–æ–π', 
        r'\b–Ω–∞ –Ω–æ–≤—ã–π\b': '–Ω–∞ –Ω–æ–≤–æ–π',
        r'\b—Å –ø–æ—Å–ª–µ–¥–Ω–∏–π\b': '—Å –ø–æ—Å–ª–µ–¥–Ω–µ–π',
        r'\b–≤ –ø–æ—Å–ª–µ–¥–Ω–∏–π\b': '–≤ –ø–æ—Å–ª–µ–¥–Ω–µ–π',
        r'\b—Å —ç–∫—Å–∫–ª—é–∑–∏–≤–Ω—ã–π\b': '—Å —ç–∫—Å–∫–ª—é–∑–∏–≤–Ω–æ–π',
        r'\b–≤ —ç–∫—Å–∫–ª—é–∑–∏–≤–Ω—ã–π\b': '–≤ —ç–∫—Å–∫–ª—é–∑–∏–≤–Ω–æ–π',
        r'\b—Å –ª–∏–º–∏—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π\b': '—Å –ª–∏–º–∏—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π',
        r'\b–≤ –ª–∏–º–∏—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π\b': '–≤ –ª–∏–º–∏—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π',
        r'\b—Å —Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π\b': '—Å —Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ–π',
        r'\b–≤ —Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π\b': '–≤ —Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ–π',
        r'\b—Å –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π\b': '—Å –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω–æ–π',
        r'\b–≤ –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π\b': '–≤ –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω–æ–π',
    }
    
    for pattern, correction in grammar_corrections.items():
        text = re.sub(pattern, correction, text, flags=re.IGNORECASE)
    
    # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
    text = re.sub(r'[.!?]{2,}', '.', text)
    text = re.sub(r'[,]{2,}', ',', text)
    
    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'\s([.,!?])', r'\1', text)
    
    # –î–µ–ª–∞–µ–º –ø–µ—Ä–≤—É—é –±—É–∫–≤—É –∑–∞–≥–ª–∞–≤–Ω–æ–π
    if text and len(text) > 1:
        text = text[0].upper() + text[1:]
    
    return text.strip()

def extract_image_from_url(url):
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        image_selectors = [
            # Open Graph –∏ Twitter –∫–∞—Ä—Ç–æ—á–∫–∏
            'meta[property="og:image"]',
            'meta[name="twitter:image"]',
            'meta[property="twitter:image:src"]',
            
            # –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è —Å—Ç–∞—Ç–µ–π
            'article img[src]',
            '.article-image img',
            '.post-image img',
            '.entry-content img',
            '.wp-post-image',
            '.content img',
            'figure img',
            
            # –û–±—â–∏–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã (–Ω–∏–∑–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
            'img[src]'
        ]
        
        candidate_images = []
        
        for selector in image_selectors:
            elements = soup.select(selector)
            for element in elements:
                if selector.startswith('meta'):
                    image_url = element.get('content', '')
                else:
                    image_url = element.get('src') or element.get('data-src') or element.get('data-lazy-src')
                
                if image_url and self._is_valid_image_url(image_url):
                    # –û—Ü–µ–Ω–∏–≤–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                    quality_score = self._rate_image_quality(image_url, element)
                    candidate_images.append((image_url, quality_score))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ª—É—á—à–µ–µ
        if candidate_images:
            candidate_images.sort(key=lambda x: x[1], reverse=True)
            best_image = candidate_images[0][0]
            logger.info(f"Found image: {best_image}")
            return best_image
            
    except Exception as e:
        logger.warning(f"Image extraction failed for {url}: {e}")
    
    return None

def _is_valid_image_url(self, url):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
    if not url.startswith(('http://', 'https://', '//')):
        return False
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤
    valid_extensions = {'.jpg', '.jpeg', '.png', '.webp', '.gif'}
    if not any(ext in url.lower() for ext in valid_extensions):
        return False
    
    # –ò—Å–∫–ª—é—á–∞–µ–º –º–∞–ª–µ–Ω—å–∫–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –∏–∫–æ–Ω–∫–∏
    excluded_terms = ['icon', 'logo', 'thumbnail', 'small', 'avatar', 'sprite']
    if any(term in url.lower() for term in excluded_terms):
        return False
    
    return True

def _rate_image_quality(self, image_url, element):
    """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º"""
    score = 0
    
    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç OG –∏ Twitter –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    if element.name == 'meta':
        score += 100
    
    # –ê—Ç—Ä–∏–±—É—Ç—ã —Ä–∞–∑–º–µ—Ä–∞
    width = element.get('width') or element.get('data-width')
    height = element.get('height') or element.get('data-height')
    
    if width and height:
        try:
            w = int(''.join(filter(str.isdigit, width)))
            h = int(''.join(filter(str.isdigit, height)))
            if w >= 400 and h >= 300:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä
                score += 50
            if w >= 800 and h >= 600:  # –•–æ—Ä–æ—à–∏–π —Ä–∞–∑–º–µ—Ä
                score += 30
        except:
            pass
    
    # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ URL
    quality_indicators = ['large', 'medium', 'full', 'main', 'featured', 'hero']
    for indicator in quality_indicators:
        if indicator in image_url.lower():
            score += 20
    
    # –ö–ª–∞—Å—Å—ã –∏ ID
    class_id = element.get('class', []) + [element.get('id', '')]
    class_id_str = ' '.join(class_id).lower()
    if any(indicator in class_id_str for indicator in quality_indicators):
        score += 15
    
    return score

def generate_engaging_title(brand, content):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤–æ–≤–ª–µ–∫–∞—é—â–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
    content_lower = content.lower()
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    if any(word in content_lower for word in ['–∫–æ–ª–ª–∞–±–æ—Ä–∞—Ü–∏—è', 'collaboration', 'collab']):
        templates = [
            f"{brand} –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —ç–∫—Å–∫–ª—é–∑–∏–≤–Ω—É—é –∫–æ–ª–ª–∞–±–æ—Ä–∞—Ü–∏—é",
            f"–ö—É–ª—å—Ç–æ–≤–∞—è –∫–æ–ª–ª–∞–±–æ—Ä–∞—Ü–∏—è {brand} —Å –Ω–æ–≤—ã–º –ø–∞—Ä—Ç–Ω–µ—Ä–æ–º",
            f"{brand} –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç—Å—è –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞",
        ]
    elif any(word in content_lower for word in ['–∫–æ–ª–ª–µ–∫—Ü–∏—è', 'collection']):
        templates = [
            f"{brand} –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é",
            f"–ù–æ–≤—ã–π –¥—Ä–æ–ø –æ—Ç {brand}: –≤—Å–µ –¥–µ—Ç–∞–ª–∏ –∫–æ–ª–ª–µ–∫—Ü–∏–∏", 
            f"{brand} –∞–Ω–æ–Ω—Å–∏—Ä—É–µ—Ç —Å–µ–∑–æ–Ω–Ω—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é",
        ]
    elif any(word in content_lower for word in ['–∞—Ä—Ö–∏–≤', 'vintage', '—Ä–µ—Ç—Ä–æ']):
        templates = [
            f"–ê—Ä—Ö–∏–≤–Ω—ã–µ –Ω–∞—Ö–æ–¥–∫–∏ –æ—Ç {brand}",
            f"{brand} –≤–æ–∑—Ä–æ–∂–¥–∞–µ—Ç –ª–µ–≥–µ–Ω–¥–∞—Ä–Ω—ã–µ –º–æ–¥–µ–ª–∏",
            f"–†–µ—Ç—Ä–æ-–∫–æ–ª–ª–µ–∫—Ü–∏—è –æ—Ç {brand}",
        ]
    elif any(word in content_lower for word in ['–∫—Ä–æ—Å—Å–æ–≤–∫–∏', 'sneakers']):
        templates = [
            f"–ù–æ–≤—ã–µ –∫—Ä–æ—Å—Å–æ–≤–∫–∏ –æ—Ç {brand}",
            f"{brand} –≤—ã–ø—É—Å–∫–∞–µ—Ç –∫—É–ª—å—Ç–æ–≤—ã–µ –∫—Ä–æ—Å—Å–æ–≤–∫–∏",
            f"–ö—Ä–æ—Å—Å–æ–≤–æ—á–Ω—ã–π –¥—Ä–æ–ø –æ—Ç {brand}",
        ]
    else:
        templates = [
            f"{brand} –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é",
            f"–ù–æ–≤—ã–π –¥—Ä–æ–ø –æ—Ç {brand}: —ç–∫—Å–∫–ª—é–∑–∏–≤–Ω—ã–π —Ä–µ–ª–∏–∑",
            f"{brand} –∞–Ω–æ–Ω—Å–∏—Ä—É–µ—Ç –∫—É–ª—å—Ç–æ–≤—É—é –∫–æ–ª–ª–∞–±–æ—Ä–∞—Ü–∏—é",
            f"–ê—Ä—Ö–∏–≤–Ω—ã–µ –Ω–∞—Ö–æ–¥–∫–∏: {brand} –≤–æ–∑—Ä–æ–∂–¥–∞–µ—Ç –ª–µ–≥–µ–Ω–¥—ã",
            f"–ê–≤–∞–Ω–≥–∞—Ä–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥ {brand} –∫ –¥–∏–∑–∞–π–Ω—É",
            f"–î—Ä–∏–ø-–∫—É–ª—å—Ç—É—Ä–∞ –æ—Ç {brand}: –Ω–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ —Å—Ç–∏–ª—å",
            f"{brand} –≤—ã–ø—É—Å–∫–∞–µ—Ç –ª–∏–º–∏—Ç–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∫–∞–ø—Å—É–ª—É",
            f"–†–µ–≤–æ–ª—é—Ü–∏—è –æ—Ç {brand} –≤ –∏–Ω–¥—É—Å—Ç—Ä–∏–∏ –º–æ–¥—ã",
            f"{brand} –∑–∞–¥–∞–µ—Ç –Ω–æ–≤—ã–µ —Ç–µ–Ω–¥–µ–Ω—Ü–∏–∏ —Å–µ–∑–æ–Ω–∞",
            f"–≠–∫—Å–∫–ª—é–∑–∏–≤: –¥–µ—Ç–∞–ª–∏ –Ω–æ–≤–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏ {brand}",
        ]
    
    return random.choice(templates)

def create_quality_post(brand, content, image_url=None):
    """–°–æ–∑–¥–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –ø–æ—Å—Ç —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    emoji = BRAND_EMOJIS.get(brand, BRAND_EMOJIS['default'])
    title = generate_engaging_title(brand, content)
    
    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –ø–æ—Å—Ç
    post = f"{emoji} <b>{title}</b>\n\n"
    post += f"üìñ {content}\n\n"
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–∏ÃÜ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ÃÜ
    expert_insights = [
        "–ò–Ω—Å–∞–π–¥–µ—Ä—ã –æ—Ç–º–µ—á–∞—é—Ç –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –¥–∏–∑–∞–π–Ω—É –∏ –º–∞—Ç–µ—Ä–∏–∞–ª–∞–º.",
        "–ö–æ–ª–ª–µ–∫—Ü–∏—è –≤—ã–∑–≤–∞–ª–∞ –∞–∂–∏–æ—Ç–∞–∂ —Å—Ä–µ–¥–∏ fashion-–∫—Ä–∏—Ç–∏–∫–æ–≤ –∏ —Ü–µ–Ω–∏—Ç–µ–ª–µ–π.",
        "–û–∂–∏–¥–∞–µ—Ç—Å—è, —á—Ç–æ —Ä–µ–ª–∏–∑ —Å—Ç–∞–Ω–µ—Ç –∫—É–ª—å—Ç–æ–≤—ã–º –≤ —ç—Ç–æ–º —Å–µ–∑–æ–Ω–µ.",
        "–≠–∫—Å–ø–µ—Ä—Ç—ã –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É—é—Ç –≤—ã—Å–æ–∫–∏–π —Å–ø—Ä–æ—Å –≤ –ª—é–∫—Å–æ–≤—ã—Ö –±—É—Ç–∏–∫–∞—Ö.",
        "–î–∏–∑–∞–π–Ω–µ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –Ω–æ–≤—É—é –∫–æ–Ω—Ü–µ–ø—Ü–∏—é, —Å–æ—á–µ—Ç–∞—é—â—É—é —Ç—Ä–∞–¥–∏—Ü–∏–∏ –∏ –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏.",
        "Fashion-—Å–æ–æ–±—â–µ—Å—Ç–≤–æ –∞–∫—Ç–∏–≤–Ω–æ –æ–±—Å—É–∂–¥–∞–µ—Ç —Å–º–µ–ª—ã–µ —Ä–µ—à–µ–Ω–∏—è –±—Ä–µ–Ω–¥–∞.",
        "–ö–æ–ª–ª–∞–±–æ—Ä–∞—Ü–∏—è –æ–±–µ—â–∞–µ—Ç —Å—Ç–∞—Ç—å –æ–¥–Ω–æ–π –∏–∑ —Å–∞–º—ã—Ö –∑–∞–º–µ—Ç–Ω—ã—Ö –≤ –≥–æ–¥—É.",
        "–ê—Ä—Ö–∏–≤–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã —Å–æ—á–µ—Ç–∞—é—Ç—Å—è —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è–º–∏ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞.",
        "–ë—Ä–µ–Ω–¥ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –º–∞—Å—Ç–µ—Ä—Å—Ç–≤–∞ –∏ –≤–Ω–∏–º–∞–Ω–∏—è –∫ –¥–µ—Ç–∞–ª—è–º.",
        "–ù–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É—Å—Ç–æ–π—á–∏–≤–æ–π –º–æ–¥–µ –≤—ã–∑—ã–≤–∞–µ—Ç –∏–Ω—Ç–µ—Ä–µ—Å —ç–∫—Å–ø–µ—Ä—Ç–æ–≤.",
        "–¢–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏ –≤ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ –≤–ø–µ—á–∞—Ç–ª—è—é—Ç —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–≤.",
        "–ö–æ–ª–ª–µ–∫—Ü–∏—è –æ—Ç—Ä–∞–∂–∞–µ—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ç—Ä–µ–Ω–¥—ã –∏ –Ω–∞—Å–ª–µ–¥–∏–µ –±—Ä–µ–Ω–¥–∞.",
    ]
    
    post += f"üíé <i>{random.choice(expert_insights)}</i>"
    
    return post

def send_telegram_post(post, image_url=None):
    """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –ø–æ—Å—Ç –≤ Telegram —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
    max_retries = 3
    retry_delay = 2
    
    for attempt in range(max_retries):
        try:
            if image_url:
                # –ü—Ä–æ–±—É–µ–º –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }
                
                # –°–∫–∞—á–∏–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å —Ç–∞–π–º–∞—É—Ç–æ–º
                image_response = requests.get(image_url, headers=headers, timeout=10)
                
                if image_response.status_code == 200 and len(image_response.content) > 1024:  # –ú–∏–Ω–∏–º—É–º 1KB
                    url = f'https://api.telegram.org/bot{BOT_TOKEN}/sendPhoto'
                    data = {
                        'chat_id': CHANNEL,
                        'caption': post,
                        'parse_mode': 'HTML'
                    }
                    files = {'photo': ('image.jpg', image_response.content, 'image/jpeg')}
                    response = requests.post(url, data=data, files=files, timeout=30)
                    
                    if response.status_code == 200:
                        logger.info("Post sent successfully with image")
                        return True
                    else:
                        logger.warning(f"Image post failed: {response.status_code}")
                else:
                    logger.warning("Invalid image, falling back to text")
            
            # –û—Ç–ø—Ä–∞–≤–∫–∞ –±–µ–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (fallback)
            url = f'https://api.telegram.org/bot{BOT_TOKEN}/sendMessage'
            data = {
                'chat_id': CHANNEL,
                'text': post,
                'parse_mode': 'HTML',
                'disable_web_page_preview': True
            }
            response = requests.post(url, json=data, timeout=30)
            
            if response.status_code == 200:
                logger.info("Post sent successfully as text")
                return True
            else:
                error_msg = response.json().get('description', 'Unknown error')
                logger.error(f"Telegram API error: {error_msg}")
                
        except requests.exceptions.Timeout:
            logger.warning(f"Timeout on attempt {attempt + 1}")
        except requests.exceptions.ConnectionError:
            logger.warning(f"Connection error on attempt {attempt + 1}")
        except Exception as e:
            logger.error(f"Unexpected error on attempt {attempt + 1}: {e}")
        
        # –ü–∞—É–∑–∞ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–æ–π
        if attempt < max_retries - 1:
            time.sleep(retry_delay * (attempt + 1))
    
    logger.error("Failed to send post after all retries")
    return False

def find_and_send_news():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ–∏—Å–∫–∞ –∏ –æ—Ç–ø—Ä–∞–≤–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π —Å AI-–ø–µ—Ä–µ–≤–æ–¥–æ–º"""
    random.shuffle(SOURCES)
    
    checked_sources = 0
    successful_posts = 0
    max_posts_per_run = 3  # –ú–∞–∫—Å–∏–º—É–º –ø–æ—Å—Ç–æ–≤ –∑–∞ –æ–¥–∏–Ω –∑–∞–ø—É—Å–∫
    
    logger.info(f"üîç Starting news search across {len(SOURCES)} sources...")
    
    for source in SOURCES:
        if successful_posts >= max_posts_per_run:
            logger.info("üéØ Reached maximum posts per run")
            break
            
        checked_sources += 1
        logger.info(f"[{checked_sources}/{len(SOURCES)}] Checking {source['name']}...")
        
        try:
            # –ü–∞—Ä—Å–∏–º RSS –ª–µ–Ω—Ç—É
            feed = feedparser.parse(source['url'])
            
            if not feed.entries:
                logger.info(f"   üì≠ No entries found in {source['name']}")
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –∑–∞–ø–∏—Å–µ–π –≤ —Å–ª—É—á–∞–π–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ
            entries_to_check = min(20, len(feed.entries))
            entries = feed.entries[:entries_to_check]
            random.shuffle(entries)
            
            brand_found = False
            
            for entry in entries:
                if successful_posts >= max_posts_per_run:
                    break
                    
                title = getattr(entry, 'title', '')
                description = getattr(entry, 'description', '')
                link = getattr(entry, 'link', '')
                published = getattr(entry, 'published', '')
                
                if not title:
                    continue
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–≤–µ–∂–µ—Å—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å –¥–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏)
                if published:
                    try:
                        publish_date = datetime.strptime(published, '%a, %d %b %Y %H:%M:%S %Z')
                        days_old = (datetime.now() - publish_date).days
                        if days_old > 7:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å—Ç–∞—Ä—ã–µ –∑–∞–ø–∏—Å–∏
                            continue
                    except:
                        pass
                
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞
                full_content = f"{title} {description}".lower()
                
                # –ò—â–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –±—Ä–µ–Ω–¥–æ–≤
                for brand in BRANDS:
                    if brand.lower() in full_content:
                        logger.info(f"   ‚úÖ Found news about {brand}")
                        
                        try:
                            # –°–æ–∑–¥–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
                            original_content = f"{title}. {description}"
                            
                            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏ –ø–µ—Ä–µ–≤–æ–¥–∏–º –∫–æ–Ω—Ç–µ–Ω—Ç
                            logger.info(f"   üîÑ Processing content for {brand}...")
                            rich_content = extract_rich_content(original_content, 600)
                            
                            if len(rich_content) < 150:
                                logger.info(f"   üìù Content too short for {brand} ({len(rich_content)} chars)")
                                continue
                            
                            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                            logger.info(f"   üñºÔ∏è Extracting image from {link}...")
                            image_url = extract_image_from_url(link)
                            
                            if image_url:
                                logger.info(f"   ‚úÖ Found quality image")
                            else:
                                logger.info(f"   üì∑ No suitable image found")
                            
                            # –°–æ–∑–¥–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –ø–æ—Å—Ç
                            logger.info(f"   ‚úçÔ∏è Creating post for {brand}...")
                            post = create_quality_post(brand, rich_content, image_url)
                            
                            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –ø–æ—Å—Ç
                            logger.info(f"   üì§ Sending post to Telegram...")
                            if send_telegram_post(post, image_url):
                                logger.info(f"   üéâ Successfully posted about {brand}!")
                                successful_posts += 1
                                brand_found = True
                                
                                # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –ø–æ—Å—Ç–∞–º–∏
                                time.sleep(5)
                                break  # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–º—É –∏—Å—Ç–æ—á–Ω–∏–∫—É –ø–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–π –æ—Ç–ø—Ä–∞–≤–∫–∏
                            else:
                                logger.error(f"   ‚ùå Failed to send post about {brand}")
                                
                        except Exception as e:
                            logger.error(f"   üîß Error processing {brand}: {str(e)}")
                            continue
                
                if brand_found:
                    break  # –í—ã—Ö–æ–¥–∏–º –∏–∑ —Ü–∏–∫–ª–∞ –ø–æ –∑–∞–ø–∏—Å—è–º –µ—Å–ª–∏ –Ω–∞—à–ª–∏ –ø–æ–¥—Ö–æ–¥—è—â–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç
            
        except Exception as e:
            logger.error(f"‚ùå Error with source {source['name']}: {str(e)}")
            continue
    
    logger.info(f"üìä Search completed: {successful_posts} posts sent from {checked_sources} sources checked")
    return successful_posts

def send_curated_post():
    """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∫—É—Ä–∏—Ä—É–µ–º—ã–π –ø–æ—Å—Ç –∫–æ–≥–¥–∞ –Ω–æ–≤–æ—Å—Ç–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"""
    logger.info("üé® Creating curated post...")
    
    brands = ['Supreme', 'Palace', 'Bape', 'Off-White', 'Balenciaga', 'Nike', 'Gucci', 'Dior']
    brand = random.choice(brands)
    
    curated_contents = [
        f"{brand} –∞–Ω–æ–Ω—Å–∏—Ä—É–µ—Ç –≤—ã–ø—É—Å–∫ –Ω–æ–≤–æ–π –∫–∞–ø—Å—É–ª—å–Ω–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏, –≤–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–Ω–æ–π –∞—Ä—Ö–∏–≤–Ω—ã–º–∏ –Ω–∞—Ö–æ–¥–∫–∞–º–∏ –∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º —É–ª–∏—á–Ω—ã–º –∏—Å–∫—É—Å—Å—Ç–≤–æ–º. –í —Ä–µ–ª–∏–∑ –≤–æ—à–ª–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–µ edition –∫—Ä–æ—Å—Å–æ–≤–∫–∏, —Ö—É–¥–∏ –∏ –∞–∫—Å–µ—Å—Å—É–∞—Ä—ã —Å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º –¥–∏–∑–∞–π–Ω–æ–º –∏ –ø—Ä–µ–º–∏–∞–ª—å–Ω—ã–º–∏ –º–∞—Ç–µ—Ä–∏–∞–ª–∞–º–∏. –û–∂–∏–¥–∞–µ—Ç—Å—è, —á—Ç–æ –∫–æ–ª–ª–µ–∫—Ü–∏—è —Å—Ç–∞–Ω–µ—Ç –æ–¥–Ω–æ–π –∏–∑ —Å–∞–º—ã—Ö –∂–µ–ª–∞–Ω–Ω—ã—Ö –≤ —ç—Ç–æ–º —Å–µ–∑–æ–Ω–µ.",
        f"{brand} –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é, —Å–æ–∑–¥–∞–Ω–Ω—É—é –≤ –∫–æ–ª–ª–∞–±–æ—Ä–∞—Ü–∏–∏ —Å –∏–∑–≤–µ—Å—Ç–Ω—ã–º —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º —Ö—É–¥–æ–∂–Ω–∏–∫–æ–º. –≠–∫—Å–∫–ª—é–∑–∏–≤–Ω—ã–µ –≤–µ—â–∏ —Å –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–º–∏ –º–∞—Ç–µ—Ä–∏–∞–ª–∞–º–∏ –∏ –∞–≤–∞–Ω–≥–∞—Ä–¥–Ω—ã–º –¥–∏–∑–∞–π–Ω–æ–º —É–∂–µ –≤—ã–∑–≤–∞–ª–∏ –∞–∂–∏–æ—Ç–∞–∂ —Å—Ä–µ–¥–∏ –∫–æ–ª–ª–µ–∫—Ü–∏–æ–Ω–µ—Ä–æ–≤ –∏ —Ü–µ–Ω–∏—Ç–µ–ª–µ–π –≤—ã—Å–æ–∫–æ–π –º–æ–¥—ã.",
        f"–ù–æ–≤—ã–π –¥—Ä–æ–ø –æ—Ç {brand} —Å–æ—á–µ—Ç–∞–µ—Ç —ç–ª–µ–º–µ–Ω—Ç—ã —É–ª–∏—á–Ω–æ–≥–æ —Å—Ç–∏–ª—è –∏ –≤—ã—Å–æ–∫–æ–π –º–æ–¥—ã. –ö–æ–ª–ª–µ–∫—Ü–∏—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Å–≤–µ–∂–∏–π –≤–∑–≥–ª—è–¥ –Ω–∞ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –≥–∞—Ä–¥–µ—Ä–æ–±, –æ–±—ä–µ–¥–∏–Ω—è—è –∫–æ–º—Ñ–æ—Ä—Ç –∏ —Ä–æ—Å–∫–æ—à—å –≤ –∫–∞–∂–¥–æ–º –∏–∑–¥–µ–ª–∏–∏. –î–∏–∑–∞–π–Ω–µ—Ä—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–ª–∏ —Å —Å–∏–ª—É—ç—Ç–∞–º–∏ –∏ —Ç–µ–∫—Å—Ç—É—Ä–∞–º–∏, —Å–æ–∑–¥–∞–≤–∞—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ –≤–µ—â–∏ –¥–ª—è –ø–æ–≤—Å–µ–¥–Ω–µ–≤–Ω–æ–π –Ω–æ—Å–∫–∏.",
        f"–ê—Ä—Ö–∏–≤–Ω–∞—è –Ω–∞—Ö–æ–¥–∫–∞: {brand} –≤–æ–∑—Ä–æ–∂–¥–∞–µ—Ç –∫—É–ª—å—Ç–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –∏–∑ 90-—Ö —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –∞–ø–≥—Ä–µ–π–¥–∞–º–∏. –û–∂–∏–¥–∞–µ—Ç—Å—è –≤—ã—Å–æ–∫–∏–π —Å–ø—Ä–æ—Å —Å—Ä–µ–¥–∏ –∫–æ–ª–ª–µ–∫—Ü–∏–æ–Ω–µ—Ä–æ–≤ –∏ —Ü–µ–Ω–∏—Ç–µ–ª–µ–π –≤–∏–Ω—Ç–∞–∂–Ω—ã—Ö –≤–µ—â–µ–π. –ù–æ–≤—ã–µ –≤–µ—Ä—Å–∏–∏ —Å–æ—Ö—Ä–∞–Ω–∏–ª–∏ –¥—É—Ö –æ—Ä–∏–≥–∏–Ω–∞–ª–æ–≤, –Ω–æ –ø–æ–ª—É—á–∏–ª–∏ —É–ª—É—á—à–µ–Ω–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã –∏ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—é.",
        f"{brand} –∑–∞–ø—É—Å–∫–∞–µ—Ç sustainable –∫–æ–ª–ª–µ–∫—Ü–∏—é —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø–µ—Ä–µ—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –∏ —ç–∫–æ–ª–æ–≥–∏—á–Ω—ã—Ö –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤. –ò–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç commitment –±—Ä–µ–Ω–¥–∞ –∫ —É—Å—Ç–æ–π—á–∏–≤–æ–º—É —Ä–∞–∑–≤–∏—Ç–∏—é –∏ –æ—Ç–≤–µ—á–∞–µ—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º —Ç—Ä–µ–Ω–¥–∞–º –æ—Å–æ–∑–Ω–∞–Ω–Ω–æ–≥–æ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è."
    ]
    
    content = random.choice(curated_contents)
    
    # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º
    if any(word in content for word in ['announces', 'launches', 'collaboration', 'collection']):
        content = translator.smart_translate(content)
    
    post = create_quality_post(brand, content)
    
    if send_telegram_post(post):
        logger.info("‚úÖ Curated post sent successfully!")
        return True
    
    logger.error("‚ùå Failed to send curated post")
    return False

def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ—Ç–∞"""
    logger.info("üè• Performing health check...")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Telegram API
    try:
        url = f'https://api.telegram.org/bot{BOT_TOKEN}/getMe'
        response = requests.get(url, timeout=10)
        if response.status_code == 200:
            logger.info("‚úÖ Telegram API is accessible")
        else:
            logger.error("‚ùå Telegram API is not accessible")
            return False
    except Exception as e:
        logger.error(f"‚ùå Telegram API check failed: {e}")
        return False
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    test_sources = random.sample(SOURCES, min(3, len(SOURCES)))
    working_sources = 0
    
    for source in test_sources:
        try:
            feed = feedparser.parse(source['url'])
            if feed.entries:
                working_sources += 1
                logger.info(f"‚úÖ {source['name']} is working")
            else:
                logger.warning(f"‚ö†Ô∏è {source['name']} has no entries")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è {source['name']} failed: {e}")
    
    logger.info(f"üìä Health check: {working_sources}/{len(test_sources)} test sources working")
    return working_sources > 0

if __name__ == "__main__":
    logger.info(f"üöÄ Starting AI Fashion News Bot")
    logger.info(f"üìö Sources: {len(SOURCES)}, Brands: {len(BRANDS)}")
    
    start_time = time.time()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å
    if not health_check():
        logger.warning("‚ö†Ô∏è Health check failed, but continuing...")
    
    # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –∏ –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —Ä–µ–∞–ª—å–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
    posts_sent = find_and_send_news()
    
    # –ï—Å–ª–∏ –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –∫—É—Ä–∏—Ä—É–µ–º—ã–π –ø–æ—Å—Ç
    if posts_sent == 0:
        logger.info("üìù No news found, creating curated content...")
        send_curated_post()
    else:
        logger.info(f"üéØ Successfully sent {posts_sent} posts")
    
    execution_time = time.time() - start_time
    logger.info(f"‚è±Ô∏è Total execution time: {execution_time:.2f} seconds")
    logger.info("‚úÖ Bot finished successfully!")

"""
Fashion News Bot - Enterprise Edition
Author: Gemini AI
Version: 2.0.0
Description: High-end news scraper tailored for Fashion Industry with intelligent text sanitization.
"""

import os
import re
import time
import hashlib
import sqlite3
import logging
import requests
import random
from abc import ABC, abstractmethod
from typing import List, Optional, Dict, Tuple
from dataclasses import dataclass
from datetime import datetime
from urllib.parse import urljoin
from bs4 import BeautifulSoup
from deep_translator import GoogleTranslator
from fake_useragent import UserAgent

# =================================================================================================
# 1. CONFIGURATION & LOGGING
# =================================================================================================

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è "–∫–∞–∫ —É –≤–∑—Ä–æ—Å–ª—ã—Ö"
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)-8s | %(module)-15s | %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger("FashionBot")

class Config:
    BOT_TOKEN = os.environ.get('BOT_TOKEN')
    CHANNEL = os.environ.get('CHANNEL')
    DB_NAME = 'news.db'
    MAX_NEWS_PER_RUN = 1  # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ 1 —Å–∞–º—É—é –∫—Ä—É—Ç—É—é –Ω–æ–≤–æ—Å—Ç—å –∑–∞ –∑–∞–ø—É—Å–∫, —á—Ç–æ–±—ã –Ω–µ —Å–ø–∞–º–∏—Ç—å
    MAX_RETRIES = 3
    TIMEOUT = 20
    MIN_TEXT_LENGTH = 150  # –ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ –∫–æ—Ä–æ—á–µ —ç—Ç–æ–≥–æ (—Å–∏–º–≤–æ–ª–æ–≤)

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞—Å—Ç—Ä–æ–µ–∫
    if not BOT_TOKEN or not CHANNEL:
        logger.critical("‚ùå FATAL: BOT_TOKEN or CHANNEL not found in env vars.")
        exit(1)

# =================================================================================================
# 2. DATA STRUCTURES
# =================================================================================================

@dataclass
class Article:
    title: str
    url: str
    content: str
    images: List[str]
    source_name: str

# =================================================================================================
# 3. DATABASE LAYER
# =================================================================================================

class Database:
    """–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ–º –¥–∞–Ω–Ω—ã—Ö SQLite —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –¥—É–±–ª–µ–π"""
    
    def __init__(self, db_path: str):
        self.db_path = db_path
        self._init_db()

    def _get_connection(self):
        return sqlite3.connect(self.db_path)

    def _init_db(self):
        with self._get_connection() as conn:
            conn.execute('''
                CREATE TABLE IF NOT EXISTS history (
                    hash TEXT PRIMARY KEY,
                    url TEXT,
                    title TEXT,
                    source TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            conn.commit()

    def exists(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, –±—ã–ª–∞ –ª–∏ –Ω–æ–≤–æ—Å—Ç—å —É–∂–µ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–∞"""
        url_hash = hashlib.md5(url.encode()).hexdigest()
        with self._get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute('SELECT 1 FROM history WHERE hash = ?', (url_hash,))
            return cursor.fetchone() is not None

    def add(self, article: Article):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –≤ –∏—Å—Ç–æ—Ä–∏—é"""
        url_hash = hashlib.md5(article.url.encode()).hexdigest()
        with self._get_connection() as conn:
            try:
                conn.execute(
                    'INSERT INTO history (hash, url, title, source) VALUES (?, ?, ?, ?)',
                    (url_hash, article.url, article.title, article.source_name)
                )
                conn.commit()
            except sqlite3.IntegrityError:
                pass

# =================================================================================================
# 4. TEXT PROCESSING ENGINE (SANITIZER & TRANSLATOR)
# =================================================================================================

class TextSanitizer:
    """–ö–ª–∞—Å—Å –¥–ª—è –∂–µ—Å—Ç–∫–æ–π –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –º—É—Å–æ—Ä–∞"""
    
    BAD_PATTERNS = [
        r'read more', r'click here', r'continue reading', r'subscribe', 
        r'sign up', r'follow us', r'source:', r'photo:', r'credit:', 
        r'images courtesy', r'via getty', r'advertisement',
        r'share this article', r'download the app'
    ]

    @staticmethod
    def clean(text: str) -> str:
        if not text: 
            return ""
        
        # 1. –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
        text = re.sub(r'\s+', ' ', text).strip()
        
        # 2. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è "–º—É—Å–æ—Ä–Ω—ã—Ö" —Ñ—Ä–∞–∑
        for pattern in TextSanitizer.BAD_PATTERNS:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)
            
        # 3. –£–¥–∞–ª–µ–Ω–∏–µ URL –≤–Ω—É—Ç—Ä–∏ —Ç–µ–∫—Å—Ç–∞
        text = re.sub(r'http\S+', '', text)
        
        return text.strip()

class TranslatorService:
    """–°–µ—Ä–≤–∏—Å –ø–µ—Ä–µ–≤–æ–¥–∞ —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
    
    def __init__(self):
        self.translator = GoogleTranslator(source='auto', target='ru')

    def translate(self, text: str) -> str:
        if not text:
            return ""
        
        # –ü–µ—Ä–µ–≤–æ–¥–∏–º —á–∞—Å—Ç—è–º–∏, –µ—Å–ª–∏ —Ç–µ–∫—Å—Ç –æ–≥—Ä–æ–º–Ω—ã–π (—Ö–æ—Ç—è –º—ã –µ–≥–æ —É–∂–µ –æ–±—Ä–µ–∑–∞–ª–∏)
        try:
            time.sleep(1) # –í–µ–∂–ª–∏–≤–æ—Å—Ç—å –∫ API
            return self.translator.translate(text[:4500])
        except Exception as e:
            logger.error(f"Translation failed: {e}")
            # –í –∫—Ä–∞–π–Ω–µ–º —Å–ª—É—á–∞–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª, –Ω–æ –ª—É—á—à–µ —Ç–∞–∫ –Ω–µ –¥–µ–ª–∞—Ç—å
            return text 

# =================================================================================================
# 5. NETWORK & PARSING LAYER
# =================================================================================================

class Browser:
    """–ò–º–∏—Ç–∞—Ü–∏—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ –±—Ä–∞—É–∑–µ—Ä–∞ –¥–ª—è –æ–±—Ö–æ–¥–∞ –∑–∞—â–∏—Ç—ã"""
    
    def __init__(self):
        self.ua = UserAgent()
        self.session = requests.Session()
    
    def get(self, url: str) -> Optional[requests.Response]:
        headers = {
            'User-Agent': self.ua.random,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Referer': 'https://www.google.com/'
        }
        try:
            response = self.session.get(url, headers=headers, timeout=Config.TIMEOUT)
            if response.status_code == 200:
                return response
            logger.warning(f"‚ö†Ô∏è HTTP {response.status_code} for {url}")
        except Exception as e:
            logger.error(f"Network error: {e}")
        return None

class BaseParser(ABC):
    """–ê–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–π –∫–ª–∞—Å—Å –ø–∞—Ä—Å–µ—Ä–∞"""
    def __init__(self):
        self.browser = Browser()
        self.sanitizer = TextSanitizer()

    @abstractmethod
    def get_latest_news(self) -> List[Article]:
        pass

    def _extract_images(self, soup, base_url) -> List[str]:
        """–£–º–Ω—ã–π –ø–æ–∏—Å–∫ –∫–∞—Ä—Ç–∏–Ω–æ–∫ –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è"""
        images = []
        # –ò—â–µ–º –∫–∞—Ä—Ç–∏–Ω–∫–∏ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –∫–æ–Ω—Ç–µ–Ω—Ç–µ
        content_div = soup.find('article') or soup.find('main') or soup.body
        if not content_div: return []

        img_tags = content_div.find_all('img')
        for img in img_tags:
            src = img.get('src') or img.get('data-src') or img.get('srcset')
            if not src: continue
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ srcset (–±–µ—Ä–µ–º —Å–∞–º—É—é –±–æ–ª—å—à—É—é)
            if ' ' in src: 
                src = src.split(' ')[0]
            
            if src.startswith('//'): src = 'https:' + src
            if src.startswith('/'): src = urljoin(base_url, src)
            
            # –§–∏–ª—å—Ç—Ä—ã –º—É—Å–æ—Ä–∞
            if any(x in src for x in ['logo', 'icon', 'avatar', 'gif', 'svg']):
                continue
                
            images.append(src)
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º 3 —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö
        return list(dict.fromkeys(images))[:3]

# ---------------- SPECIFIC PARSERS ----------------

class VogueParser(BaseParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è Vogue (–ß–∏—Å—Ç–∞—è –º–æ–¥–∞)"""
    BASE_URL = "https://www.vogue.com"
    NEWS_URL = "https://www.vogue.com/fashion/news"

    def get_latest_news(self) -> List[Article]:
        logger.info("üïµÔ∏è Scanning Vogue...")
        response = self.browser.get(self.NEWS_URL)
        if not response: return []

        soup = BeautifulSoup(response.content, 'lxml')
        articles = []
        
        # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç–∞—Ç—å–∏ (Vogue –∏–º–µ–µ—Ç —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É)
        links = soup.select('a.SummaryItemHedLink-civMjp') # –ö–ª–∞—Å—Å—ã –º–æ–≥—É—Ç –º–µ–Ω—è—Ç—å—Å—è, –ø–æ—ç—Ç–æ–º—É –∏—â–µ–º –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ
        if not links:
            links = soup.select('div[class*="SummaryItem"] a[href*="/article/"]')

        for link in links[:4]: # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ 4 —Å—Å—ã–ª–∫–∏
            href = link.get('href')
            if not href or '/article/' not in href: continue
            full_url = urljoin(self.BASE_URL, href)
            
            article = self._parse_article(full_url)
            if article:
                articles.append(article)
                
        return articles

    def _parse_article(self, url: str) -> Optional[Article]:
        response = self.browser.get(url)
        if not response: return None
        soup = BeautifulSoup(response.content, 'lxml')

        # 1. –ó–∞–≥–æ–ª–æ–≤–æ–∫
        h1 = soup.find('h1')
        title = h1.get_text().strip() if h1 else "Fashion News"

        # 2. –¢–µ–∫—Å—Ç (–ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã —Ç–µ–ª–∞ —Å—Ç–∞—Ç—å–∏)
        body = soup.find('div', {'class': lambda x: x and 'body' in x.lower()})
        if not body:
            body = soup.find('article')
        
        if not body: return None

        paragraphs = body.find_all('p')
        text_parts = []
        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 4 –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞ - —Ç–∞–º –æ–±—ã—á–Ω–æ —Å—É—Ç—å
        for p in paragraphs[:4]:
            clean = self.sanitizer.clean(p.get_text())
            if len(clean) > 50: # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –≤—Å—Ç–∞–≤–∫–∏
                text_parts.append(clean)
        
        content = "\n\n".join(text_parts)
        if len(content) < Config.MIN_TEXT_LENGTH: return None

        # 3. –ö–∞—Ä—Ç–∏–Ω–∫–∏
        images = self._extract_images(soup, url)

        return Article(title=title, url=url, content=content, images=images, source_name="Vogue")

class HypebeastParser(BaseParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è Hypebeast (–£–ª–∏—á–Ω–∞—è –º–æ–¥–∞)"""
    URL = "https://hypebeast.com/fashion"

    def get_latest_news(self) -> List[Article]:
        logger.info("üïµÔ∏è Scanning Hypebeast...")
        response = self.browser.get(self.URL)
        if not response: return []
        
        soup = BeautifulSoup(response.content, 'lxml')
        articles = []
        
        posts = soup.select('.post-box')
        for post in posts[:4]:
            link_tag = post.select_one('a.title')
            if not link_tag: continue
            
            url = link_tag.get('href')
            
            article = self._parse_article(url)
            if article:
                articles.append(article)
        return articles

    def _parse_article(self, url: str) -> Optional[Article]:
        response = self.browser.get(url)
        if not response: return None
        soup = BeautifulSoup(response.content, 'lxml')
        
        # –£–¥–∞–ª—è–µ–º "Related posts" —Å—Ä–∞–∑—É
        for div in soup.select('.related-posts, .post-footer, .comments'):
            div.decompose()

        title = soup.select_one('h1.post-title').get_text().strip()
        
        content_div = soup.select_one('.post-body-content')
        if not content_div: return None

        text_parts = []
        for p in content_div.find_all('p', recursive=False):
            clean = self.sanitizer.clean(p.get_text())
            # –§–∏–ª—å—Ç—Ä: –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å—Ç—Ä–æ–∫–∏, –≥–¥–µ –µ—Å—Ç—å "Price:" –∏–ª–∏ "Buy here"
            if len(clean) > 40 and "price:" not in clean.lower():
                text_parts.append(clean)
        
        # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 3 –∑–Ω–∞—á–∏–º—ã—Ö –∞–±–∑–∞—Ü–∞
        content = "\n\n".join(text_parts[:3])
        
        images = self._extract_images(soup, url)
        
        return Article(title=title, url=url, content=content, images=images, source_name="Hypebeast")

# =================================================================================================
# 6. TELEGRAM LAYER
# =================================================================================================

class TelegramBot:
    def __init__(self):
        self.token = Config.BOT_TOKEN
        self.channel = Config.CHANNEL
        self.api_url = f"https://api.telegram.org/bot{self.token}"

    def send_article(self, article: Article):
        """–û—Ç–ø—Ä–∞–≤–∫–∞ –∫—Ä–∞—Å–∏–≤–æ –æ—Ñ–æ—Ä–º–ª–µ–Ω–Ω–æ–π –Ω–æ–≤–æ—Å—Ç–∏"""
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
        # –ñ–∏—Ä–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫, –∑–∞—Ç–µ–º –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞, –∑–∞—Ç–µ–º —Ç–µ–∫—Å—Ç
        caption = f"<b>{article.title}</b>\n\n{article.content}"
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã –¥–ª—è Telegram (1024 —Å–∏–º–≤–æ–ª–∞ –¥–ª—è –ø–æ–¥–ø–∏—Å–∏ –∫ —Ñ–æ—Ç–æ)
        if len(caption) > 1000:
            caption = caption[:990] + "..."

        # –õ–æ–≥–∏–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏:
        # 1. –ï—Å–ª–∏ –µ—Å—Ç—å —Ñ–æ—Ç–æ -> sendMediaGroup (–∞–ª—å–±–æ–º)
        # 2. –ï—Å–ª–∏ —Ñ–æ—Ç–æ –æ–¥–Ω–æ -> sendPhoto
        # 3. –ï—Å–ª–∏ –Ω–µ—Ç —Ñ–æ—Ç–æ -> sendMessage

        try:
            if not article.images:
                return self._send_text(caption)
            
            if len(article.images) == 1:
                return self._send_single_photo(caption, article.images[0])
            
            return self._send_album(caption, article.images)

        except Exception as e:
            logger.error(f"Telegram send error: {e}")
            return False

    def _send_text(self, text):
        data = {'chat_id': self.channel, 'text': text, 'parse_mode': 'HTML', 'disable_web_page_preview': True}
        r = requests.post(f"{self.api_url}/sendMessage", json=data)
        return r.status_code == 200

    def _send_single_photo(self, caption, photo_url):
        data = {
            'chat_id': self.channel, 
            'photo': photo_url, 
            'caption': caption, 
            'parse_mode': 'HTML'
        }
        r = requests.post(f"{self.api_url}/sendPhoto", json=data)
        return r.status_code == 200

    def _send_album(self, caption, photos):
        media = []
        for i, url in enumerate(photos):
            item = {'type': 'photo', 'media': url}
            if i == 0: # –ü–æ–¥–ø–∏—Å—å —Ç–æ–ª—å–∫–æ –∫ –ø–µ—Ä–≤–æ–º—É
                item['caption'] = caption
                item['parse_mode'] = 'HTML'
            media.append(item)
        
        data = {'chat_id': self.channel, 'media': media}
        r = requests.post(f"{self.api_url}/sendMediaGroup", json=data)
        return r.status_code == 200

# =================================================================================================
# 7. MAIN CONTROLLER
# =================================================================================================

class BotController:
    """–ì–ª–∞–≤–Ω—ã–π –º–æ–∑–≥ –±–æ—Ç–∞"""
    
    def __init__(self):
        self.db = Database(Config.DB_NAME)
        self.tg = TelegramBot()
        self.translator = TranslatorService()
        # –°–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        self.parsers = [
            VogueParser(),
            HypebeastParser()
        ]

    def run(self):
        logger.info("üöÄ Starting Fashion News Cycle...")
        
        # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º –ø–∞—Ä—Å–µ—Ä—ã, —á—Ç–æ–±—ã –∫–∞–∂–¥—ã–π –∑–∞–ø—É—Å–∫ –Ω–∞—á–∏–Ω–∞–ª—Å—è —Å —Ä–∞–∑–Ω–æ–≥–æ —Å–∞–π—Ç–∞
        random.shuffle(self.parsers)
        
        news_sent_count = 0
        
        for parser in self.parsers:
            if news_sent_count >= Config.MAX_NEWS_PER_RUN:
                break
                
            try:
                # 1. –ü–æ–ª—É—á–∞–µ–º —Å—ã—Ä—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
                articles = parser.get_latest_news()
                
                for article in articles:
                    # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã
                    if self.db.exists(article.url):
                        continue
                    
                    logger.info(f"‚ú® Found fresh news: {article.title}")
                    
                    # 3. –ü–µ—Ä–µ–≤–æ–¥ (–°–∞–º—ã–π –¥–æ–ª–≥–∏–π –ø—Ä–æ—Ü–µ—Å—Å)
                    ru_title = self.translator.translate(article.title)
                    ru_content = self.translator.translate(article.content)
                    
                    # –°–æ–∑–¥–∞–µ–º –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é
                    final_article = Article(
                        title=ru_title,
                        url=article.url,
                        content=ru_content,
                        images=article.images,
                        source_name=article.source_name
                    )
                    
                    # 4. –û—Ç–ø—Ä–∞–≤–∫–∞
                    if self.tg.send_article(final_article):
                        logger.info("‚úÖ Published successfully!")
                        self.db.add(article)
                        news_sent_count += 1
                        
                        # –ï—Å–ª–∏ –æ—Ç–ø—Ä–∞–≤–∏–ª–∏ –æ–¥–Ω—É –Ω–æ–≤–æ—Å—Ç—å, –∑–∞–≤–µ—Ä—à–∞–µ–º —Ä–∞–±–æ—Ç—É, —á—Ç–æ–±—ã –Ω–µ —Å–ø–∞–º–∏—Ç—å
                        # (—Å–ª–µ–¥—É—é—â—É—é –Ω–æ–≤–æ—Å—Ç—å –±–æ—Ç –æ—Ç–ø—Ä–∞–≤–∏—Ç —á–µ—Ä–µ–∑ 30 –º–∏–Ω –ø—Ä–∏ —Å–ª–µ–¥—É—é—â–µ–º –∑–∞–ø—É—Å–∫–µ)
                        return 
                    else:
                        logger.error("‚ùå Failed to publish")
                    
                    time.sleep(5) # –ü–∞—É–∑–∞
                    
            except Exception as e:
                logger.error(f"Error processing source: {e}")

        logger.info("üí§ Cycle finished.")

if __name__ == "__main__":
    controller = BotController()
    controller.run()

"""
Fashion News Bot - Final Enterprise-Grade Version
Author: Gemini AI
Version: 3.0 (The Image Hunter)
Description: Stable RSS parsing with aggressive content filtering and multi-level image extraction.
"""

import os
import re
import time
import hashlib
import sqlite3
import logging
import requests
import feedparser
import random
import json
from typing import List, Optional, Tuple, Set
from dataclasses import dataclass
from urllib.parse import urljoin
from bs4 import BeautifulSoup
from deep_translator import GoogleTranslator
from fake_useragent import UserAgent

# ================= 1. CONFIGURATION & LOGGING =================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger("Bot")

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
BOT_TOKEN = os.environ.get('BOT_TOKEN')
CHANNEL = os.environ.get('CHANNEL')
DB_NAME = 'news.db'

# –°–ø–∏—Å–æ–∫ –Ω–∞–¥–µ–∂–Ω—ã—Ö RSS –ª–µ–Ω—Ç –¥–ª—è –º–æ–¥—ã
RSS_SOURCES = [
    {'name': 'Vogue', 'url': 'https://www.vogue.com/feed/rss'},
    {'name': 'Fashionista', 'url': 'https://fashionista.com/.rss/full/'},
    {'name': 'Hypebeast', 'url': 'https://hypebeast.com/fashion/feed'},
    {'name': 'Guardian Fashion', 'url': 'https://www.theguardian.com/fashion/rss'}
]

# –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –Ω–æ–≤–æ—Å—Ç–µ–π
FASHION_KEYWORDS = [
    'fashion house', 'collaboration', 'collab', 'clothing', 
    'drop', 'collection', 'brand', 'designer', 'runway', 
    'couture', 'ready-to-wear', 'capsule', 'sneaker', 'apparel',
    '–º–æ–¥–∞', '–¥—Ä–æ–ø', '–∫–æ–ª–ª–µ–∫—Ü–∏—è', '–±—Ä–µ–Ω–¥', '–¥–∏–∑–∞–π–Ω–µ—Ä', '–æ–¥–µ–∂–¥–∞', '–∫—Ä–æ—Å—Å–æ–≤–∫–∏'
]

@dataclass
class Article:
    title: str
    url: str
    content: str
    images: List[str]
    source: str

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
if not BOT_TOKEN or not CHANNEL:
    logger.critical("‚ùå FATAL: BOT_TOKEN or CHANNEL not found in env vars.")
    exit(1) 

# ================= 2. DATABASE LAYER =================
class Database:
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–π."""
    def __init__(self):
        self.conn = sqlite3.connect(DB_NAME)
        self.cursor = self.conn.cursor()
        self.cursor.execute('''
            CREATE TABLE IF NOT EXISTS history (
                hash TEXT PRIMARY KEY,
                title TEXT,
                date TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        self.conn.commit()

    def exists(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –±—ã–ª–∞ –ª–∏ –Ω–æ–≤–æ—Å—Ç—å —Å —ç—Ç–∏–º URL —É–∂–µ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–∞."""
        h = hashlib.md5(url.encode()).hexdigest()
        res = self.cursor.execute('SELECT 1 FROM history WHERE hash = ?', (h,)).fetchone()
        return res is not None

    def add(self, url: str, title: str):
        """–î–æ–±–∞–≤–ª—è–µ—Ç –Ω–æ–≤–æ—Å—Ç—å –≤ –∏—Å—Ç–æ—Ä–∏—é."""
        h = hashlib.md5(url.encode()).hexdigest()
        try:
            self.cursor.execute('INSERT OR IGNORE INTO history (hash, title) VALUES (?, ?)', (h, title))
            self.conn.commit()
        except Exception as e:
            logger.error(f"DB add error: {e}")

# ================= 3. TOOLS: SANITIZER & TRANSLATOR =================
class TextSanitizer:
    """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç —Å–ª—É–∂–µ–±–Ω—ã—Ö —Ñ—Ä–∞–∑ –∏ HTML-—Ç–µ–≥–æ–≤."""
    @staticmethod
    def clean(text: str) -> str:
        if not text: return ""
        text = BeautifulSoup(text, "lxml").get_text(separator=' ')
        bad_phrases = ['Read more', 'Source:', 'Photo:', 'Courtesy of', 'Click here', 
                       'Subscribe', 'Advertisement', 'Image Credit', 'Shop Now', 'Share this article']
        for phrase in bad_phrases:
            text = re.sub(phrase, '', text, flags=re.IGNORECASE)
        text = re.sub(r'\s+', ' ', text).strip()
        return text

class TranslatorService:
    """–°–µ—Ä–≤–∏—Å –ø–µ—Ä–µ–≤–æ–¥–∞ —á–µ—Ä–µ–∑ Google Translator."""
    def __init__(self):
        self.translator = GoogleTranslator(source='auto', target='ru')

    def translate(self, text: str) -> str:
        """–ü–µ—Ä–µ–≤–æ–¥–∏—Ç —Ç–µ–∫—Å—Ç —Å –ø–∞—É–∑–æ–π –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏."""
        try:
            if not text: return ""
            if len(text) > 4000: text = text[:4000]
            time.sleep(1) 
            return self.translator.translate(text)
        except Exception as e:
            logger.error(f"Translation error: {e}")
            return text

# ================= 4. CONTENT EXTRACTOR (THE IMAGE HUNTER) =================
class Extractor:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ —Å—Ç–∞—Ç—å–∏ —Å –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–º –ø–æ–∏—Å–∫–æ–º —Ñ–æ—Ç–æ."""
    def __init__(self):
        self.ua = UserAgent()
        self.session = requests.Session()
        self.sanitizer = TextSanitizer()

    def _get_article_soup(self, url: str) -> Optional[BeautifulSoup]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É –∏ —É–¥–∞–ª—è–µ—Ç –º—É—Å–æ—Ä–Ω—ã–µ –±–ª–æ–∫–∏."""
        try:
            headers = {'User-Agent': self.ua.random, 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9'}
            resp = self.session.get(url, headers=headers, timeout=15)
            if resp.status_code != 200:
                logger.warning(f"‚ö†Ô∏è Failed to load content (HTTP {resp.status_code}) from {url}")
                return None
            
            soup = BeautifulSoup(resp.content, 'lxml')
            for el in soup.find_all(['script', 'style', 'nav', 'footer', 'aside', 'iframe', 'header', '.ad', '.sidebar', '.paywall']):
                if el: el.decompose()
            return soup
        except Exception as e:
            logger.error(f"Network or Soup error: {e}")
            return None

    def _clean_image_url(self, src: str, base_url: str) -> Optional[str]:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç, —á–∏—Å—Ç–∏—Ç –∏ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è."""
        if not src: return None
        
        # –ï—Å–ª–∏ —ç—Ç–æ srcset, –±–µ—Ä–µ–º –ø–µ—Ä–≤—É—é (—Å–∞–º—É—é –±–æ–ª—å—à—É—é) —Å—Å—ã–ª–∫—É
        if ' ' in src and ',' in src: 
            src = src.split(',')[0].strip().split(' ')[0]
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ç–æ–∫–æ–ª–∞
        if src.startswith('//'): src = 'https:' + src
        if not src.startswith('http'): src = urljoin(base_url, src)

        # –§–∏–ª—å—Ç—Ä –º—É—Å–æ—Ä–∞ –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞
        if any(x in src.lower() for x in ['logo', 'icon', 'avatar', 'svg', 'thumb', 'small', 'ads', 'gif']):
            return None
        if not src.endswith(('.jpg', '.jpeg', '.png', '.webp', '.avif')):
            return None
            
        return src

    def _find_all_images(self, soup: BeautifulSoup, url: str) -> Set[str]:
        """<-- –°–ê–ú–´–ô –í–ê–ñ–ù–´–ô –ë–õ–û–ö: –ú–ù–û–ì–û–£–†–û–í–ù–ï–í–´–ô –ü–û–ò–°–ö –ö–ê–†–¢–ò–ù–û–ö -->"""
        images: Set[str] = set()
        
        # 1. –ü–æ–∏—Å–∫ –≤ –º–µ—Ç–∞-—Ç–µ–≥–∞—Ö (—Å–∞–º—ã–π –Ω–∞–¥–µ–∂–Ω—ã–π —Å–ø–æ—Å–æ–± –Ω–∞–π—Ç–∏ –≥–ª–∞–≤–Ω–æ–µ —Ñ–æ—Ç–æ)
        # og:image (Facebook/Open Graph)
        og_image = soup.find('meta', property='og:image')
        if og_image and og_image.get('content'):
            clean_src = self._clean_image_url(og_image['content'], url)
            if clean_src: images.add(clean_src)

        # 2. –ü–æ–∏—Å–∫ –≤ JSON-LD (Schema.org, —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Google)
        try:
            scripts = soup.find_all('script', type='application/ld+json')
            for script in scripts:
                data = json.loads(script.string)
                if isinstance(data, dict) and data.get('image'):
                    img_data = data['image']
                    if isinstance(img_data, str):
                        clean_src = self._clean_image_url(img_data, url)
                        if clean_src: images.add(clean_src)
                    elif isinstance(img_data, dict) and img_data.get('url'):
                        clean_src = self._clean_image_url(img_data['url'], url)
                        if clean_src: images.add(clean_src)
        except:
            pass
        
        # 3. –ê–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –≤ HTML-—Ç–µ–ª–µ —Å—Ç–∞—Ç—å–∏
        article_body = soup.find('article') or soup.find('main') or soup.body
        if article_body:
            # –°–µ–ª–µ–∫—Ç–æ—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –∏—â—É—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤–æ –≤—Å–µ—Ö –≤–æ–∑–º–æ–∂–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö
            img_tags = article_body.select('picture img, figure img, img[data-src], img[srcset], img')
            
            for tag in img_tags:
                src = tag.get('data-src') or tag.get('srcset') or tag.get('src')
                if src:
                    clean_src = self._clean_image_url(src, url)
                    if clean_src: images.add(clean_src)
                    
        return images

    def get_full_content(self, url: str) -> Tuple[Optional[str], List[str]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç (—Å–æ–∫—Ä–∞—â–µ–Ω–æ –¥–æ 3 –∞–±–∑–∞—Ü–µ–≤) –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ —Å—Ç–∞—Ç—å–∏."""
        soup = self._get_article_soup(url)
        if not soup: return None, []
        
        article_body = soup.find('article') or soup.find('main') or soup.body

        # 1. –ü–æ–∏—Å–∫ —Ç–µ–∫—Å—Ç–∞ (–±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 3 —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞)
        full_text = ""
        if article_body:
            paragraphs = article_body.find_all('p')
            text_parts = [self.sanitizer.clean(p.get_text()) for p in paragraphs if len(p.get_text()) > 50][:3]
            full_text = "\n\n".join(text_parts)

        # 2. –ü–æ–∏—Å–∫ –∫–∞—Ä—Ç–∏–Ω–æ–∫
        raw_images = self._find_all_images(soup, url)
        
        # 3. –í—ã–±–æ—Ä–∫–∞: –±–µ—Ä–µ–º –Ω–µ –±–æ–ª–µ–µ 3 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        final_images = list(raw_images)[:3]
        logger.info(f"üñºÔ∏è Successfully extracted {len(final_images)} images from {url}")
        
        return full_text, final_images

# ================= 5. TELEGRAM SENDER =================
class TelegramSender:
    """–û—Ç–ø—Ä–∞–≤–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏—è –≤ Telegram —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ñ–æ—Ç–æ–∞–ª—å–±–æ–º–æ–≤."""
    def __init__(self):
        self.api = f"https://api.telegram.org/bot{BOT_TOKEN}"

    def send(self, article: Article) -> bool:
        """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Å—Ç–∞—Ç—å—é (—Ñ–æ—Ç–æ + —Ç–µ–∫—Å—Ç –∏–ª–∏ —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç)."""
        caption = f"<b>{article.title}</b>\n\n{article.content}"
        
        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã –ø–æ–¥–ø–∏—Å–∏
        MAX_CAPTION_LENGTH = 700 
        if len(caption) > MAX_CAPTION_LENGTH: 
            caption = caption[:(MAX_CAPTION_LENGTH - 4)] + "..."
        
        try:
            if article.images:
                media = []
                # –§–æ—Ä–º–∏—Ä—É–µ–º –º–µ–¥–∏–∞-–≥—Ä—É–ø–ø—É
                for i, img in enumerate(article.images):
                    item = {'type': 'photo', 'media': img}
                    if i == 0: 
                        item['caption'] = caption
                        item['parse_mode'] = 'HTML'
                    media.append(item)
                
                # –ü–æ–ø—ã—Ç–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ —Ñ–æ—Ç–æ–∞–ª—å–±–æ–º–∞
                r = requests.post(f"{self.api}/sendMediaGroup", json={'chat_id': CHANNEL, 'media': media})
                if r.status_code == 200: 
                    logger.info("Sent via MediaGroup successfully.")
                    return True
                
                logger.warning(f"Failed to send media group. Trying text fallback. Status: {r.status_code}")
            
            # –§–æ–ª–±–µ–∫: –û—Ç–ø—Ä–∞–≤–∫–∞ —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–∞
            data = {'chat_id': CHANNEL, 'text': caption, 'parse_mode': 'HTML', 'disable_web_page_preview': True}
            r = requests.post(f"{self.api}/sendMessage", json=data)
            return r.status_code == 200

        except Exception as e:
            logger.error(f"Telegram send critical error: {e}")
            return False

# ================= 6. MAIN CONTROLLER =================
def is_relevant(title: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ª–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Ç–µ–º–µ –º–æ–¥—ã/–¥—Ä–æ–ø–æ–≤."""
    check_text = title.lower()
    return any(k in check_text for k in FASHION_KEYWORDS)

def run():
    logger.info("üöÄ Bot started (Final Enterprise Run)")

    db = Database()
    extractor = Extractor()
    sender = TelegramSender()
    translator_service = TranslatorService()
    
    random.shuffle(RSS_SOURCES)
    
    news_sent = 0
    MAX_NEWS_PER_RUN = 1 
    
    for source in RSS_SOURCES:
        if news_sent >= MAX_NEWS_PER_RUN: break

        logger.info(f"üì° Checking {source['name']}...")
        try:
            feed = feedparser.parse(source['url'])
            if not feed.entries:
                logger.warning(f"Empty feed for {source['name']}")
                continue

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ 5 —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π, —á—Ç–æ–±—ã –Ω–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é
            for entry in feed.entries[:5]: 
                url = entry.link
                title = entry.title
                
                if db.exists(url): continue
                
                # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Ç–µ–º–µ
                if not is_relevant(title):
                    logger.info(f"Title '{title}' is not relevant to drops/collabs. Skipping.")
                    continue
                
                logger.info(f"Found relevant news: {title}")
                
                # 1. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏ –∫–∞—Ä—Ç–∏–Ω–æ–∫
                site_text, site_images = extractor.get_full_content(url)
                
                content_en = site_text or getattr(entry, 'summary', '')
                
                if len(content_en) < 150: 
                    logger.info("Content too short, skipping")
                    continue

                # 2. –ü–µ—Ä–µ–≤–æ–¥
                logger.info("Translating...")
                title_ru = translator_service.translate(title)
                content_ru = translator_service.translate(content_en)
                
                # 3. –û—Ç–ø—Ä–∞–≤–∫–∞
                article = Article(title_ru, url, content_ru, site_images, source['name'])
                if sender.send(article):
                    logger.info("‚úÖ Article published successfully!")
                    db.add(url, title)
                    news_sent += 1
                    break # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–º—É –∑–∞–ø—É—Å–∫—É
                else:
                    logger.error("‚ùå Failed to send article data.")
                
                time.sleep(5)

        except Exception as e:
            logger.error(f"Error processing source {source['name']}: {e}")

    logger.info("üí§ Cycle finished.")

if __name__ == "__main__":
    run()

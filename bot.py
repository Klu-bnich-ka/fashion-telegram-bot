import requests
import os
import re
import random
from bs4 import BeautifulSoup
import feedparser
from datetime import datetime, timedelta
import time
import json
import logging
import hashlib
from urllib.parse import urljoin, quote
import sqlite3
from contextlib import contextmanager
import urllib3
from textblob import TextBlob
from googlesearch import search as google_search
import urllib.parse

# –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
BOT_TOKEN = os.environ['BOT_TOKEN']
CHANNEL = os.environ['CHANNEL']

# –¢–æ–ª—å–∫–æ 3 —Å–∞–º—ã—Ö –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞
SOURCES = [
    {
        'name': 'Hypebeast', 
        'url': 'https://hypebeast.com/fashion/feed',
        'lang': 'en',
        'weight': 10  # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –≤ –ø–æ–∏—Å–∫–µ
    },
    {
        'name': 'Highsnobiety', 
        'url': 'https://www.highsnobiety.com/feed/',
        'lang': 'en', 
        'weight': 9
    },
    {
        'name': 'Sneaker News',
        'url': 'https://sneakernews.com/feed/',
        'lang': 'en',
        'weight': 8
    }
]

# –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –±—Ä–µ–Ω–¥–æ–≤ –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∫—Ä—ã—Ç–∏—è
BRANDS = [
    'Nike', 'Jordan', 'Adidas', 'New Balance', 'Supreme', 'Palace', 
    'Bape', 'Stussy', 'Off-White', 'Balenciaga', 'Gucci', 'Dior',
    'Louis Vuitton', 'Prada', 'Chanel', 'Versace', 'Yeezy', 'Fear of God',
    'Essentials', 'Rhude', 'Amiri', 'A-Cold-Wall', 'Kith', 'Noah',
    'Aime Leon Dore', 'Brain Dead', 'Awake NY', 'Carhartt WIP', 'Stone Island',
    'Moncler', 'Bottega Veneta', 'Loewe', 'Givenchy', 'Burberry', 'Tom Ford',
    'Alexander McQueen', 'Saint Laurent', 'Celine', 'Vetements', 'Comme des Gar√ßons',
    'Maison Margiela', 'Acne Studios', 'Rick Owens', 'Raf Simons', 'JW Anderson',
    'Palm Angels', 'Heron Preston', 'Martine Rose', 'CP Company', 'Arc\'teryx',
    'Salomon', 'Asics', 'Converse', 'Vans', 'Puma', 'Reebok', 'Dr. Martens',
    'Birkenstock', 'Crocs', 'Champion', 'Fila', 'Ellesse', 'Kappa', 'Lacoste',
    'Fred Perry', 'Ben Sherman', 'Baracuta', 'Timberland', 'Wolverine', 'Red Wing'
]

# –≠–º–æ–¥–∑–∏ –¥–ª—è –±—Ä–µ–Ω–¥–æ–≤ –≤ —Å—Ç–∏–ª–µ –¢–æ–ø–æ—Ä–∞
BRAND_EMOJIS = {
    'Nike': 'üëü', 'Jordan': 'üÖ∞Ô∏è', 'Adidas': '‚ùå', 'Supreme': 'üî¥', 
    'Palace': 'üî∑', 'Bape': 'üêí', 'Stussy': 'üèÑ', 'Off-White': 'üü®',
    'Balenciaga': 'üëü', 'Gucci': 'üêç', 'Dior': 'üåπ', 'Louis Vuitton': 'üß≥',
    'Prada': 'üî∫', 'Chanel': 'üëë', 'Yeezy': 'üåä', 'Fear of God': '‚òÅÔ∏è',
    'Essentials': '‚ö´', 'Rhude': 'üåµ', 'Amiri': '‚≠ê', 'A-Cold-Wall': 'üß±',
    'Kith': 'üç¶', 'Stone Island': 'üß≠', 'Moncler': 'ü¶¢', 'default': 'üëï'
}

class ToporStyleFormatter:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Å—Ç–∏–ª–µ –¢–æ–ø–æ—Ä–∞"""
    
    @staticmethod
    def create_news_post(brand, title, content, engagement_data, image_url=None):
        """–°–æ–∑–¥–∞–µ—Ç –ø–æ—Å—Ç –≤ —Å—Ç–∏–ª–µ –¢–æ–ø–æ—Ä–∞"""
        
        emoji = BRAND_EMOJIS.get(brand, BRAND_EMOJIS['default'])
        
        # –û—Å–Ω–æ–≤–Ω–æ–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ (–∫–∞–∫ –≤ –¢–æ–ø–æ—Ä–µ)
        main_title = f"{emoji} {title}"
        
        # –û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)
        content_paragraphs = content.split('. ')
        short_content = '. '.join(content_paragraphs[:2]) + '.'
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ (–ø–æ–¥–ø–∏—Å—á–∏–∫–∏, –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ - –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ —á–∏—Å–ª–∞)
        subscribers = f"{random.randint(800, 1500)}K" 
        comments = random.randint(200, 3000)
        views = f"{random.randint(1, 3)}.{random.randint(1, 9)}M"
        
        # –í—Ä–µ–º—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ (—Ä–∞–Ω–¥–æ–º–Ω–æ–µ –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö 2 —á–∞—Å–æ–≤)
        time_posted = f"{random.randint(12, 23)}:{random.randint(10, 59)}"
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –ø–æ—Å—Ç
        post = f"""{main_title}

{short_content}

–¢–æ–ø–æ—Ä +18. –ü–æ–¥–ø–∏—Å–∞—Ç—å—Å—è
{subscribers} {time_posted}

{comments} –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤

–¢–æ–ø–æ—Ä+
"""
        return post

    @staticmethod
    def create_viral_post(brand, title, content, engagement_data):
        """–°–æ–∑–¥–∞–µ—Ç –≤–∏—Ä–∞–ª—å–Ω—ã–π –ø–æ—Å—Ç —Å –≤—ã—Å–æ–∫–æ–π –≤–æ–≤–ª–µ—á–µ–Ω–Ω–æ—Å—Ç—å—é"""
        
        emoji = BRAND_EMOJIS.get(brand, BRAND_EMOJIS['default'])
        
        subscribers = f"{random.randint(500, 1200)}K"
        comments = random.randint(500, 5000)
        time_posted = f"{random.randint(10, 22)}:{random.randint(10, 59)}"
        
        post = f"""{emoji} {title}

{content}

–¢–æ–ø–æ—Ä +18. –ü–æ–¥–ø–∏—Å–∞—Ç—å—Å—è  
{subscribers} {time_posted}  

{comments} –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤  

–¢–æ–ø–æ—Ä+
"""
        return post

class AdvancedNewsAggregator:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        })
        
    def get_trending_keywords(self):
        """–ü–æ–ª—É—á–∞–µ—Ç —Ç—Ä–µ–Ω–¥–æ–≤—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ –º–æ–¥–µ"""
        trending_keywords = [
            "new collection", "collaboration", "limited edition", "sneaker release",
            "fashion week", "designer collection", "streetwear", "luxury fashion",
            "capsule collection", "resell market", "drop", "exclusive"
        ]
        return random.sample(trending_keywords, 3)
    
    def calculate_engagement_score(self, title, content, brand):
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –æ—Ü–µ–Ω–∫—É –≤–æ–≤–ª–µ—á–µ–Ω–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        score = 0
        
        # –ë–æ–Ω—É—Å—ã –∑–∞ –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –±—Ä–µ–Ω–¥—ã
        popular_brands = ['Nike', 'Jordan', 'Supreme', 'Adidas', 'Balenciaga', 'Gucci']
        if brand in popular_brands:
            score += 30
            
        # –ë–æ–Ω—É—Å –∑–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        viral_keywords = ['collaboration', 'limited', 'exclusive', 'release', 'new', 'first']
        for keyword in viral_keywords:
            if keyword.lower() in title.lower():
                score += 10
                
        # –ë–æ–Ω—É—Å –∑–∞ –¥–ª–∏–Ω—É –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (–æ–ø—Ç–∏–º–∞–ª—å–Ω–∞—è 100-300 —Å–∏–º–≤–æ–ª–æ–≤)
        content_length = len(content)
        if 100 <= content_length <= 300:
            score += 20
        elif content_length > 300:
            score += 10
            
        return min(score, 100)
    
    def find_most_viral_content(self):
        """–ò—â–µ—Ç —Å–∞–º—ã–π –≤–∏—Ä–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        all_news = []
        
        for source in SOURCES:
            try:
                logger.info(f"üîç Checking {source['name']}...")
                news_items = self.parse_source(source)
                all_news.extend(news_items)
                time.sleep(2)  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            except Exception as e:
                logger.error(f"‚ùå Error parsing {source['name']}: {e}")
                continue
                
        if not all_news:
            return None
            
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –æ—Ü–µ–Ω–∫–µ –≤–æ–≤–ª–µ—á–µ–Ω–Ω–æ—Å—Ç–∏
        all_news.sort(key=lambda x: x['engagement_score'], reverse=True)
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø-3 —Å–∞–º—ã—Ö –≤–∏—Ä–∞–ª—å–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–∏
        return all_news[:3]
    
    def parse_source(self, source):
        """–ü–∞—Ä—Å–∏—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫"""
        news_items = []
        
        try:
            feed = feedparser.parse(source['url'])
            
            for entry in feed.entries[:10]:  # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ 10 –ø–æ—Å–ª–µ–¥–Ω–∏—Ö
                if self.is_recent_news(entry):
                    news_item = self.process_news_entry(entry, source)
                    if news_item:
                        news_items.append(news_item)
                        
        except Exception as e:
            logger.error(f"‚ùå Error parsing feed {source['name']}: {e}")
            
        return news_items
    
    def is_recent_news(self, entry, max_hours=6):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –Ω–æ–≤–æ—Å—Ç—å —Å–≤–µ–∂–µ–π (–¥–æ 6 —á–∞—Å–æ–≤)"""
        date_fields = ['published', 'updated', 'pubDate']
        
        for field in date_fields:
            date_str = getattr(entry, field, None)
            if date_str:
                try:
                    news_date = self.parse_date(date_str)
                    if news_date:
                        time_diff = datetime.now() - news_date
                        return time_diff.total_seconds() / 3600 <= max_hours
                except:
                    continue
        return False
    
    def parse_date(self, date_string):
        """–ü–∞—Ä—Å–∏—Ç –¥–∞—Ç—É –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤"""
        formats = [
            '%a, %d %b %Y %H:%M:%S %Z',
            '%a, %d %b %Y %H:%M:%S %z',
            '%Y-%m-%dT%H:%M:%SZ',
            '%Y-%m-%d %H:%M:%S',
            '%d %b %Y %H:%M:%S'
        ]
        
        for fmt in formats:
            try:
                return datetime.strptime(date_string, fmt)
            except:
                continue
        return None
    
    def process_news_entry(self, entry, source):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∑–∞–ø–∏—Å—å –Ω–æ–≤–æ—Å—Ç–∏"""
        title = getattr(entry, 'title', '')
        description = getattr(entry, 'description', '')
        link = getattr(entry, 'link', '')
        
        if not title:
            return None
            
        # –ò—â–µ–º –±—Ä–µ–Ω–¥—ã –≤ –∫–æ–Ω—Ç–µ–Ω—Ç–µ
        brand = self.extract_brand(title + " " + description)
        if not brand:
            return None
            
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
        content = self.clean_content(description or title)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        image_url = self.extract_image(entry, link)
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –æ—Ü–µ–Ω–∫—É –≤–æ–≤–ª–µ—á–µ–Ω–Ω–æ—Å—Ç–∏
        engagement_score = self.calculate_engagement_score(title, content, brand)
        
        return {
            'title': title,
            'content': content,
            'brand': brand,
            'source': source['name'],
            'engagement_score': engagement_score,
            'image_url': image_url,
            'link': link,
            'original_title': title
        }
    
    def extract_brand(self, text):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –±—Ä–µ–Ω–¥ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        text_lower = text.lower()
        
        for brand in BRANDS:
            if brand.lower() in text_lower:
                return brand
                
        return None
    
    def clean_content(self, content):
        """–û—á–∏—â–∞–µ—Ç –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç"""
        # –£–¥–∞–ª—è–µ–º HTML —Ç–µ–≥–∏
        clean = re.sub('<[^<]+?>', '', content)
        
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        clean = re.sub('\s+', ' ', clean).strip()
        
        # –û–±—Ä–µ–∑–∞–µ–º –¥–æ —Ä–∞–∑—É–º–Ω–æ–π –¥–ª–∏–Ω—ã
        if len(clean) > 300:
            sentences = clean.split('. ')
            clean = '. '.join(sentences[:2]) + '.'
            
        return clean
    
    def extract_image(self, entry, link):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑ –∑–∞–ø–∏—Å–∏"""
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –º–µ–¥–∏–∞-–∫–æ–Ω—Ç–µ–Ω—Ç –≤ RSS
        if hasattr(entry, 'media_content'):
            for media in entry.media_content:
                if media.get('type', '').startswith('image/'):
                    return media['url']
                    
        if hasattr(entry, 'links'):
            for link_obj in entry.links:
                if link_obj.get('type', '').startswith('image/'):
                    return link_obj['href']
        
        # –ü–∞—Ä—Å–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        return self.extract_image_from_page(link)
    
    def extract_image_from_page(self, url):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            response = self.session.get(url, timeout=10, verify=False)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            selectors = [
                'meta[property="og:image"]',
                'meta[name="twitter:image"]',
                'meta[property="twitter:image:src"]',
                '.article-image img',
                '.post-image img',
                '.wp-post-image',
                '.entry-content img',
                '.content img',
                'figure img',
                '.hero-image img',
                '.featured-image img',
                'img[src*="large"]',
                'img[src*="medium"]',
                'img'
            ]
            
            for selector in selectors:
                images = soup.select(selector)
                for img in images:
                    src = None
                    if selector.startswith('meta'):
                        src = img.get('content', '')
                    else:
                        src = img.get('src') or img.get('data-src') or img.get('data-lazy-src')
                    
                    if src and self.is_quality_image(src):
                        if src.startswith('//'):
                            src = 'https:' + src
                        elif src.startswith('/'):
                            src = urljoin(url, src)
                            
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–æ—Å—Ç—É–ø–Ω–æ
                        if self.verify_image(src):
                            return src
                            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Image extraction failed: {e}")
            
        return None
    
    def is_quality_image(self, url):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–º"""
        if not url.startswith(('http://', 'https://')):
            return False
            
        excluded = ['icon', 'logo', 'thumbnail', 'avatar', 'pixel', 'spinner']
        if any(term in url.lower() for term in excluded):
            return False
            
        valid_ext = ['.jpg', '.jpeg', '.png', '.webp']
        if not any(ext in url.lower() for ext in valid_ext):
            return False
            
        return True
    
    def verify_image(self, url):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        try:
            response = self.session.head(url, timeout=5, verify=False)
            return response.status_code == 200
        except:
            return False

class SmartContentEnhancer:
    """–£–º–Ω—ã–π —É—Å–∏–ª–∏—Ç–µ–ª—å –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
    
    def __init__(self):
        self.translation_cache = {}
        
    def enhance_content(self, original_content, brand):
        """–£–ª—É—á—à–∞–µ—Ç –∏ –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç"""
        # –°–Ω–∞—á–∞–ª–∞ –ø–µ—Ä–µ–≤–æ–¥–∏–º, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        content = self.smart_translate(original_content)
        
        # –£–ª—É—á—à–∞–µ–º —Å—Ç–∏–ª—å
        content = self.improve_writing_style(content, brand)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤–∏—Ä–∞–ª—å–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        content = self.add_viral_elements(content, brand)
        
        return content
    
    def smart_translate(self, text):
        """–£–º–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        if self.is_english(text):
            return self.translate_to_russian(text)
        return text
    
    def is_english(self, text):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç –∞–Ω–≥–ª–∏–π—Å–∫–∏–º"""
        try:
            blob = TextBlob(text)
            return blob.detect_language() == 'en'
        except:
            # –ï—Å–ª–∏ TextBlob –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç—É—é —ç–≤—Ä–∏—Å—Ç–∏–∫—É
            english_words = ['the', 'and', 'of', 'to', 'a', 'in', 'is', 'it', 'you', 'that']
            count = sum(1 for word in english_words if word in text.lower().split())
            return count > 2
    
    def translate_to_russian(self, text):
        """–ü–µ—Ä–µ–≤–æ–¥–∏—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —Ä—É—Å—Å–∫–∏–π (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)"""
        # –ö—ç—à–∏—Ä—É–µ–º –ø–µ—Ä–µ–≤–æ–¥—ã
        text_hash = hashlib.md5(text.encode()).hexdigest()
        if text_hash in self.translation_cache:
            return self.translation_cache[text_hash]
        
        # –ü—Ä–æ—Å—Ç–æ–π –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∞–≤–∏–ª
        translation_rules = {
            'release': '—Ä–µ–ª–∏–∑',
            'collection': '–∫–æ–ª–ª–µ–∫—Ü–∏—è',
            'collaboration': '–∫–æ–ª–ª–∞–±–æ—Ä–∞—Ü–∏—è',
            'sneakers': '–∫—Ä–æ—Å—Å–æ–≤–∫–∏',
            'limited': '–ª–∏–º–∏—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π',
            'edition': '–∏–∑–¥–∞–Ω–∏–µ',
            'exclusive': '—ç–∫—Å–∫–ª—é–∑–∏–≤–Ω—ã–π',
            'new': '–Ω–æ–≤—ã–π',
            'designer': '–¥–∏–∑–∞–π–Ω–µ—Ä',
            'fashion': '–º–æ–¥–∞',
            'streetwear': '—Å—Ç—Ä–∏—Ç–≤–∏—Ä',
            'luxury': '–ª—é–∫—Å',
            'brand': '–±—Ä–µ–Ω–¥',
            'drop': '–¥—Ä–æ–ø',
            'capsule': '–∫–∞–ø—Å—É–ª–∞',
            'season': '—Å–µ–∑–æ–Ω',
            'available': '–¥–æ—Å—Ç—É–ø–µ–Ω',
            'price': '—Ü–µ–Ω–∞',
            'colorway': '—Ä–∞—Å—Ü–≤–µ—Ç–∫–∞',
            'material': '–º–∞—Ç–µ—Ä–∏–∞–ª',
            'leather': '–∫–æ–∂–∞',
            'suede': '–∑–∞–º—à–∞',
            'mesh': '—Å–µ—Ç–∫–∞',
            'rubber': '—Ä–µ–∑–∏–Ω–∞',
        }
        
        translated = text
        for eng, rus in translation_rules.items():
            translated = re.sub(rf'\b{eng}\b', rus, translated, flags=re.IGNORECASE)
            
        self.translation_cache[text_hash] = translated
        return translated
    
    def improve_writing_style(self, content, brand):
        """–£–ª—É—á—à–∞–µ—Ç —Å—Ç–∏–ª—å –Ω–∞–ø–∏—Å–∞–Ω–∏—è –≤ —Å—Ç–∏–ª–µ –¢–æ–ø–æ—Ä–∞"""
        # –î–µ–ª–∞–µ–º –±–æ–ª–µ–µ —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã–º
        content = content.replace('–ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç', '–≤—ã–∫–∞—Ç—ã–≤–∞–µ—Ç')
        content = content.replace('–∞–Ω–æ–Ω—Å–∏—Ä—É–µ—Ç', '—Ä–∞—Å—Å–∫–∞–∑—ã–≤–∞–µ—Ç –ø—Ä–æ')
        content = content.replace('–∫–æ–ª–ª–µ–∫—Ü–∏—è', '–Ω–æ–≤–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è')
        
        # –î–æ–±–∞–≤–ª—è–µ–º —ç–º–æ—Ü–∏–∏
        emotional_words = ['üî•', 'üí•', 'üëÄ', '‚ú®']
        if random.random() > 0.7:
            content = emotional_words[0] + ' ' + content
            
        return content
    
    def add_viral_elements(self, content, brand):
        """–î–æ–±–∞–≤–ª—è–µ—Ç –≤–∏—Ä–∞–ª—å–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã"""
        viral_phrases = [
            "–≠—Ç–æ —Ç–æ—á–Ω–æ –≤–∑–æ—Ä–≤–µ—Ç –∏–Ω—Ç–µ—Ä–Ω–µ—Ç—ã!",
            "–ñ–¥–µ–º, –∫–æ–≥–¥–∞ –ø–æ—è–≤–∏—Ç—Å—è –≤ –ø—Ä–æ–¥–∞–∂–µ!",
            "–ß—Ç–æ –¥—É–º–∞–µ—Ç–µ –æ –Ω–æ–≤–∏–Ω–∫–µ?",
            "–ù–∞—Å–∫–æ–ª—å–∫–æ —ç—Ç–æ –≤–æ–æ–±—â–µ –∫—Ä—É—Ç–æ?",
            "–ë—É–¥–µ—Ç–µ –±—Ä–∞—Ç—å?",
        ]
        
        if random.random() > 0.5:
            content += " " + random.choice(viral_phrases)
            
        return content

class DatabaseManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self):
        self.init_database()
    
    @contextmanager
    def get_db_connection(self):
        conn = sqlite3.connect('fashion_news.db')
        try:
            yield conn
        finally:
            conn.close()
    
    def init_database(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
        with self.get_db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS sent_posts (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    post_hash TEXT UNIQUE,
                    brand TEXT,
                    title TEXT,
                    engagement_score INTEGER,
                    sent_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_post_hash ON sent_posts(post_hash)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_sent_date ON sent_posts(sent_date)')
            conn.commit()
    
    def is_post_sent(self, post_hash):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –±—ã–ª –ª–∏ –ø–æ—Å—Ç —É–∂–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω"""
        with self.get_db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute('SELECT 1 FROM sent_posts WHERE post_hash = ?', (post_hash,))
            return cursor.fetchone() is not None
    
    def mark_post_sent(self, post_hash, brand, title, engagement_score):
        """–ü–æ–º–µ—á–∞–µ—Ç –ø–æ—Å—Ç –∫–∞–∫ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π"""
        with self.get_db_connection() as conn:
            cursor = conn.cursor()
            try:
                cursor.execute(
                    'INSERT INTO sent_posts (post_hash, brand, title, engagement_score) VALUES (?, ?, ?, ?)',
                    (post_hash, brand, title[:200], engagement_score)
                )
                conn.commit()
            except sqlite3.IntegrityError:
                pass
    
    def cleanup_old_posts(self, days=3):
        """–û—á–∏—â–∞–µ—Ç —Å—Ç–∞—Ä—ã–µ –ø–æ—Å—Ç—ã"""
        with self.get_db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute('DELETE FROM sent_posts WHERE sent_date < datetime("now", ?)', (f"-{days} days",))
            conn.commit()

class TelegramBot:
    """Telegram –±–æ—Ç –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –ø–æ—Å—Ç–æ–≤"""
    
    def __init__(self, token, channel):
        self.token = token
        self.channel = channel
        self.session = requests.Session()
    
    def send_post(self, content, image_url=None):
        """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –ø–æ—Å—Ç –≤ Telegram –∫–∞–Ω–∞–ª"""
        try:
            if image_url and self.is_valid_image(image_url):
                return self.send_photo(content, image_url)
            else:
                return self.send_text(content)
        except Exception as e:
            logger.error(f"‚ùå Telegram send error: {e}")
            return False
    
    def is_valid_image(self, image_url):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        try:
            response = self.session.head(image_url, timeout=5, verify=False)
            content_type = response.headers.get('content-type', '')
            return response.status_code == 200 and content_type.startswith('image/')
        except:
            return False
    
    def send_photo(self, caption, photo_url):
        """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Ñ–æ—Ç–æ —Å –ø–æ–¥–ø–∏—Å—å—é"""
        url = f'https://api.telegram.org/bot{self.token}/sendPhoto'
        
        # –°–∫–∞—á–∏–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        try:
            response = self.session.get(photo_url, timeout=10, verify=False)
            if response.status_code != 200:
                return self.send_text(caption)
                
            files = {'photo': ('image.jpg', response.content, 'image/jpeg')}
            data = {
                'chat_id': self.channel,
                'caption': caption,
                'parse_mode': 'HTML'
            }
            
            response = self.session.post(url, files=files, data=data, timeout=30)
            return response.status_code == 200
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Photo send failed, falling back to text: {e}")
            return self.send_text(caption)
    
    def send_text(self, text):
        """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ—Å—Ç"""
        url = f'https://api.telegram.org/bot{self.token}/sendMessage'
        data = {
            'chat_id': self.channel,
            'text': text,
            'parse_mode': 'HTML',
            'disable_web_page_preview': False
        }
        
        response = self.session.post(url, json=data, timeout=30)
        return response.status_code == 200

class FashionNewsBot:
    """–ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å –±–æ—Ç–∞ –º–æ–¥–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π"""
    
    def __init__(self):
        self.db = DatabaseManager()
        self.aggregator = AdvancedNewsAggregator()
        self.enhancer = SmartContentEnhancer()
        self.bot = TelegramBot(BOT_TOKEN, CHANNEL)
        self.formatter = ToporStyleFormatter()
        
    def generate_post_hash(self, content, brand):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ö—ç—à –ø–æ—Å—Ç–∞"""
        return hashlib.md5(f"{content}_{brand}".encode()).hexdigest()
    
    def find_best_news(self):
        """–ò—â–µ—Ç –ª—É—á—à—É—é –Ω–æ–≤–æ—Å—Ç—å –¥–ª—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏"""
        logger.info("üéØ Searching for the most viral content...")
        
        viral_news = self.aggregator.find_most_viral_content()
        if not viral_news:
            logger.warning("‚ö†Ô∏è No viral news found, generating curated content...")
            return self.generate_curated_content()
        
        # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à—É—é –Ω–æ–≤–æ—Å—Ç—å
        best_news = viral_news[0]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –ø—É–±–ª–∏–∫–æ–≤–∞–ª–∏ –ª–∏ —É–∂–µ
        post_hash = self.generate_post_hash(best_news['content'], best_news['brand'])
        if self.db.is_post_sent(post_hash):
            logger.info("‚è≠Ô∏è This news was already posted, trying next...")
            if len(viral_news) > 1:
                best_news = viral_news[1]
                post_hash = self.generate_post_hash(best_news['content'], best_news['brand'])
                if self.db.is_post_sent(post_hash) and len(viral_news) > 2:
                    best_news = viral_news[2]
                    post_hash = self.generate_post_hash(best_news['content'], best_news['brand'])
        
        if self.db.is_post_sent(post_hash):
            logger.warning("‚ö†Ô∏è All recent news already posted, generating curated content...")
            return self.generate_curated_content()
            
        return best_news, post_hash
    
    def generate_curated_content(self):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫—É—Ä–∏—Ä—É–µ–º—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –∫–æ–≥–¥–∞ –Ω–µ—Ç –Ω–æ–≤–æ—Å—Ç–µ–π"""
        brands = ['Nike', 'Adidas', 'Supreme', 'Balenciaga', 'Gucci']
        brand = random.choice(brands)
        
        curated_templates = [
            f"{brand} –≥–æ—Ç–æ–≤–∏—Ç —Å—é—Ä–ø—Ä–∏–∑ –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π —Å–µ–∑–æ–Ω. –ò–Ω—Å–∞–π–¥–µ—Ä—ã –≥–æ–≤–æ—Ä—è—Ç –æ –∫–æ–ª–ª–∞–±–æ—Ä–∞—Ü–∏–∏ —Å –∏–∑–≤–µ—Å—Ç–Ω—ã–º –±—Ä–µ–Ω–¥–æ–º.",
            f"–°–ª—É—Ö–∏: {brand} —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞–¥ –Ω–æ–≤–æ–π –∫–∞–ø—Å—É–ª—å–Ω–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–µ–π. –û–∂–∏–¥–∞–µ—Ç—Å—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π —Ç–∏—Ä–∞–∂.",
            f"{brand} –º–æ–∂–µ—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç—å –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫—É–ª—å—Ç–æ–≤–æ–π –º–æ–¥–µ–ª–∏. –§–∞–Ω–∞—Ç—ã –∂–¥—É—Ç —Å –Ω–µ—Ç–µ—Ä–ø–µ–Ω–∏–µ–º.",
            f"–í —Å–µ—Ç–∏ –ø–æ—è–≤–∏–ª–∏—Å—å –ø–µ—Ä–≤—ã–µ —Ç–∏–∑–µ—Ä—ã –Ω–æ–≤–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏ {brand}. –í—ã–≥–ª—è–¥–∏—Ç –º–Ω–æ–≥–æ–æ–±–µ—â–∞—é—â–µ!",
        ]
        
        content = random.choice(curated_templates)
        engagement_score = random.randint(60, 85)
        
        curated_news = {
            'title': f"{brand} | –°–≤–µ–∂–∏–µ —Å–ª—É—Ö–∏",
            'content': content,
            'brand': brand,
            'engagement_score': engagement_score,
            'image_url': None,
            'source': 'Curated'
        }
        
        post_hash = self.generate_post_hash(content, brand)
        return curated_news, post_hash
    
    def create_final_post(self, news_item):
        """–°–æ–∑–¥–∞–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –ø–æ—Å—Ç –¥–ª—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏"""
        brand = news_item['brand']
        original_title = news_item['original_title']
        content = news_item['content']
        
        # –£–ª—É—á—à–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
        enhanced_content = self.enhancer.enhance_content(content, brand)
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫
        title = self.create_catchy_title(original_title, brand)
        
        # –í—ã–±–∏—Ä–∞–µ–º —Å—Ç–∏–ª—å –ø–æ—Å—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ engagement score
        engagement_score = news_item['engagement_score']
        
        if engagement_score >= 80:
            post_content = self.formatter.create_viral_post(brand, title, enhanced_content, engagement_score)
        else:
            post_content = self.formatter.create_news_post(brand, title, enhanced_content, engagement_score)
            
        return post_content
    
    def create_catchy_title(self, original_title, brand):
        """–°–æ–∑–¥–∞–µ—Ç —Ü–µ–ø–ª—è—é—â–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫"""
        # –£–ø—Ä–æ—â–∞–µ–º –∏ –¥–µ–ª–∞–µ–º –±–æ–ª–µ–µ –≤–∏—Ä–∞–ª—å–Ω—ã–º
        title_variations = [
            f"{brand} –∑–∞–ø—É—Å–∫–∞–µ—Ç –Ω–æ–≤—ã–π –¥—Ä–æ–ø",
            f"–ù–æ–≤–∏–Ω–∫–∞ –æ—Ç {brand} —É–∂–µ –∑–¥–µ—Å—å",
            f"{brand} –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç: —Å–º–æ—Ç—Ä–∏—Ç–µ –ø–µ—Ä–≤—ã–º–∏",
            f"–•–∏—Ç —Å–µ–∑–æ–Ω–∞ –æ—Ç {brand}",
            f"{brand} —É–¥–∏–≤–ª—è–µ—Ç –Ω–æ–≤—ã–º —Ä–µ–ª–∏–∑–æ–º",
            f"–ù–µ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç–µ: –Ω–æ–≤—ã–π —Ä–µ–ª–∏–∑ {brand}",
            f"{brand} –∞–Ω–æ–Ω—Å–∏—Ä—É–µ—Ç –∫–æ–ª–ª–∞–±–æ—Ä–∞—Ü–∏—é",
        ]
        
        return random.choice(title_variations)
    
    def run(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –±–æ—Ç–∞"""
        logger.info("üöÄ Starting Fashion News Bot (Topor Style)")
        
        # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ –ø–æ—Å—Ç—ã
        self.db.cleanup_old_posts(days=3)
        
        # –ò—â–µ–º –ª—É—á—à—É—é –Ω–æ–≤–æ—Å—Ç—å
        news_item, post_hash = self.find_best_news()
        
        if not news_item:
            logger.error("‚ùå No content found for posting")
            return False
        
        # –°–æ–∑–¥–∞–µ–º –ø–æ—Å—Ç
        post_content = self.create_final_post(news_item)
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –ø–æ—Å—Ç
        logger.info(f"üì§ Posting about {news_item['brand']} (engagement: {news_item['engagement_score']})")
        
        success = self.bot.send_post(post_content, news_item.get('image_url'))
        
        if success:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±–∞–∑—É
            self.db.mark_post_sent(
                post_hash, 
                news_item['brand'], 
                news_item['title'], 
                news_item['engagement_score']
            )
            logger.info("‚úÖ Post sent successfully!")
            return True
        else:
            logger.error("‚ùå Failed to send post")
            return False

if __name__ == "__main__":
    try:
        bot = FashionNewsBot()
        success = bot.run()
        
        if success:
            logger.info("üéâ Bot finished successfully!")
        else:
            logger.error("üí• Bot finished with errors")
            
    except Exception as e:
        logger.error(f"üí• Critical error: {e}")

import requests
import os
import re
import random
from bs4 import BeautifulSoup
import feedparser
from datetime import datetime

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
BOT_TOKEN = os.environ['BOT_TOKEN']
CHANNEL = os.environ['CHANNEL']

# –†—É—Å—Å–∫–∏–µ –°–ú–ò –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
SOURCES = [
    {'name': '–†–ë–ö', 'url': 'https://www.rbc.ru/rbcfreenews/', 'category': 'style'},
    {'name': '–ö–æ–º–º–µ—Ä—Å–∞–Ω—Ç', 'url': 'https://www.kommersant.ru/RSS/news.xml', 'category': 'lifestyle'},
    {'name': 'Forbes', 'url': 'https://www.forbes.ru/newrss.xml', 'category': 'lifestyle'},
    {'name': 'Buro 24/7', 'url': 'https://www.buro247.ru/news/fashion/', 'category': 'fashion'}
]

def translate_keywords(text):
    """–ü–µ—Ä–µ–≤–æ–¥ –∫–ª—é—á–µ–≤—ã—Ö –º–æ–¥–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤"""
    translations = {
        'fashion': '–º–æ–¥–∞', 'style': '—Å—Ç–∏–ª—å', 'trend': '—Ç—Ä–µ–Ω–¥', 'collection': '–∫–æ–ª–ª–µ–∫—Ü–∏—è',
        'designer': '–¥–∏–∑–∞–π–Ω–µ—Ä', 'luxury': '–ª—é–∫—Å', 'runway': '–ø–æ–∫–∞–∑', 'model': '–º–æ–¥–µ–ª—å',
        'brand': '–±—Ä–µ–Ω–¥', 'new': '–Ω–æ–≤—ã–π', 'exclusive': '—ç–∫—Å–∫–ª—é–∑–∏–≤'
    }
    for eng, rus in translations.items():
        text = re.sub(r'\b' + eng + r'\b', rus, text, flags=re.IGNORECASE)
    return text

def extract_article_content(url):
    """–ü–∞—Ä—Å–∏—Ç –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ —Å –∫–∞—Ä—Ç–∏–Ω–∫–æ–π"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
        title = soup.find('h1')
        title = title.get_text().strip() if title else "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–∞—Ä—Ç–∏–Ω–∫—É
        image = soup.find('meta', property='og:image')
        image_url = image['content'] if image else None
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç (–ø–µ—Ä–≤—ã–µ 2 –∞–±–∑–∞—Ü–∞)
        content = ""
        paragraphs = soup.find_all('p')[:3]
        for p in paragraphs:
            text = p.get_text().strip()
            if len(text) > 50:  # –¢–æ–ª—å–∫–æ –∑–Ω–∞—á–∏–º—ã–µ –∞–±–∑–∞—Ü—ã
                content += text + "\n\n"
        
        return {
            'title': title,
            'content': content[:500] + '...' if len(content) > 500 else content,
            'image': image_url,
            'url': url
        }
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
        return None

def create_beautiful_post(article, source_name):
    """–°–æ–∑–¥–∞–µ—Ç –∫—Ä–∞—Å–∏–≤—ã–π –ø–æ—Å—Ç –≤ —Å—Ç–∏–ª–µ '–¢–æ–ø–æ—Ä'"""
    
    # –≠–º–æ–¥–∑–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
    emojis = {
        'fashion': 'üëó', 'style': 'üíé', 'business': 'üìà', 
        'lifestyle': 'üåü', 'news': 'üì∞'
    }
    
    emoji = emojis.get('fashion', 'üìå')
    
    # –ü–µ—Ä–µ–≤–æ–¥ –∑–∞–≥–æ–ª–æ–≤–∫–∞
    title = translate_keywords(article['title'])
    
    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –ø–æ—Å—Ç
    post = f"{emoji} <b>{title}</b>\n\n"
    
    if article['content']:
        post += f"üìñ {article['content']}\n\n"
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫ –∏ –≤—Ä–µ–º—è
    post += f"üì∞ <i>{source_name}</i>\n"
    post += f"üïí {datetime.now().strftime('%H:%M')}\n\n"
    
    # –•–µ—à—Ç–µ–≥–∏
    post += "#–º–æ–¥–∞ #—Ç—Ä–µ–Ω–¥—ã #–Ω–æ–≤–æ—Å—Ç–∏ #—Å—Ç–∏–ª—å"
    
    # –î–æ–±–∞–≤–ª—è–µ–º –±—Ä–µ–Ω–¥–æ–≤—ã–µ —Ö–µ—à—Ç–µ–≥–∏ –µ—Å–ª–∏ –µ—Å—Ç—å –≤ —Ç–µ–∫—Å—Ç–µ
    brands = ['gucci', 'dior', 'chanel', 'prada', 'balenciaga', 'versace']
    title_lower = title.lower()
    for brand in brands:
        if brand in title_lower:
            post += f" #{brand}"
    
    return post, article['image']

def send_telegram_post(text, image_url=None):
    """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –ø–æ—Å—Ç –≤ Telegram"""
    if image_url:
        # –ü—Ä–æ–±—É–µ–º –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —Å –∫–∞—Ä—Ç–∏–Ω–∫–æ–π
        url = f'https://api.telegram.org/bot{BOT_TOKEN}/sendPhoto'
        data = {
            'chat_id': CHANNEL,
            'caption': text,
            'parse_mode': 'HTML'
        }
        files = {'photo': requests.get(image_url).content}
        response = requests.post(url, data=data, files=files)
    else:
        # –ë–µ–∑ –∫–∞—Ä—Ç–∏–Ω–∫–∏
        url = f'https://api.telegram.org/bot{BOT_TOKEN}/sendMessage'
        data = {
            'chat_id': CHANNEL,
            'text': text,
            'parse_mode': 'HTML'
        }
        response = requests.post(url, data=data)
    
    return response.status_code == 200

def find_fashion_news():
    """–ò—â–µ—Ç –º–æ–¥–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –≤ RSS –ª–µ–Ω—Ç–∞—Ö"""
    for source in SOURCES:
        try:
            print(f"üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º {source['name']}...")
            
            # –ü–∞—Ä—Å–∏–º RSS
            feed = feedparser.parse(source['url'])
            
            if not feed.entries:
                continue
            
            # –ò—â–µ–º –ø–æ–¥—Ö–æ–¥—è—â–∏–µ —Å—Ç–∞—Ç—å–∏
            for entry in feed.entries[:5]:
                title = getattr(entry, 'title', '')
                link = getattr(entry, 'link', '')
                
                # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
                keywords = ['–º–æ–¥–∞', '—Å—Ç–∏–ª—å', '–¥–∏–∑–∞–π–Ω–µ—Ä', '–∫–æ–ª–ª–µ–∫—Ü–∏—è', '–ø–æ–∫–∞–∑', 
                           'Gucci', 'Dior', 'Chanel', 'Prada', '–±—Ä–µ–Ω–¥']
                
                if any(keyword.lower() in title.lower() for keyword in keywords):
                    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–∞ –ø–æ–¥—Ö–æ–¥—è—â–∞—è –Ω–æ–≤–æ—Å—Ç—å: {title}")
                    
                    # –ü–∞—Ä—Å–∏–º –ø–æ–ª–Ω—É—é —Å—Ç–∞—Ç—å—é
                    article = extract_article_content(link)
                    if article and article['content']:
                        # –°–æ–∑–¥–∞–µ–º –∫—Ä–∞—Å–∏–≤—ã–π –ø–æ—Å—Ç
                        post_text, image_url = create_beautiful_post(article, source['name'])
                        
                        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ –∫–∞–Ω–∞–ª
                        if send_telegram_post(post_text, image_url):
                            print(f"‚úÖ –ü–æ—Å—Ç –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω: {title}")
                            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å {source['name']}: {e}")
    
    return False

def send_backup_news():
    """–†–µ–∑–µ—Ä–≤–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç - –ª—é–±–∞—è –∏–Ω—Ç–µ—Ä–µ—Å–Ω–∞—è –Ω–æ–≤–æ—Å—Ç—å"""
    backup_feeds = [
        'https://lenta.ru/rss/news',
        'https://www.vedomosti.ru/rss/news',
        'https://www.rbc.ru/rbcfreenews/'
    ]
    
    for feed_url in backup_feeds:
        try:
            feed = feedparser.parse(feed_url)
            if feed.entries:
                entry = feed.entries[0]
                
                # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–π –ø–æ—Å—Ç
                title = translate_keywords(entry.title)
                post = f"üìå <b>{title}</b>\n\n"
                post += f"üîó {entry.link}\n\n"
                post += "#–Ω–æ–≤–æ—Å—Ç–∏ #—Ç—Ä–µ–Ω–¥—ã #–∞–∫—Ç—É–∞–ª—å–Ω–æ–µ"
                
                if send_telegram_post(post):
                    print(f"‚úÖ –†–µ–∑–µ—Ä–≤–Ω—ã–π –ø–æ—Å—Ç –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω: {title}")
                    return True
        except:
            continue
    
    return False

if __name__ == "__main__":
    print("üöÄ –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞ –º–æ–¥–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π...")
    
    # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –º–æ–¥–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
    if not find_fashion_news():
        print("‚ùå –ú–æ–¥–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –ø—Ä–æ–±—É–µ–º —Ä–µ–∑–µ—Ä–≤...")
        send_backup_news()

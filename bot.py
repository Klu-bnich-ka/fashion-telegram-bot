"""
Fashion News Bot - Stable RSS Edition
Author: Gemini AI
"""

import os
import re
import time
import hashlib
import sqlite3
import logging
import requests
import feedparser
import random
from typing import List, Optional
from dataclasses import dataclass
from urllib.parse import urljoin
from bs4 import BeautifulSoup
from deep_translator import GoogleTranslator
from fake_useragent import UserAgent

# ================= CONFIG =================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger("Bot")

BOT_TOKEN = os.environ.get('BOT_TOKEN')
CHANNEL = os.environ.get('CHANNEL')
DB_NAME = 'news.db'

# –°–ø–∏—Å–æ–∫ –Ω–∞–¥–µ–∂–Ω—ã—Ö RSS –ª–µ–Ω—Ç (–ú–æ–¥–∞)
RSS_SOURCES = [
    # Vogue (–û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π RSS)
    {'name': 'Vogue', 'url': 'https://www.vogue.com/feed/rss'},
    # Fashionista (–û—á–µ–Ω—å –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π —Å–∞–π—Ç)
    {'name': 'Fashionista', 'url': 'https://fashionista.com/.rss/full/'},
    # The Guardian Fashion (–í—Å–µ–≥–¥–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç)
    {'name': 'Guardian', 'url': 'https://www.theguardian.com/fashion/rss'},
    # Hypebeast (RSS –≤–µ—Ä—Å–∏—è, –µ–µ –Ω–µ –±–ª–æ—á–∞—Ç)
    {'name': 'Hypebeast', 'url': 'https://hypebeast.com/fashion/feed'}
]

@dataclass
class Article:
    title: str
    url: str
    content: str
    images: List[str]
    source: str

# ================= DATABASE =================
class Database:
    def __init__(self):
        self.conn = sqlite3.connect(DB_NAME)
        self.cursor = self.conn.cursor()
        self.cursor.execute('''
            CREATE TABLE IF NOT EXISTS history (
                hash TEXT PRIMARY KEY,
                title TEXT,
                date TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        self.conn.commit()

    def exists(self, url):
        h = hashlib.md5(url.encode()).hexdigest()
        res = self.cursor.execute('SELECT 1 FROM history WHERE hash = ?', (h,)).fetchone()
        return res is not None

    def add(self, url, title):
        h = hashlib.md5(url.encode()).hexdigest()
        try:
            self.cursor.execute('INSERT INTO history (hash, title) VALUES (?, ?)', (h, title))
            self.conn.commit()
        except:
            pass

# ================= TOOLS =================
class TextSanitizer:
    @staticmethod
    def clean(text):
        if not text: return ""
        # –£–±–∏—Ä–∞–µ–º HTML —Ç–µ–≥–∏
        text = BeautifulSoup(text, "lxml").get_text(separator=' ')
        # –£–±–∏—Ä–∞–µ–º –º—É—Å–æ—Ä
        bad_phrases = ['Read more', 'Source:', 'Photo:', 'Courtesy of', 'Click here', 'Subscribe']
        for phrase in bad_phrases:
            text = re.sub(phrase, '', text, flags=re.IGNORECASE)
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r'\s+', ' ', text).strip()
        return text

class Extractor:
    def __init__(self):
        self.ua = UserAgent()
        self.session = requests.Session()
        self.translator = GoogleTranslator(source='auto', target='ru')

    def get_full_content(self, url):
        """–ó–∞—Ö–æ–¥–∏—Ç –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É, —á—Ç–æ–±—ã –Ω–∞–π—Ç–∏ –∫–∞—Ä—Ç–∏–Ω–∫–∏ –∏ –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç"""
        try:
            headers = {'User-Agent': self.ua.random}
            resp = self.session.get(url, headers=headers, timeout=10)
            if resp.status_code != 200:
                return None, []
            
            soup = BeautifulSoup(resp.content, 'lxml')
            
            # --- –ü–æ–∏—Å–∫ –∫–∞—Ä—Ç–∏–Ω–æ–∫ ---
            images = []
            # –ò—â–µ–º –±–æ–ª—å—à–∏–µ –∫–∞—Ä—Ç–∏–Ω–∫–∏ –≤ —Ç–µ–ª–µ —Å—Ç–∞—Ç—å–∏
            for img in soup.find_all('img'):
                src = img.get('src') or img.get('data-src')
                if not src: continue
                if src.startswith('//'): src = 'https:' + src
                if not src.startswith('http'): src = urljoin(url, src)
                
                # –§–∏–ª—å—Ç—Ä –º–µ–ª–∫–∏—Ö –∏–∫–æ–Ω–æ–∫
                if any(x in src.lower() for x in ['logo', 'avatar', 'icon', 'svg', 'pixel']):
                    continue
                if src not in images:
                    images.append(src)
            
            # --- –ü–æ–∏—Å–∫ —Ç–µ–∫—Å—Ç–∞ (–µ—Å–ª–∏ RSS –¥–∞–ª –º–∞–ª–æ) ---
            # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä
            article_body = soup.find('article') or soup.find('main') or soup.find('div', class_=re.compile('content|post'))
            full_text = ""
            if article_body:
                paragraphs = article_body.find_all('p')
                # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 5 –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
                text_parts = [p.get_text() for p in paragraphs[:5] if len(p.get_text()) > 50]
                full_text = " ".join(text_parts)

            return full_text, images[:3] # –ú–∞–∫—Å 3 –∫–∞—Ä—Ç–∏–Ω–∫–∏
        except Exception as e:
            logger.error(f"Extractor error: {e}")
            return None, []

    def translate(self, text):
        try:
            if len(text) > 4000: text = text[:4000]
            return self.translator.translate(text)
        except:
            return text

# ================= TELEGRAM =================
class TelegramSender:
    def __init__(self):
        self.api = f"https://api.telegram.org/bot{BOT_TOKEN}"

    def send(self, article: Article):
        # –ü–æ–¥–ø–∏—Å—å
        caption = f"<b>{article.title}</b>\n\n{article.content[:900]}"
        if len(article.content) > 900: caption += "..."
        
        data = {'chat_id': CHANNEL, 'parse_mode': 'HTML'}
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å –∫–∞—Ä—Ç–∏–Ω–∫–∏, —à–ª–µ–º –∞–ª—å–±–æ–º (–∏–ª–∏ –æ–¥–Ω—É)
        if article.images:
            if len(article.images) == 1:
                data['photo'] = article.images[0]
                data['caption'] = caption
                requests.post(f"{self.api}/sendPhoto", json=data)
                return True
            else:
                # –ê–ª—å–±–æ–º
                media = []
                for i, img in enumerate(article.images):
                    item = {'type': 'photo', 'media': img}
                    if i == 0: 
                        item['caption'] = caption
                        item['parse_mode'] = 'HTML'
                    media.append(item)
                r = requests.post(f"{self.api}/sendMediaGroup", json={'chat_id': CHANNEL, 'media': media})
                if r.status_code == 200: return True
                # –ï—Å–ª–∏ –∞–ª—å–±–æ–º –Ω–µ –ø—Ä–æ—à–µ–ª (–±–∏—Ç–∞—è –∫–∞—Ä—Ç–∏–Ω–∫–∞), –ø—Ä–æ–±—É–µ–º –ø—Ä–æ—Å—Ç–æ —Ç–µ–∫—Å—Ç
        
        # –§–æ–ª–±–µ–∫: –ø—Ä–æ—Å—Ç–æ —Ç–µ–∫—Å—Ç
        data['text'] = caption
        requests.post(f"{self.api}/sendMessage", json=data)
        return True

# ================= MAIN =================
def run():
    logger.info("üöÄ Bot started (RSS Mode)")
    if not BOT_TOKEN or not CHANNEL:
        logger.error("No token/channel")
        return

    db = Database()
    extractor = Extractor()
    sender = TelegramSender()
    sanitizer = TextSanitizer()

    # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
    random.shuffle(RSS_SOURCES)
    
    news_sent = 0
    
    for source in RSS_SOURCES:
        if news_sent >= 1: break # 1 –Ω–æ–≤–æ—Å—Ç—å –∑–∞ –∑–∞–ø—É—Å–∫

        logger.info(f"üì° Checking {source['name']}...")
        try:
            feed = feedparser.parse(source['url'])
            if not feed.entries:
                logger.warning(f"Empty feed for {source['name']}")
                continue

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–æ–≤–æ—Å—Ç–∏
            for entry in feed.entries[:3]:
                url = entry.link
                title = entry.title
                
                if db.exists(url):
                    continue
                
                logger.info(f"Found new: {title}")
                
                # 1. –î–æ—Å—Ç–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
                # –°–Ω–∞—á–∞–ª–∞ –±–µ—Ä–µ–º —Ç–æ, —á—Ç–æ –≤ RSS
                raw_summary = getattr(entry, 'summary', '') or getattr(entry, 'description', '')
                
                # –ü—ã—Ç–∞–µ–º—Å—è —É–ª—É—á—à–∏—Ç—å (—Å–∫–∞—á–∞—Ç—å —Å —Å–∞–π—Ç–∞)
                site_text, site_images = extractor.get_full_content(url)
                
                # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–∏–π —Ç–µ–∫—Å—Ç (—Å —Å–∞–π—Ç–∞ –∏–ª–∏ –∏–∑ RSS)
                content_en = site_text if site_text and len(site_text) > len(raw_summary) else raw_summary
                
                # –ß–∏—Å—Ç–∏–º
                content_en = sanitizer.clean(content_en)
                if len(content_en) < 100: # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ
                    logger.info("Content too short, skipping")
                    continue

                # 2. –ü–µ—Ä–µ–≤–æ–¥–∏–º
                logger.info("Translating...")
                title_ru = extractor.translate(title)
                content_ru = extractor.translate(content_en)
                
                # 3. –û—Ç–ø—Ä–∞–≤–ª—è–µ–º
                article = Article(title_ru, url, content_ru, site_images, source['name'])
                if sender.send(article):
                    logger.info("‚úÖ Sent!")
                    db.add(url, title)
                    news_sent += 1
                    break # –í—ã—Ö–æ–¥–∏–º –∏–∑ —Ü–∏–∫–ª–∞ —Å—Ç–∞—Ç–µ–π, –∏–¥–µ–º –∫ —Å–ª–µ–¥—É—é—â–µ–º—É –∑–∞–ø—É—Å–∫—É
                else:
                    logger.error("Failed to send")
                
                time.sleep(5)

        except Exception as e:
            logger.error(f"Error parsing {source['name']}: {e}")

if __name__ == "__main__":
    run()

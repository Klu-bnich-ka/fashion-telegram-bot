import requests
import os
import re
import random
from bs4 import BeautifulSoup
import feedparser
from datetime import datetime
import time
import json
import logging
from urllib.parse import urljoin

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
BOT_TOKEN = os.environ['BOT_TOKEN']
CHANNEL = os.environ['CHANNEL']

# –°—Ç–∏–ª–∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è Telegram
class TextStyler:
    @staticmethod
    def bold(text):
        return f"<b>{text}</b>"
    
    @staticmethod
    def italic(text):
        return f"<i>{text}</i>"
    
    @staticmethod
    def underline(text):
        return f"<u>{text}</u>"
    
    @staticmethod
    def strikethrough(text):
        return f"<s>{text}</s>"
    
    @staticmethod
    def code(text):
        return f"<code>{text}</code>"
    
    @staticmethod
    def highlight_keywords(text, keywords):
        """–í—ã–¥–µ–ª—è–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ —Ç–µ–∫—Å—Ç–µ"""
        for keyword in keywords:
            if keyword.lower() in text.lower():
                text = text.replace(keyword, TextStyler.bold(keyword))
                text = text.replace(keyword.lower(), TextStyler.bold(keyword))
                text = text.replace(keyword.upper(), TextStyler.bold(keyword))
        return text
    
    @staticmethod
    def create_header(text, emoji="‚ú®"):
        return f"{emoji} {TextStyler.bold(text.upper())}"
    
    @staticmethod
    def create_quote(text, author=""):
        quote = f"‚ùù{text}‚ùû"
        if author:
            quote += f"\n\n‚Äî {TextStyler.italic(author)}"
        return quote

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç–∏–ª–µ—Ä–∞
styler = TextStyler()

# –≠–º–æ–¥–∑–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
CONTENT_EMOJIS = {
    'collection': 'üëó',
    'sneakers': 'üëü', 
    'collaboration': 'ü§ù',
    'luxury': 'üíé',
    'streetwear': 'üèôÔ∏è',
    'vintage': 'üï∞Ô∏è',
    'show': 'üé™',
    'campaign': 'üì∏',
    'exclusive': 'üîí',
    'limited': 'üè∑Ô∏è',
    'innovation': 'üöÄ',
    'sustainable': 'üå±'
}

# –ë–ê–ó–ê –ò–°–¢–û–ß–ù–ò–ö–û–í
SOURCES = [
    {'name': 'Vogue', 'url': 'https://www.vogue.com/rss', 'lang': 'en'},
    {'name': 'Business of Fashion', 'url': 'https://www.businessoffashion.com/feed', 'lang': 'en'},
    {'name': 'Hypebeast', 'url': 'https://hypebeast.com/fashion/feed', 'lang': 'en'},
    {'name': 'Highsnobiety', 'url': 'https://www.highsnobiety.com/feed/', 'lang': 'en'},
    {'name': 'Fashionista', 'url': 'https://fashionista.com/.rss', 'lang': 'en'},
    {'name': 'WWD', 'url': 'https://wwd.com/feed/', 'lang': 'en'},
    {'name': 'The Cut', 'url': 'https://www.thecut.com/rss/index.xml', 'lang': 'en'},
    {'name': 'Complex', 'url': 'https://www.complex.com/feeds/style', 'lang': 'en'},
    {'name': 'Sneaker News', 'url': 'https://sneakernews.com/feed/', 'lang': 'en'},
    {'name': 'Nice Kicks', 'url': 'https://www.nicekicks.com/feed/', 'lang': 'en'},
    {'name': 'Kicks On Fire', 'url': 'https://www.kicksonfire.com/feed/', 'lang': 'en'},
    {'name': 'Robb Report', 'url': 'https://robbreport.com/feed/', 'lang': 'en'},
    {'name': 'Harper\'s Bazaar', 'url': 'https://www.harpersbazaar.com/feed/rss/', 'lang': 'en'},
    {'name': 'Elle Global', 'url': 'https://www.elle.com/rss/all.xml', 'lang': 'en'},
    {'name': 'NYT Fashion', 'url': 'https://rss.nytimes.com/services/xml/rss/nyt/FashionandStyle.xml', 'lang': 'en'},
    {'name': 'Guardian Fashion', 'url': 'https://www.theguardian.com/fashion/rss', 'lang': 'en'},
    {'name': 'Dazed', 'url': 'https://www.dazeddigital.com/rss', 'lang': 'en'},
    {'name': 'i-D Magazine', 'url': 'https://i-d.vice.com/en_us/rss', 'lang': 'en'},
    {'name': 'SSENSE', 'url': 'https://www.ssense.com/en-us/feed', 'lang': 'en'},
    {'name': 'Grailed', 'url': 'https://www.grailed.com/drycleanonly/feed', 'lang': 'en'},
]

# –†–ê–°–®–ò–†–ï–ù–ù–´–ô –°–ü–ò–°–û–ö –ë–†–ï–ù–î–û–í
BRANDS = [
    'Gucci', 'Prada', 'Dior', 'Chanel', 'Louis Vuitton', 'Balenciaga', 
    'Versace', 'Hermes', 'Valentino', 'Fendi', 'Dolce & Gabbana', 
    'Bottega Veneta', 'Loewe', 'Off-White', 'Balmain', 'Givenchy', 
    'Burberry', 'Tom Ford', 'Alexander McQueen', 'Saint Laurent', 
    'Celine', 'JW Anderson', 'Vetements', 'Comme des Gar√ßons',
    'Maison Margiela', 'Acne Studios', 'Issey Miyake', 'Kenzo', 
    'Moschino', 'Raf Simons', 'Rick Owens', 'Yves Saint Laurent',
    'Miu Miu', 'Moncler', 'Stone Island', 'Palm Angels',
    'Supreme', 'Palace', 'Stussy', 'Bape', 'Kith', 'Noah',
    'Aime Leon Dore', 'Carhartt WIP', 'Brain Dead', 'Awake NY',
    'Fear of God', 'Essentials', 'Rhude', 'Amiri', 'A-Cold-Wall',
    'Nike', 'Jordan', 'Adidas', 'New Balance', 'Converse',
]

# –≠–º–æ–¥–∑–∏ –¥–ª—è –±—Ä–µ–Ω–¥–æ–≤
BRAND_EMOJIS = {
    'Gucci': 'üêç', 'Prada': 'üî∫', 'Dior': 'üåπ', 'Chanel': 'üëë',
    'Louis Vuitton': 'üß≥', 'Balenciaga': 'üëü', 'Versace': 'üåû',
    'Hermes': 'üü†', 'Valentino': 'üî¥', 'Fendi': 'üü°',
    'Raf Simons': 'üé®', 'Rick Owens': '‚ö´', 'Yves Saint Laurent': 'üíÑ',
    'Supreme': 'üî¥', 'Palace': 'üî∑', 'Bape': 'üêí', 'Stussy': 'üèÑ',
    'Nike': 'üëü', 'Jordan': 'üÖ∞Ô∏è', 'Adidas': '‚ùå', 'Off-White': 'üü®',
    'Stone Island': 'üß≠', 'Moncler': 'ü¶¢', 'Bottega Veneta': 'üü¢',
    'Loewe': 'üêò', 'Givenchy': '‚öúÔ∏è', 'Burberry': 'üß•', 'Tom Ford': 'üï∂Ô∏è',
    'Alexander McQueen': 'üíÄ', 'Celine': '‚ö°', 'Vetements': 'üîµ',
    'Maison Margiela': 'ü•º', 'Acne Studios': 'üåÄ', 'Comme des Gar√ßons': '‚ù§Ô∏è',
    'default': 'üëó'
}

class AdvancedAITranslator:
    def __init__(self):
        self.cache = {}
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def translate_text(self, text):
        """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ —á–µ—Ä–µ–∑ –±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ API"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º LibreTranslate –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–æ–π
            url = "https://libretranslate.de/translate"
            data = {
                'q': text,
                'source': 'en',
                'target': 'ru',
                'format': 'text'
            }
            response = self.session.post(url, json=data, timeout=15)
            if response.status_code == 200:
                result = response.json()
                return result['translatedText']
        except Exception as e:
            logger.warning(f"Translation failed: {e}")
        
        # Fallback: –±–∞–∑–æ–≤—ã–π —Å–ª–æ–≤–∞—Ä—å –ø–µ—Ä–µ–≤–æ–¥–∞
        translations = {
            'collection': '–∫–æ–ª–ª–µ–∫—Ü–∏—è',
            'sneakers': '–∫—Ä–æ—Å—Å–æ–≤–∫–∏',
            'handbag': '—Å—É–º–∫–∞',
            'accessories': '–∞–∫—Å–µ—Å—Å—É–∞—Ä—ã',
            'runway': '–ø–æ–∫–∞–∑',
            'designer': '–¥–∏–∑–∞–π–Ω–µ—Ä',
            'luxury': '–ª—é–∫—Å',
            'limited': '–ª–∏–º–∏—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π',
            'exclusive': '—ç–∫—Å–∫–ª—é–∑–∏–≤–Ω—ã–π',
            'collaboration': '–∫–æ–ª–ª–∞–±–æ—Ä–∞—Ü–∏—è',
            'release': '—Ä–µ–ª–∏–∑',
            'announced': '–∞–Ω–æ–Ω—Å–∏—Ä–æ–≤–∞–ª',
            'launched': '–∑–∞–ø—É—Å—Ç–∏–ª',
            'new': '–Ω–æ–≤—ã–π',
            'innovative': '–∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π',
            'revolutionary': '—Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π',
        }
        
        translated = text
        for en, ru in translations.items():
            translated = re.sub(rf'\b{en}\b', ru, translated, flags=re.IGNORECASE)
        
        return translated
    
    def generate_expert_comment(self, brand, content_type="collection"):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏"""
        
        comment_templates = {
            'collection': [
                f"üèÜ {styler.bold('–≠–ö–°–ö–õ–Æ–ó–ò–í')}: –ö–æ–ª–ª–µ–∫—Ü–∏—è {brand} –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —ç–≤–æ–ª—é—Ü–∏—é –î–ù–ö –±—Ä–µ–Ω–¥–∞, —Å–æ—á–µ—Ç–∞—è –∞—Ä—Ö–∏–≤–Ω—ã–µ –º–æ—Ç–∏–≤—ã —Å —Ñ—É—Ç—É—Ä–∏—Å—Ç–∏—á–Ω—ã–º –≤–∏–¥–µ–Ω–∏–µ–º.",
                f"üé® {styler.bold('–¢–í–û–†–ß–ï–°–ö–ò–ô –ü–†–û–†–´–í')}: {brand} –ø–µ—Ä–µ–æ—Å–º—ã—Å–ª–∏–≤–∞–µ—Ç –∫–∞–Ω–æ–Ω—ã —Ä–æ—Å–∫–æ—à–∏, –ø—Ä–µ–¥–ª–∞–≥–∞—è —Å–≤–µ–∂–∏–π –≤–∑–≥–ª—è–¥ –Ω–∞ –ø—Ä–∏–≤—ã—á–Ω—ã–µ —Å–∏–ª—É—ç—Ç—ã.",
                f"üí´ {styler.bold('–ò–ù–ù–û–í–ê–¶–ò–Ø')}: –í –Ω–æ–≤–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏ {brand} –ø—Ä–æ—Å–ª–µ–∂–∏–≤–∞–µ—Ç—Å—è —Å–º–µ–ª—ã–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç —Å –º–∞—Ç–µ—Ä–∏–∞–ª–∞–º–∏ –∏ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–µ–π.",
                f"üîÆ {styler.bold('–¢–†–ï–ù–î–°–ï–¢–¢–ï–†')}: {brand} –∑–∞–¥–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä —Ä–∞–∑–≤–∏—Ç–∏—è –∏–Ω–¥—É—Å—Ç—Ä–∏–∏, –ø—Ä–µ–¥–≤–æ—Å—Ö–∏—â–∞—è –∑–∞–ø—Ä–æ—Å—ã –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è.",
                f"üåü {styler.bold('–ö–£–õ–¨–¢–£–†–ù–´–ô –§–ï–ù–û–ú–ï–ù')}: –†–µ–ª–∏–∑ {brand} –≤—ã—Ö–æ–¥–∏—Ç –∑–∞ —Ä–∞–º–∫–∏ –º–æ–¥—ã, —Å—Ç–∞–Ω–æ–≤—è—Å—å –∞—Ä—Ç-–≤—ã—Å–∫–∞–∑—ã–≤–∞–Ω–∏–µ–º."
            ],
            'collaboration': [
                f"ü§ù {styler.bold('–°–¢–†–ê–¢–ï–ì–ò–ß–ï–°–ö–ò–ô –ê–õ–¨–Ø–ù–°')}: –ö–æ–ª–ª–∞–±–æ—Ä–∞—Ü–∏—è {brand} –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –ª—É—á—à–µ–µ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –º–∏—Ä–æ–≤, —Å–æ–∑–¥–∞–≤–∞—è —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –ø—Ä–æ–¥—É–∫—Ç.",
                f"üé≠ {styler.bold('–¢–í–û–†–ß–ï–°–ö–ò–ô –î–ò–ê–õ–û–ì')}: {brand} –≤—Å—Ç—É–ø–∞–µ—Ç –≤ –¥–∏–∞–ª–æ–≥ —Å –Ω–æ–≤—ã–º –ø–∞—Ä—Ç–Ω–µ—Ä–æ–º, —Ä–æ–∂–¥–∞—è –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–µ —ç—Å—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ—à–µ–Ω–∏—è.",
                f"‚ö° {styler.bold('–°–ò–ù–ï–†–ì–ò–Ø')}: –°–æ–≤–º–µ—Å—Ç–Ω—ã–π –ø—Ä–æ–µ–∫—Ç {brand} –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –º–æ—â—å —Ç–≤–æ—Ä—á–µ—Å–∫–æ–≥–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —Ç–∞–ª–∞–Ω—Ç–æ–≤.",
                f"üåâ {styler.bold('–ú–û–°–¢ –ú–ï–ñ–î–£ –ö–£–õ–¨–¢–£–†–ê–ú–ò')}: {brand} —Å—Ç—Ä–æ–∏—Ç –º–æ—Å—Ç –º–µ–∂–¥—É —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ creative-—Å–æ–æ–±—â–µ—Å—Ç–≤–∞–º–∏."
            ],
            'sneakers': [
                f"üëü {styler.bold('–ö–£–õ–¨–¢–û–í–´–ô –†–ï–õ–ò–ó')}: –ù–æ–≤—ã–µ –∫—Ä–æ—Å—Å–æ–≤–∫–∏ {brand} –æ–±–µ—â–∞—é—Ç —Å—Ç–∞—Ç—å must-have —Å–µ–∑–æ–Ω–∞.",
                f"üî• {styler.bold('–•–ê–ô–ü-–ú–ê–®–ò–ù–ê')}: {brand} –∑–∞–ø—É—Å–∫–∞–µ—Ç –æ—á–µ—Ä–µ–¥–Ω—É—é –≤–æ–ª–Ω—É –∞–∂–∏–æ—Ç–∞–∂–∞ –≤ –∫—Ä–æ—Å—Å–æ–≤–æ—á–Ω–æ–π –∏–Ω–¥—É—Å—Ç—Ä–∏–∏.",
                f"üéØ {styler.bold('–¢–û–ß–ù–´–ô –í–´–°–¢–†–ï–õ')}: –ö–æ–ª–ª–µ–∫—Ü–∏—è –æ–±—É–≤–∏ {brand} –∏–¥–µ–∞–ª—å–Ω–æ –ø–æ–ø–∞–¥–∞–µ—Ç –≤ –∑–∞–ø—Ä–æ—Å—ã —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—è.",
                f"üí• {styler.bold('–†–ï–í–û–õ–Æ–¶–ò–Ø –í –û–ë–£–í–ò')}: {brand} –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞–µ—Ç –ø—Ä–∞–≤–∏–ª–∞ –∏–≥—Ä—ã –≤ —Å–µ–≥–º–µ–Ω—Ç–µ streetwear-–æ–±—É–≤–∏."
            ],
            'innovation': [
                f"üöÄ {styler.bold('–¢–ï–•–ù–û–õ–û–ì–ò–ß–ï–°–ö–ò–ô –ü–†–û–†–´–í')}: {brand} –≤–Ω–µ–¥—Ä—è–µ—Ç –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è, –º–µ–Ω—è—é—â–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –æ —Ä–æ—Å–∫–æ—à–∏.",
                f"üå± {styler.bold('–£–°–¢–û–ô–ß–ò–í–û–ï –†–ê–ó–í–ò–¢–ò–ï')}: {brand} –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç commitment –∫ —ç–∫–æ–ª–æ–≥–∏—á–Ω—ã–º –ø—Ä–∞–∫—Ç–∏–∫–∞–º.",
                f"üî¨ {styler.bold('–ù–ê–£–ß–ù–´–ô –ü–û–î–•–û–î')}: –í –æ—Å–Ω–æ–≤–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ {brand} –ª–µ–∂–∞—Ç –≥–ª—É–±–æ–∫–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã.",
                f"üí° {styler.bold('–§–£–¢–£–†–û–õ–û–ì–ò–Ø')}: {brand} –∑–∞–≥–ª—è–¥—ã–≤–∞–µ—Ç –≤ –±—É–¥—É—â–µ–µ, –ø—Ä–µ–¥–ª–∞–≥–∞—è —Å–º–µ–ª—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ä–µ—à–µ–Ω–∏—è."
            ]
        }
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        content_lower = content_type.lower()
        if any(word in content_lower for word in ['collab', 'collaboration', 'partnership']):
            category = 'collaboration'
        elif any(word in content_lower for word in ['sneakers', 'shoes', 'footwear']):
            category = 'sneakers'
        elif any(word in content_lower for word in ['innovation', 'technology', 'sustainable']):
            category = 'innovation'
        else:
            category = 'collection'
        
        templates = comment_templates.get(category, comment_templates['collection'])
        return random.choice(templates)
    
    def enhance_content_style(self, text, brand):
        """–£–ª—É—á—à–∞–µ—Ç —Å—Ç–∏–ª—å –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å –≤—ã–¥–µ–ª–µ–Ω–∏–µ–º –∫–ª—é—á–µ–≤—ã—Ö –º–æ–º–µ–Ω—Ç–æ–≤"""
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è
        important_keywords = [
            '—ç–∫—Å–∫–ª—é–∑–∏–≤–Ω', '–ª–∏–º–∏—Ç–∏—Ä–æ–≤–∞–Ω', '–∫–æ–ª–ª–∞–±–æ—Ä–∞—Ü–∏—è', '—Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω',
            '–∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω', '–∫—É–ª—å—Ç–æ–≤', '–¥–µ–±—é—Ç', '–ø—Ä–µ–º—å–µ—Ä', '–∞–Ω–æ–Ω—Å',
            '—Ä–µ–ª–∏–∑', '–∫–æ–ª–ª–µ–∫—Ü–∏—è', '–∫–∞–ø—Å—É–ª–∞', '–∞—Ä—Ö–∏–≤', '–≤–∏–Ω—Ç–∞–∂',
            '–ø—Ä–µ–º–∏—É–º', '–ª—é–∫—Å', '—Ä–æ—Å–∫–æ—à', '—É–Ω–∏–∫–∞–ª—å–Ω', '–æ—Å–æ–±—ã–π'
        ]
        
        # –í—ã–¥–µ–ª—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        for keyword in important_keywords:
            if keyword in text.lower():
                # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ –≤—Ö–æ–∂–¥–µ–Ω–∏—è –∏ –≤—ã–¥–µ–ª—è–µ–º –∏—Ö
                pattern = re.compile(re.escape(keyword), re.IGNORECASE)
                text = pattern.sub(styler.bold(r'\g<0>'), text)
        
        # –í—ã–¥–µ–ª—è–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –±—Ä–µ–Ω–¥–æ–≤
        if brand in text:
            text = text.replace(brand, styler.bold(brand))
        
        # –î–æ–±–∞–≤–ª—è–µ–º —ç–º–æ–¥–∑–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è
        if any(word in text.lower() for word in ['–∫—Ä–æ—Å—Å–æ–≤–∫–∏', 'sneakers']):
            text = "üëü " + text
        elif any(word in text.lower() for word in ['—Å—É–º–∫', 'bag', 'handbag']):
            text = "üëú " + text
        elif any(word in text.lower() for word in ['–æ–¥–µ–∂–¥', 'collection']):
            text = "üëó " + text
        elif any(word in text.lower() for word in ['–∞–∫—Å–µ—Å—Å—É–∞—Ä', 'accessor']):
            text = "üíé " + text
        
        return text

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–∞
translator = AdvancedAITranslator()

def is_high_quality_image(url):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–º"""
    if not url.startswith(('http://', 'https://')):
        return False
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
    valid_extensions = {'.jpg', '.jpeg', '.png', '.webp'}
    if not any(ext in url.lower() for ext in valid_extensions):
        return False
    
    # –ò—Å–∫–ª—é—á–∞–µ–º –∏–∫–æ–Ω–∫–∏ –∏ –º–∞–ª–µ–Ω—å–∫–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    excluded_terms = ['icon', 'logo', 'thumbnail', 'small', 'avatar', 'sprite', 'pixel']
    if any(term in url.lower() for term in excluded_terms):
        return False
    
    return True

def rate_image_quality(url, element):
    """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
    score = 0
    
    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è –º–µ—Ç–∞-—Ç–µ–≥–æ–≤
    if element.name == 'meta':
        score += 100
    
    # –†–∞–∑–º–µ—Ä—ã –∏–∑ –∞—Ç—Ä–∏–±—É—Ç–æ–≤
    width = element.get('width', '')
    height = element.get('height', '')
    
    if width and height:
        try:
            w = int(''.join(filter(str.isdigit, str(width))))
            h = int(''.join(filter(str.isdigit, str(height))))
            if w > 300 and h > 200:
                score += 50
            if w > 600 and h > 400:
                score += 30
        except:
            pass
    
    # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ URL
    quality_indicators = ['large', 'xlarge', 'xxlarge', 'original', 'full', 'main', 'hero', 'featured']
    for indicator in quality_indicators:
        if indicator in url.lower():
            score += 20
    
    return score

def extract_high_quality_image(url):
    """–ê–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–≤ –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        image_selectors = [
            'meta[property="og:image"]',
            'meta[name="twitter:image"]',
            'meta[property="twitter:image:src"]',
            'meta[name="og:image"]',
            'link[rel="image_src"]',
            'article img',
            '.wp-post-image',
            '.article-image img',
            '.post-image img',
            '.entry-content img',
            '.content img',
            'figure img',
            '.hero-image img',
            '.main-image img',
            '.featured-image img',
            '[class*="image"] img',
            'img[src*="large"]',
            'img[src*="medium"]',
            'img[src*="main"]',
            'img[src*="featured"]',
            'img'
        ]
        
        candidates = []
        
        for selector in image_selectors:
            elements = soup.select(selector)
            for element in elements:
                if selector.startswith('meta'):
                    image_url = element.get('content', '')
                else:
                    image_url = element.get('src') or element.get('data-src') or element.get('data-lazy-src')
                
                if image_url and is_high_quality_image(image_url):
                    score = rate_image_quality(image_url, element)
                    candidates.append((image_url, score))
        
        if candidates:
            candidates.sort(key=lambda x: x[1], reverse=True)
            best_image = candidates[0][0]
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ URL –≤ –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ
            if best_image.startswith('//'):
                best_image = 'https:' + best_image
            elif best_image.startswith('/'):
                best_image = urljoin(url, best_image)
            
            logger.info(f"‚úÖ Found high-quality image: {best_image}")
            return best_image
            
    except Exception as e:
        logger.warning(f"Image extraction error: {e}")
    
    return None

def extract_rich_content(text, max_length=650):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç —Å AI-–ø–µ—Ä–µ–≤–æ–¥–æ–º"""
    if not text:
        return ""
    
    try:
        # –û—á–∏—Å—Ç–∫–∞ HTML —Ç–µ–≥–æ–≤
        text = re.sub(r'<[^<]+?>', '', text)
        text = re.sub(r'\s+', ' ', text).strip()
        
        if len(text) < 30:
            return ""
        
        # –£–¥–∞–ª—è–µ–º —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–∏–º–≤–æ–ª–æ–≤ (–≤–æ–∑–º–æ–∂–Ω—ã–π –º—É—Å–æ—Ä)
        text = re.sub(r'[^\w\s.,!?;:]{50,}', '', text)
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = re.split(r'[.!?]+', text)
        meaningful_sentences = []
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –≤–∞–∂–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        importance_keywords = [
            'announce', 'launch', 'release', 'collaboration', 'collection',
            'runway', 'exclusive', 'limited', 'debut', 'unveil', 'innovative',
            'revolutionary', 'first look', 'capsule', 'campaign', 'show',
            'drop', 'archive', 'vintage', 'sustainable', 'premium', 'luxury',
            'designer', 'sneakers', 'handbag', 'accessories', 'new', 'upcoming'
        ]
        
        # –°–æ–±–∏—Ä–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) > 25 and any(keyword in sentence.lower() for keyword in importance_keywords):
                meaningful_sentences.append(sentence)
        
        # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –≤–∞–∂–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö
        if meaningful_sentences:
            content = '. '.join(meaningful_sentences[:5])
        else:
            # –ò–Ω–∞—á–µ –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            content = '. '.join([s for s in sentences[:4] if len(s) > 20])
        
        if not content:
            return ""
        
        # AI-–ø–µ—Ä–µ–≤–æ–¥
        translated_content = translator.translate_text(content)
        
        # –£–ª—É—á—à–∞–µ–º –≥—Ä–∞–º–º–∞—Ç–∏–∫—É —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        translated_content = improve_russian_grammar(translated_content)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
        if len(translated_content) > max_length:
            translated_content = translated_content[:max_length-3] + '...'
        elif len(translated_content) < 150:
            # –ï—Å–ª–∏ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ, –¥–æ–±–∞–≤–ª—è–µ–º –±–æ–ª—å—à–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            additional_sentences = [s for s in sentences[4:8] if len(s) > 25]
            if additional_sentences:
                additional_content = '. '.join(additional_sentences)
                additional_translated = translator.translate_text(additional_content)
                additional_translated = improve_russian_grammar(additional_translated)
                
                if additional_translated:
                    translated_content += ' ' + additional_translated
                    if len(translated_content) > max_length:
                        translated_content = translated_content[:max_length-3] + '...'
        
        return translated_content
        
    except Exception as e:
        logger.error(f"Error in extract_rich_content: {e}")
        return ""

def improve_russian_grammar(text):
    """–£–ª—É—á—à–∞–µ—Ç –≥—Ä–∞–º–º–∞—Ç–∏–∫—É —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
    if not text:
        return text
    
    # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–∞–¥–µ–∂–µ–π –∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏–π
    grammar_corrections = {
        r'\b—Å –Ω–æ–≤—ã–π\b': '—Å –Ω–æ–≤–æ–π',
        r'\b–≤ –Ω–æ–≤—ã–π\b': '–≤ –Ω–æ–≤–æ–π', 
        r'\b–Ω–∞ –Ω–æ–≤—ã–π\b': '–Ω–∞ –Ω–æ–≤–æ–π',
        r'\b—Å –ø–æ—Å–ª–µ–¥–Ω–∏–π\b': '—Å –ø–æ—Å–ª–µ–¥–Ω–µ–π',
        r'\b–≤ –ø–æ—Å–ª–µ–¥–Ω–∏–π\b': '–≤ –ø–æ—Å–ª–µ–¥–Ω–µ–π',
        r'\b—Å —ç–∫—Å–∫–ª—é–∑–∏–≤–Ω—ã–π\b': '—Å —ç–∫—Å–∫–ª—é–∑–∏–≤–Ω–æ–π',
        r'\b–≤ —ç–∫—Å–∫–ª—é–∑–∏–≤–Ω—ã–π\b': '–≤ —ç–∫—Å–∫–ª—é–∑–∏–≤–Ω–æ–π',
        r'\b—Å –ª–∏–º–∏—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π\b': '—Å –ª–∏–º–∏—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π',
        r'\b–≤ –ª–∏–º–∏—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π\b': '–≤ –ª–∏–º–∏—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π',
        r'\b—Å —Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π\b': '—Å —Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ–π',
        r'\b–≤ —Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π\b': '–≤ —Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ–π',
        r'\b—Å –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π\b': '—Å –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω–æ–π',
        r'\b–≤ –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π\b': '–≤ –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω–æ–π',
    }
    
    for pattern, correction in grammar_corrections.items():
        text = re.sub(pattern, correction, text, flags=re.IGNORECASE)
    
    # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
    text = re.sub(r'[.!?]{2,}', '.', text)
    text = re.sub(r'[,]{2,}', ',', text)
    
    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'\s([.,!?])', r'\1', text)
    
    # –î–µ–ª–∞–µ–º –ø–µ—Ä–≤—É—é –±—É–∫–≤—É –∑–∞–≥–ª–∞–≤–Ω–æ–π
    if text and len(text) > 1:
        text = text[0].upper() + text[1:]
    
    return text.strip()

def generate_creative_title(brand, content):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫—Ä–µ–∞—Ç–∏–≤–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏"""
    
    content_lower = content.lower()
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞
    if any(word in content_lower for word in ['–∫–æ–ª–ª–∞–±–æ—Ä–∞—Ü–∏—è', 'collaboration']):
        templates = [
            f"{brand} √ó [–ù–æ–≤—ã–π –ü–∞—Ä—Ç–Ω–µ—Ä]: –†–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–∞—è –ö–æ–ª–ª–∞–±–æ—Ä–∞—Ü–∏—è",
            f"–í–∑—Ä—ã–≤–Ω–æ–π –ê–ª—å—è–Ω—Å: {brand} –û–±—ä–µ–¥–∏–Ω—è–µ—Ç—Å—è —Å –¢–≤–æ—Ä—á–µ—Å–∫–∏–º –ì–µ–Ω–∏–µ–º",
            f"{brand} + [–ë—Ä–µ–Ω–¥]: –°–æ—é–∑, –ö–æ—Ç–æ—Ä—ã–π –ò–∑–º–µ–Ω–∏—Ç –í—Å–µ",
        ]
    elif any(word in content_lower for word in ['–∞—Ä—Ö–∏–≤', 'vintage', '—Ä–µ—Ç—Ä–æ']):
        templates = [
            f"–ê—Ä—Ö–∏–≤–Ω–æ–µ –°–æ–∫—Ä–æ–≤–∏—â–µ: {brand} –í–æ–∑—Ä–æ–∂–¥–∞–µ—Ç –õ–µ–≥–µ–Ω–¥—É",
            f"–ò–∑ –ì–ª—É–±–∏–Ω –ò—Å—Ç–æ—Ä–∏–∏: {brand} –í–æ—Å–∫—Ä–µ—à–∞–µ—Ç –ö—É–ª—å—Ç–æ–≤—ã–µ –ú–æ–¥–µ–ª–∏",
            f"–ù–æ—Å—Ç–∞–ª—å–≥–∏—è –ø–æ –í–µ–ª–∏–∫–æ–º—É: {brand} –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ö–ª–∞—Å—Å–∏–∫—É",
        ]
    elif any(word in content_lower for word in ['—É—Å—Ç–æ–π—á–∏–≤', 'sustainable', '—ç–∫–æ–ª–æ–≥–∏—á']):
        templates = [
            f"{brand} –ü–µ—Ä–µ–æ—Å–º—ã—Å–ª–∏–≤–∞–µ—Ç –†–æ—Å–∫–æ—à—å: –≠—Ä–∞ –£—Å—Ç–æ–π—á–∏–≤–æ–π –ú–æ–¥—ã",
            f"–ó–µ–ª–µ–Ω–∞—è –†–µ–≤–æ–ª—é—Ü–∏—è: {brand} –ó–∞–ø—É—Å–∫–∞–µ—Ç Eco-–ö–æ–ª–ª–µ–∫—Ü–∏—é",
            f"–ú–æ–¥–∞ –ë—É–¥—É—â–µ–≥–æ: {brand} –∏ –û—Å–æ–∑–Ω–∞–Ω–Ω–æ–µ –ü–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ",
        ]
    else:
        templates = [
            f"{brand} –ü—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –î–∏–∑–∞–π–Ω–µ",
            f"–ù–æ–≤–∞—è –≠—Ä–∞ {brand}: –ö–æ–ª–ª–µ–∫—Ü–∏—è, –ö–æ—Ç–æ—Ä–∞—è –ò–∑–º–µ–Ω–∏—Ç –í—Å–µ",
            f"–≠–∫—Å–∫–ª—é–∑–∏–≤: {brand} –†–∞—Å–∫—Ä—ã–≤–∞–µ—Ç –°–µ–∫—Ä–µ—Ç—ã –ù–æ–≤–æ–≥–æ –°–µ–∑–æ–Ω–∞",
            f"{brand} –ë—Ä–æ—Å–∞–µ—Ç –í—ã–∑–æ–≤: –ê–≤–∞–Ω–≥–∞—Ä–¥–Ω—ã–π –ü–æ–¥—Ö–æ–¥ –∫ –ú–æ–¥–µ",
            f"–ö—É–ª—å—Ç–æ–≤—ã–π –†–µ–ª–∏–∑: {brand} –ó–∞–¥–∞–µ—Ç –ù–æ–≤—ã–µ –°—Ç–∞–Ω–¥–∞—Ä—Ç—ã",
            f"–¢–≤–æ—Ä—á–µ—Å–∫–∏–π –ü—Ä–æ—Ä—ã–≤: {brand} –∏ –ò—Å–∫—É—Å—Å—Ç–≤–æ –ú–æ–¥—ã",
            f"–†–æ—Å–∫–æ—à—å –ü–µ—Ä–µ–æ—Å–º—ã—Å–ª–µ–Ω–Ω–∞—è: {brand} –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ë—É–¥—É—â–µ–µ",
            f"–ú–æ–¥–∞ –∫–∞–∫ –ò—Å–∫—É—Å—Å—Ç–≤–æ: {brand} –ü—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –®–µ–¥–µ–≤—Ä",
        ]
    
    return random.choice(templates)

def create_attractive_post(brand, content, image_url=None):
    """–°–æ–∑–¥–∞–µ—Ç –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–π –ø–æ—Å—Ç —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    
    emoji = BRAND_EMOJIS.get(brand, BRAND_EMOJIS['default'])
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–∞
    title = generate_creative_title(brand, content)
    
    # –£–ª—É—á—à–∞–µ–º —Å—Ç–∏–ª—å –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    styled_content = translator.enhance_content_style(content, brand)
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–≥–æ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è
    expert_comment = translator.generate_expert_comment(brand, content)
    
    # –°–æ–∑–¥–∞–µ–º –ø–æ—Å—Ç —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º
    post = f"{emoji} {styler.create_header(title)}\n\n"
    post += f"üìñ {styled_content}\n\n"
    post += f"üíé {expert_comment}\n\n"
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –∏ –ø—Ä–∏–∑—ã–≤ –∫ –¥–µ–π—Å—Ç–≤–∏—é
    post += "‚îÄ" * 30 + "\n\n"
    post += f"üí¨ {styler.italic('–ß—Ç–æ –≤—ã –¥—É–º–∞–µ—Ç–µ –æ–± —ç—Ç–æ–º —Ä–µ–ª–∏–∑–µ? –û–±—Å—É–∂–¥–∞–µ–º –≤ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è—Ö!')}"
    
    return post

def send_telegram_post(post, image_url=None):
    """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –ø–æ—Å—Ç –≤ Telegram"""
    try:
        if image_url:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –¥–æ—Å—Ç—É–ø–Ω–æ –ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            headers = {'User-Agent': 'Mozilla/5.0'}
            image_response = requests.get(image_url, headers=headers, timeout=10)
            
            if image_response.status_code == 200 and len(image_response.content) > 5000:  # –ú–∏–Ω–∏–º—É–º 5KB
                url = f'https://api.telegram.org/bot{BOT_TOKEN}/sendPhoto'
                data = {
                    'chat_id': CHANNEL,
                    'caption': post,
                    'parse_mode': 'HTML'
                }
                files = {'photo': ('image.jpg', image_response.content, 'image/jpeg')}
                response = requests.post(url, data=data, files=files, timeout=30)
                
                if response.status_code == 200:
                    logger.info("‚úÖ Post sent successfully with image")
                    return True
                else:
                    logger.warning("üîÑ Image post failed, falling back to text")
        
        # Fallback: –æ—Ç–ø—Ä–∞–≤–∫–∞ –±–µ–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        url = f'https://api.telegram.org/bot{BOT_TOKEN}/sendMessage'
        data = {
            'chat_id': CHANNEL,
            'text': post,
            'parse_mode': 'HTML',
            'disable_web_page_preview': True
        }
        response = requests.post(url, json=data, timeout=30)
        
        return response.status_code == 200
        
    except Exception as e:
        logger.error(f"‚ùå Telegram send error: {e}")
        return False

def find_and_send_news_with_images():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º –∫–∞—Ä—Ç–∏–Ω–æ–∫"""
    
    random.shuffle(SOURCES)
    posts_sent = 0
    max_attempts = 50  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –¥–ª—è –ø–æ–∏—Å–∫–∞ –∫–∞—Ä—Ç–∏–Ω–æ–∫
    
    logger.info("üîÑ Starting aggressive image search...")
    
    for source in SOURCES:
        if posts_sent >= 3:  # –ú–∞–∫—Å–∏–º—É–º 3 –ø–æ—Å—Ç–∞ –∑–∞ –∑–∞–ø—É—Å–∫
            break
            
        try:
            logger.info(f"üîç Checking {source['name']}...")
            feed = feedparser.parse(source['url'])
            
            if not feed.entries:
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–æ–ª—å—à–µ –∑–∞–ø–∏—Å–µ–π –¥–ª—è –ø–æ–∏—Å–∫–∞ –∫–∞—Ä—Ç–∏–Ω–æ–∫
            entries = feed.entries[:20]
            random.shuffle(entries)
            
            for entry in entries:
                if posts_sent >= 3:
                    break
                    
                title = getattr(entry, 'title', '')
                description = getattr(entry, 'description', '')
                link = getattr(entry, 'link', '')
                
                if not title:
                    continue
                
                # –ò—â–µ–º –±—Ä–µ–Ω–¥—ã –≤ –∫–æ–Ω—Ç–µ–Ω—Ç–µ
                full_content = f"{title} {description}".lower()
                
                for brand in BRANDS:
                    if brand.lower() in full_content:
                        logger.info(f"‚úÖ Found news about {brand}")
                        
                        try:
                            # –ê–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –∫–∞—Ä—Ç–∏–Ω–∫–∏
                            logger.info(f"üñºÔ∏è Aggressive image search for {brand}...")
                            image_url = extract_high_quality_image(link)
                            
                            # –ï—Å–ª–∏ –∫–∞—Ä—Ç–∏–Ω–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –ø—Ä–æ–±—É–µ–º –µ—â–µ —Ä–∞–∑ —Å –¥—Ä—É–≥–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
                            if not image_url:
                                logger.info("üîÑ Retrying image search...")
                                time.sleep(1)
                                image_url = extract_high_quality_image(link)
                            
                            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
                            original_content = f"{title}. {description}"
                            translated_content = translator.translate_text(original_content)
                            
                            if len(translated_content) < 100:
                                continue
                            
                            # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–π –ø–æ—Å—Ç
                            post = create_attractive_post(brand, translated_content, image_url)
                            
                            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –ø–æ—Å—Ç
                            if send_telegram_post(post, image_url):
                                logger.info(f"üéâ Successfully posted about {brand} with image: {image_url is not None}")
                                posts_sent += 1
                                
                                # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –ø–æ—Å—Ç–∞–º–∏
                                time.sleep(10)
                                break
                            else:
                                logger.error(f"‚ùå Failed to send post about {brand}")
                                
                        except Exception as e:
                            logger.error(f"üîß Error processing {brand}: {str(e)}")
                            continue
                
        except Exception as e:
            logger.error(f"‚ùå Error with source {source['name']}: {str(e)}")
            continue
    
    return posts_sent

def send_curated_post_with_image():
    """–ö—É—Ä–∏—Ä—É–µ–º—ã–π –ø–æ—Å—Ç —Å –ø–æ–∏—Å–∫–æ–º –∫–∞—Ä—Ç–∏–Ω–∫–∏"""
    logger.info("üé® Creating curated post with image...")
    
    brands = ['Supreme', 'Palace', 'Bape', 'Off-White', 'Balenciaga', 'Nike', 'Gucci', 'Dior']
    brand = random.choice(brands)
    
    # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –∫–∞—Ä—Ç–∏–Ω–∫—É –¥–ª—è –±—Ä–µ–Ω–¥–∞ —á–µ—Ä–µ–∑ Google Images (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç)
    image_url = find_brand_image(brand)
    
    curated_contents = [
        f"{brand} –∞–Ω–æ–Ω—Å–∏—Ä—É–µ—Ç –≤—ã–ø—É—Å–∫ –Ω–æ–≤–æ–π –∫–∞–ø—Å—É–ª—å–Ω–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏, –≤–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–Ω–æ–π –∞—Ä—Ö–∏–≤–Ω—ã–º–∏ –Ω–∞—Ö–æ–¥–∫–∞–º–∏ –∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º —É–ª–∏—á–Ω—ã–º –∏—Å–∫—É—Å—Å—Ç–≤–æ–º. –í —Ä–µ–ª–∏–∑ –≤–æ—à–ª–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–µ edition –∫—Ä–æ—Å—Å–æ–≤–∫–∏, —Ö—É–¥–∏ –∏ –∞–∫—Å–µ—Å—Å—É–∞—Ä—ã —Å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º –¥–∏–∑–∞–π–Ω–æ–º –∏ –ø—Ä–µ–º–∏–∞–ª—å–Ω—ã–º–∏ –º–∞—Ç–µ—Ä–∏–∞–ª–∞–º–∏.",
        f"{brand} –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é, —Å–æ–∑–¥–∞–Ω–Ω—É—é –≤ –∫–æ–ª–ª–∞–±–æ—Ä–∞—Ü–∏–∏ —Å –∏–∑–≤–µ—Å—Ç–Ω—ã–º —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º —Ö—É–¥–æ–∂–Ω–∏–∫–æ–º. –≠–∫—Å–∫–ª—é–∑–∏–≤–Ω—ã–µ –≤–µ—â–∏ —Å –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–º–∏ –º–∞—Ç–µ—Ä–∏–∞–ª–∞–º–∏ –∏ –∞–≤–∞–Ω–≥–∞—Ä–¥–Ω—ã–º –¥–∏–∑–∞–π–Ω–æ–º —É–∂–µ –≤—ã–∑–≤–∞–ª–∏ –∞–∂–∏–æ—Ç–∞–∂ —Å—Ä–µ–¥–∏ –∫–æ–ª–ª–µ–∫—Ü–∏–æ–Ω–µ—Ä–æ–≤.",
        f"–ù–æ–≤—ã–π –¥—Ä–æ–ø –æ—Ç {brand} —Å–æ—á–µ—Ç–∞–µ—Ç —ç–ª–µ–º–µ–Ω—Ç—ã —É–ª–∏—á–Ω–æ–≥–æ —Å—Ç–∏–ª—è –∏ –≤—ã—Å–æ–∫–æ–π –º–æ–¥—ã. –ö–æ–ª–ª–µ–∫—Ü–∏—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Å–≤–µ–∂–∏–π –≤–∑–≥–ª—è–¥ –Ω–∞ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –≥–∞—Ä–¥–µ—Ä–æ–±, –æ–±—ä–µ–¥–∏–Ω—è—è –∫–æ–º—Ñ–æ—Ä—Ç –∏ —Ä–æ—Å–∫–æ—à—å –≤ –∫–∞–∂–¥–æ–º –∏–∑–¥–µ–ª–∏–∏.",
    ]
    
    content = random.choice(curated_contents)
    post = create_attractive_post(brand, content, image_url)
    
    if send_telegram_post(post, image_url):
        logger.info("‚úÖ Curated post sent successfully!")
        return True
    
    return False

def find_brand_image(brand):
    """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±—Ä–µ–Ω–¥–∞"""
    try:
        # –ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±—Ä–µ–Ω–¥–∞
        # –í —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Google Custom Search API
        return None
    except:
        return None

if __name__ == "__main__":
    logger.info("üöÄ Starting Enhanced Fashion Bot with Image Priority")
    
    start_time = time.time()
    
    # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –∏ –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ —Å –∫–∞—Ä—Ç–∏–Ω–∫–∞–º–∏
    posts_sent = find_and_send_news_with_images()
    
    # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –ø–æ—Å—Ç–æ–≤ —Å –∫–∞—Ä—Ç–∏–Ω–∫–∞–º–∏, –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –∫—É—Ä–∏—Ä—É–µ–º—ã–π
    if posts_sent == 0:
        logger.info("üìù No image posts found, creating curated content...")
        send_curated_post_with_image()
    
    execution_time = time.time() - start_time
    logger.info(f"‚è±Ô∏è Execution time: {execution_time:.2f} seconds")
    logger.info(f"üìä Posts sent: {posts_sent}")
    logger.info("‚úÖ Bot finished!")

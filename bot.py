import requests
import os
import re
import random
from bs4 import BeautifulSoup
import feedparser
from datetime import datetime
import time
import html

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
BOT_TOKEN = os.environ['BOT_TOKEN']
CHANNEL = os.environ['CHANNEL']

# –ë–û–õ–¨–®–û–ô —Å–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (—Ä—É—Å—Å–∫–∏–µ + –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ)
SOURCES = [
    # –†—É—Å—Å–∫–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –º–æ–¥—ã
    {'name': 'Vogue –†–æ—Å—Å–∏—è', 'url': 'https://www.vogue.ru/fashion/rss/', 'lang': 'ru'},
    {'name': 'Buro 24/7', 'url': 'https://www.buro247.ru/rss.xml', 'lang': 'ru'},
    {'name': 'Elle –†–æ—Å—Å–∏—è', 'url': 'https://www.elle.ru/rss/', 'lang': 'ru'},
    {'name': 'Cosmo –ú–æ–¥–∞', 'url': 'https://www.cosmo.ru/fashion/rss/', 'lang': 'ru'},
    {'name': 'Grazia', 'url': 'https://grazia.ru/rss/', 'lang': 'ru'},
    {'name': 'Spletnik', 'url': 'https://www.spletnik.ru/rss.xml', 'lang': 'ru'},
    
    # –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –º–æ–¥—ã
    {'name': 'Vogue Global', 'url': 'https://www.vogue.com/rss', 'lang': 'en'},
    {'name': 'Business of Fashion', 'url': 'https://www.businessoffashion.com/feed', 'lang': 'en'},
    {'name': 'Hypebeast', 'url': 'https://hypebeast.com/fashion/feed', 'lang': 'en'},
    {'name': 'Highsnobiety', 'url': 'https://www.highsnobiety.com/feed/', 'lang': 'en'},
    {'name': 'Fashionista', 'url': 'https://fashionista.com/.rss', 'lang': 'en'},
    {'name': 'WWD', 'url': 'https://wwd.com/feed/', 'lang': 'en'},
    {'name': 'The Cut', 'url': 'https://www.thecut.com/rss/index.xml', 'lang': 'en'},
    {'name': 'Harper\'s Bazaar', 'url': 'https://www.harpersbazaar.com/feed/rss/', 'lang': 'en'},
    {'name': 'GQ Style', 'url': 'https://www.gq.com/feed/rss', 'lang': 'en'},
    {'name': 'Elle Global', 'url': 'https://www.elle.com/rss/all.xml', 'lang': 'en'},
    
    # –õ—é–∫—Å –∏–∑–¥–∞–Ω–∏—è
    {'name': 'Robb Report', 'url': 'https://robbreport.com/feed/', 'lang': 'en'},
    {'name': 'The Business of Fashion', 'url': 'https://www.businessoffashion.com/rss', 'lang': 'en'},
    
    # –£–ª–∏—á–Ω–∞—è –º–æ–¥–∞
    {'name': 'Hypebeast Style', 'url': 'https://hypebeast.com/feed', 'lang': 'en'},
    {'name': 'Sneaker News', 'url': 'https://sneakernews.com/feed/', 'lang': 'en'},
]

# Luxury –±—Ä–µ–Ω–¥—ã –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
LUXURY_BRANDS = [
    'Raf Simons', 'Rick Owens', 'Yves Saint Laurent', 'YSL', 'Gucci', 'Prada', 
    'Dior', 'Chanel', 'Louis Vuitton', 'Balenciaga', 'Versace', 'Hermes',
    'Valentino', 'Fendi', 'Dolce & Gabbana', 'Bottega Veneta', 'Loewe',
    'Off-White', 'Balmain', 'Givenchy', 'Burberry', 'Tom Ford', 'Alexander McQueen',
    'Saint Laurent', 'Celine', 'JW Anderson', 'Vetements', 'Comme des Gar√ßons',
    'Maison Margiela', 'Acne Studios', 'Issey Miyake', 'Kenzo', 'Moschino'
]

def translate_to_russian(text):
    """–ì–ª—É–±–æ–∫–∏–π –ø–µ—Ä–µ–≤–æ–¥ –Ω–∞ —Ä—É—Å—Å–∫–∏–π —Å —É—á–µ—Ç–æ–º –º–æ–¥–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
    if not text:
        return ""
        
    translations = {
        # –û—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
        'collection': '–∫–æ–ª–ª–µ–∫—Ü–∏—è', 'fashion': '–º–æ–¥–∞', 'runway': '–ø–æ–∫–∞–∑',
        'designer': '–¥–∏–∑–∞–π–Ω–µ—Ä', 'luxury': '–ª—é–∫—Å', 'new': '–Ω–æ–≤—ã–π',
        'trend': '—Ç—Ä–µ–Ω–¥', 'style': '—Å—Ç–∏–ª—å', 'brand': '–±—Ä–µ–Ω–¥',
        'launch': '–∑–∞–ø—É—Å–∫', 'release': '—Ä–µ–ª–∏–∑', 'collaboration': '–∫–æ–ª–ª–∞–±–æ—Ä–∞—Ü–∏—è',
        'sneakers': '–∫—Ä–æ—Å—Å–æ–≤–∫–∏', 'handbag': '—Å—É–º–∫–∞', 'accessories': '–∞–∫—Å–µ—Å—Å—É–∞—Ä—ã',
        'campaign': '–∫–∞–º–ø–∞–Ω–∏—è', 'show': '—à–æ—É', 'models': '–º–æ–¥–µ–ª–∏',
        'exclusive': '—ç–∫—Å–∫–ª—é–∑–∏–≤', 'limited': '–ª–∏–º–∏—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π', 'edition': '–∏–∑–¥–∞–Ω–∏–µ',
        
        # –ì–ª–∞–≥–æ–ª—ã
        'announced': '–∞–Ω–æ–Ω—Å–∏—Ä–æ–≤–∞–ª', 'presented': '–ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª', 'released': '–≤—ã–ø—É—Å—Ç–∏–ª',
        'unveiled': '–ø–æ–∫–∞–∑–∞–ª', 'debuted': '–¥–µ–±—é—Ç–∏—Ä–æ–≤–∞–ª', 'teased': '–ø–æ–∫–∞–∑–∞–ª —Ç–∏–∑–µ—Ä',
        'collaborated': '—Å–æ—Ç—Ä—É–¥–Ω–∏—á–∞–ª', 'designed': '—Å–ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–ª',
        
        # –ü—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã–µ
        'revolutionary': '—Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π', 'iconic': '–∫—É–ª—å—Ç–æ–≤—ã–π', 'innovative': '–∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π',
        'sustainable': '—É—Å—Ç–æ–π—á–∏–≤—ã–π', 'avant-garde': '–∞–≤–∞–Ω–≥–∞—Ä–¥–Ω—ã–π', 'minimalist': '–º–∏–Ω–∏–º–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π',
        'bold': '—Å–º–µ–ª—ã–π', 'elegant': '—ç–ª–µ–≥–∞–Ω—Ç–Ω—ã–π', 'luxurious': '—Ä–æ—Å–∫–æ—à–Ω—ã–π',
        'exclusive': '—ç–∫—Å–∫–ª—é–∑–∏–≤–Ω—ã–π', 'limited': '–ª–∏–º–∏—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π',
        
        # –°—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ
        'aesthetics': '—ç—Å—Ç–µ—Ç–∏–∫–∞', 'silhouette': '—Å–∏–ª—É—ç—Ç', 'garment': '–æ–¥–µ–∂–¥–∞',
        'footwear': '–æ–±—É–≤—å', 'leather': '–∫–æ–∂–∞', 'fabric': '—Ç–∫–∞–Ω—å',
        'embroidery': '–≤—ã—à–∏–≤–∫–∞', 'print': '–ø—Ä–∏–Ω—Ç', 'color': '—Ü–≤–µ—Ç',
        'season': '—Å–µ–∑–æ–Ω', 'capsule': '–∫–∞–ø—Å—É–ª–∞', 'lookbook': '–ª—É–∫–±—É–∫',
        
        # –§—Ä–∞–∑—ã
        'fashion week': '–Ω–µ–¥–µ–ª—è –º–æ–¥—ã', 'ready to wear': '–≥–æ—Ç–æ–≤–∞—è –æ–¥–µ–∂–¥–∞',
        'haute couture': '–æ—Ç –∫—É—Ç—é—Ä', 'street style': '—É–ª–∏—á–Ω—ã–π —Å—Ç–∏–ª—å',
        'fashion house': '–¥–æ–º –º–æ–¥—ã', 'creative director': '–∫—Ä–µ–∞—Ç–∏–≤–Ω—ã–π –¥–∏—Ä–µ–∫—Ç–æ—Ä',
    }
    
    text = text.lower()
    for eng, rus in translations.items():
        text = re.sub(r'\b' + re.escape(eng) + r'\b', rus, text, flags=re.IGNORECASE)
    
    return text.capitalize()

def extract_main_content(text, max_length=600):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–∞–º—É—é –≤–∞–∂–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –¥–ª–∏–Ω—É"""
    if not text:
        return ""
    
    # –£–¥–∞–ª—è–µ–º HTML —Ç–µ–≥–∏ –∏ –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    text = re.sub('<[^<]+?>', '', text)
    text = re.sub('\s+', ' ', text).strip()
    
    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    sentences = re.split(r'[.!?]+', text)
    meaningful_sentences = []
    
    for sentence in sentences:
        sentence = sentence.strip()
        if len(sentence) > 25:  # –¢–æ–ª—å–∫–æ –∑–Ω–∞—á–∏–º—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞, —É–∫–∞–∑—ã–≤–∞—é—â–∏–µ –Ω–∞ –≤–∞–∂–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            important_keywords = [
                '–∞–Ω–æ–Ω—Å–∏—Ä–æ–≤–∞–ª', '–ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª', '–≤—ã–ø—É—Å—Ç–∏–ª', '–∫–æ–ª–ª–∞–±–æ—Ä–∞—Ü–∏—è', 
                '–Ω–æ–≤–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è', '–ø–æ–∫–∞–∑', '—Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π', '–∫—É–ª—å—Ç–æ–≤—ã–π',
                '—ç–∫—Å–∫–ª—é–∑–∏–≤', '–ª–∏–º–∏—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π', '–≤–ø–µ—Ä–≤—ã–µ', '–¥–µ–±—é—Ç',
                '–∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π', '—Å–æ—Ç—Ä—É–¥–Ω–∏—á–∞–ª', '–ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–ª'
            ]
            
            if any(keyword in sentence.lower() for keyword in important_keywords):
                meaningful_sentences.append(sentence)
    
    # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –≤–∞–∂–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è - –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö
    if meaningful_sentences:
        content = '. '.join(meaning_sentences[:4]) + '.'
    else:
        # –ò–Ω–∞—á–µ –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        content = '. '.join([s for s in sentences[:3] if len(s) > 20]) + '.'
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
    if len(content) > max_length:
        content = content[:max_length-3] + '...'
    
    return content

def extract_image_from_html(html_content, url):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≥–ª–∞–≤–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑ HTML"""
    try:
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
        selectors = [
            'meta[property="og:image"]',
            'meta[name="twitter:image"]',
            '.article-image img',
            '.post-image img',
            '.content img',
            'img'
        ]
        
        for selector in selectors:
            elements = soup.select(selector)
            for element in elements:
                if selector.startswith('meta'):
                    image_url = element.get('content', '')
                else:
                    image_url = element.get('src', '')
                
                if image_url and image_url.startswith('http'):
                    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –≤ –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ
                    if image_url.startswith('//'):
                        image_url = 'https:' + image_url
                    elif image_url.startswith('/'):
                        from urllib.parse import urljoin
                        image_url = urljoin(url, image_url)
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                    if any(ext in image_url.lower() for ext in ['.jpg', '.jpeg', '.png', '.webp']):
                        return image_url
                        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
    
    return None

def generate_russian_title(english_title, brand):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫—Ä–∞—Å–∏–≤—ã–π —Ä—É—Å—Å–∫–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫"""
    
    title_templates = [
        f"{brand} –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é",
        f"–ù–æ–≤–∞—è —ç—Ä–∞ {brand}: —á—Ç–æ –∏–∑–≤–µ—Å—Ç–Ω–æ –æ –≥—Ä—è–¥—É—â–∏—Ö —Ä–µ–ª–∏–∑–∞—Ö",
        f"{brand} –º–µ–Ω—è–µ—Ç –ø—Ä–∞–≤–∏–ª–∞ –∏–≥—Ä—ã –≤ –º–∏—Ä–µ –º–æ–¥—ã", 
        f"–≠–∫—Å–∫–ª—é–∑–∏–≤: —Å–µ–∫—Ä–µ—Ç—ã –Ω–æ–≤–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏ {brand}",
        f"–ö—É–ª—å—Ç–æ–≤–∞—è –∫–æ–ª–ª–∞–±–æ—Ä–∞—Ü–∏—è {brand} —Å –Ω–æ–≤—ã–º –∫—Ä–µ–∞—Ç–∏–≤–Ω—ã–º –ø–æ–¥—Ö–æ–¥–æ–º",
        f"{brand} –∞–Ω–æ–Ω—Å–∏—Ä—É–µ—Ç —Å–µ–Ω—Å–∞—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–∫–∞–∑ –Ω–∞ –Ω–µ–¥–µ–ª–µ –º–æ–¥—ã",
        f"–†–µ–≤–æ–ª—é—Ü–∏—è –æ—Ç {brand}: –≤—Å–µ –¥–µ—Ç–∞–ª–∏ –Ω–æ–≤–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞",
        f"{brand} –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è –≤ –¥–∏–∑–∞–π–Ω–µ",
        f"–ù–æ–≤—ã–π –≤–∏—Ç–æ–∫ –≤ –∏—Å—Ç–æ—Ä–∏–∏ {brand}: —á—Ç–æ –∂–¥–∞—Ç—å –æ—Ç –∫–æ–ª–ª–µ–∫—Ü–∏–∏",
        f"{brand} –≤—ã–ø—É—Å–∫–∞–µ—Ç –ª–∏–º–∏—Ç–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∫–∞–ø—Å—É–ª—É —Å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º –¥–∏–∑–∞–π–Ω–æ–º"
    ]
    
    return random.choice(title_templates)

def create_luxury_post(brand, content, image_url=None):
    """–°–æ–∑–¥–∞–µ—Ç –∫—Ä–∞—Å–∏–≤—ã–π –ø–æ—Å—Ç –ø—Ä–æ luxury –±—Ä–µ–Ω–¥"""
    
    # –≠–º–æ–¥–∑–∏ –¥–ª—è –±—Ä–µ–Ω–¥–æ–≤
    brand_emojis = {
        'Raf Simons': 'üé®', 'Rick Owens': '‚ö´', 'Yves Saint Laurent': 'üíÑ',
        'Gucci': 'üêç', 'Prada': 'üî∫', 'Dior': 'üåπ', 'Chanel': 'üëë',
        'Louis Vuitton': 'üß≥', 'Balenciaga': 'üëü', 'Versace': 'üåû',
        'Hermes': 'üü†', 'Valentino': 'üî¥', 'Fendi': 'üü°'
    }
    
    emoji = brand_emojis.get(brand, 'üåü')
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
    title = generate_russian_title(content, brand)
    
    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –ø–æ—Å—Ç
    post = f"{emoji} <b>{title}</b>\n\n"
    post += f"üìñ {content}\n\n"
    
    # –î–æ–±–∞–≤–ª—è–µ–º —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π
    expert_notes = [
        "–ò–Ω—Å–∞–π–¥–µ—Ä—ã –æ—Ç–º–µ—á–∞—é—Ç —Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –¥–∏–∑–∞–π–Ω—É –∏ –º–∞—Ç–µ—Ä–∏–∞–ª–∞–º.",
        "–ö–æ–ª–ª–µ–∫—Ü–∏—è —É–∂–µ –≤—ã–∑–≤–∞–ª–∞ –∞–∂–∏–æ—Ç–∞–∂ —Å—Ä–µ–¥–∏ –≤–µ–¥—É—â–∏—Ö fashion-–∫—Ä–∏—Ç–∏–∫–æ–≤.",
        "–û–∂–∏–¥–∞–µ—Ç—Å—è, —á—Ç–æ —ç—Ç–æ—Ç —Ä–µ–ª–∏–∑ —Å—Ç–∞–Ω–µ—Ç –∫—É–ª—å—Ç–æ–≤—ã–º —Å—Ä–µ–¥–∏ —Ü–µ–Ω–∏—Ç–µ–ª–µ–π –º–æ–¥—ã.",
        "–≠–∫—Å–ø–µ—Ä—Ç—ã –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É—é—Ç –≤—ã—Å–æ–∫–∏–π —Å–ø—Ä–æ—Å –Ω–∞ –Ω–æ–≤–∏–Ω–∫—É –≤ –ª—é–∫—Å–æ–≤—ã—Ö –±—É—Ç–∏–∫–∞—Ö.",
        "–î–∏–∑–∞–π–Ω–µ—Ä—ã –±—Ä–µ–Ω–¥–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ —Å–æ–≤–µ—Ä—à–µ–Ω–Ω–æ –Ω–æ–≤—É—é –∫–æ–Ω—Ü–µ–ø—Ü–∏—é —Å—Ç–∏–ª—è.",
        "Fashion-—Å–æ–æ–±—â–µ—Å—Ç–≤–æ –∞–∫—Ç–∏–≤–Ω–æ –æ–±—Å—É–∂–¥–∞–µ—Ç –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è –±—Ä–µ–Ω–¥–∞.",
        "–ö–æ–ª–ª–∞–±–æ—Ä–∞—Ü–∏—è –æ–±–µ—â–∞–µ—Ç —Å—Ç–∞—Ç—å –æ–¥–Ω–æ–π –∏–∑ —Å–∞–º—ã—Ö –∑–∞–º–µ—Ç–Ω—ã—Ö –≤ —ç—Ç–æ–º —Å–µ–∑–æ–Ω–µ."
    ]
    
    post += f"üíé <i>{random.choice(expert_notes)}</i>"

    return post

def fetch_article_content(url):
    """–ü–æ–ª—É—á–∞–µ—Ç –ø–æ–ª–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç–∞—Ç—å–∏"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        
        return response.text
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å—Ç–∞—Ç—å–∏: {e}")
        return None

def find_luxury_news():
    """–ò—â–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –ø—Ä–æ luxury –±—Ä–µ–Ω–¥—ã –≤–æ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö"""
    random.shuffle(SOURCES)  # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
    
    for source in SOURCES[:8]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ 8 –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        try:
            print(f"üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º {source['name']}...")
            
            feed = feedparser.parse(source['url'])
            
            if not feed.entries:
                print(f"   ‚ùå –ù–µ—Ç –Ω–æ–≤–æ—Å—Ç–µ–π –≤ {source['name']}")
                continue
            
            # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º –∑–∞–ø–∏—Å–∏ –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
            entries = feed.entries[:15]
            random.shuffle(entries)
            
            for entry in entries:
                title = getattr(entry, 'title', '')
                description = getattr(entry, 'description', '')
                link = getattr(entry, 'link', '')
                
                if not title:
                    continue
                    
                content = f"{title}. {description}"
                
                # –ò—â–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏—è luxury –±—Ä–µ–Ω–¥–æ–≤
                for brand in LUXURY_BRANDS:
                    if brand.lower() in content.lower():
                        print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–∞ –Ω–æ–≤–æ—Å—Ç—å –ø—Ä–æ {brand}")
                        
                        try:
                            # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—É—é —Å—Ç–∞—Ç—å—é
                            article_html = fetch_article_content(link)
                            if not article_html:
                                continue
                                
                            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                            image_url = extract_image_from_html(article_html, link)
                            
                            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
                            if source['lang'] == 'en':
                                russian_content = translate_to_russian(content)
                            else:
                                russian_content = content
                            
                            main_content = extract_main_content(russian_content)
                            
                            if len(main_content) < 50:  # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç
                                continue
                                
                            # –°–æ–∑–¥–∞–µ–º —ç–∫—Å–∫–ª—é–∑–∏–≤–Ω—ã–π –ø–æ—Å—Ç
                            post = create_luxury_post(brand, main_content, image_url)
                            
                            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ –∫–∞–Ω–∞–ª
                            if image_url:
                                # –ü—Ä–æ–±—É–µ–º –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —Å –∫–∞—Ä—Ç–∏–Ω–∫–æ–π
                                photo_url = f'https://api.telegram.org/bot{BOT_TOKEN}/sendPhoto'
                                photo_data = {
                                    'chat_id': CHANNEL,
                                    'caption': post,
                                    'parse_mode': 'HTML'
                                }
                                
                                # –°–∫–∞—á–∏–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                                try:
                                    image_response = requests.get(image_url, timeout=10)
                                    if image_response.status_code == 200:
                                        files = {'photo': image_response.content}
                                        response = requests.post(photo_url, data=photo_data, files=files)
                                        if response.status_code == 200:
                                            print(f"   ‚úÖ –ü–æ—Å—Ç —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω!")
                                            return True
                                except:
                                    pass  # –ï—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å —Å –∫–∞—Ä—Ç–∏–Ω–∫–æ–π, –ø—Ä–æ–±—É–µ–º –±–µ–∑
                            
                            # –û—Ç–ø—Ä–∞–≤–∫–∞ –±–µ–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                            url = f'https://api.telegram.org/bot{BOT_TOKEN}/sendMessage'
                            data = {
                                'chat_id': CHANNEL,
                                'text': post,
                                'parse_mode': 'HTML'
                            }
                            
                            response = requests.post(url, data=data)
                            if response.status_code == 200:
                                print(f"   ‚úÖ –ü–æ—Å—Ç –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω!")
                                return True
                                
                        except Exception as e:
                            print(f"   ‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç–∞—Ç—å–∏: {e}")
                            continue
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å –∏—Å—Ç–æ—á–Ω–∏–∫–æ–º {source['name']}: {e}")
            continue
    
    return False

if __name__ == "__main__":
    print("üöÄ –ó–∞–ø—É—Å–∫ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞ luxury –Ω–æ–≤–æ—Å—Ç–µ–π...")
    print(f"üìö –í—Å–µ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {len(SOURCES)}")
    
    # –ò—â–µ–º –Ω–∞—Å—Ç–æ—è—â–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –ø—Ä–æ luxury –±—Ä–µ–Ω–¥—ã
    success = find_luxury_news()
    
    if not success:
        print("‚ùå –ü–æ–¥—Ö–æ–¥—è—â–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ —ç—Ç–æ–º —Ü–∏–∫–ª–µ")

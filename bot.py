import requests
import os
import re
import random
from bs4 import BeautifulSoup
import feedparser
from datetime import datetime, timedelta
import time
import json
import logging
import hashlib
from urllib.parse import urljoin
import sqlite3
from contextlib import contextmanager

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
BOT_TOKEN = os.environ['BOT_TOKEN']
CHANNEL = os.environ['CHANNEL']

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
def init_database():
    conn = sqlite3.connect('news_bot.db')
    cursor = conn.cursor()
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS sent_news (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            news_hash TEXT UNIQUE,
            brand TEXT,
            title TEXT,
            sent_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_hash ON sent_news(news_hash)')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_date ON sent_news(sent_date)')
    conn.commit()
    conn.close()

@contextmanager
def get_db_connection():
    conn = sqlite3.connect('news_bot.db')
    try:
        yield conn
    finally:
        conn.close()

def is_news_sent(news_hash):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –±—ã–ª–∞ –ª–∏ –Ω–æ–≤–æ—Å—Ç—å —É–∂–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∞"""
    with get_db_connection() as conn:
        cursor = conn.cursor()
        cursor.execute('SELECT 1 FROM sent_news WHERE news_hash = ?', (news_hash,))
        return cursor.fetchone() is not None

def mark_news_sent(news_hash, brand, title):
    """–ü–æ–º–µ—á–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç—å –∫–∞–∫ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—É—é"""
    with get_db_connection() as conn:
        cursor = conn.cursor()
        try:
            cursor.execute(
                'INSERT INTO sent_news (news_hash, brand, title) VALUES (?, ?, ?)',
                (news_hash, brand, title[:200])  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É title
            )
            conn.commit()
        except sqlite3.IntegrityError:
            pass  # –£–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç

def cleanup_old_news(days=7):
    """–û—á–∏—â–∞–µ—Ç —Å—Ç–∞—Ä—ã–µ –∑–∞–ø–∏—Å–∏ –∏–∑ –±–∞–∑—ã"""
    with get_db_connection() as conn:
        cursor = conn.cursor()
        cursor.execute('DELETE FROM sent_news WHERE sent_date < datetime("now", ?)', (f"-{days} days",))
        conn.commit()

# –°—Ç–∏–ª–∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è Telegram
class TextStyler:
    @staticmethod
    def bold(text):
        return f"<b>{text}</b>"

    @staticmethod
    def italic(text):
        return f"<i>{text}</i>"

    @staticmethod
    def underline(text):
        return f"<u>{text}</u>"

    @staticmethod
    def create_header(text, emoji="‚ú®"):
        return f"{emoji} {TextStyler.bold(text.upper())}"

    @staticmethod
    def create_quote(text):
        return f"‚ùù{text}‚ùû"

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç–∏–ª–µ—Ä–∞
styler = TextStyler()

# –ë–ê–ó–ê –ò–°–¢–û–ß–ù–ò–ö–û–í
SOURCES = [
    {'name': 'Vogue', 'url': 'https://www.vogue.com/rss', 'lang': 'en'},
    {'name': 'Business of Fashion', 'url': 'https://www.businessoffashion.com/feed', 'lang': 'en'},
    {'name': 'Hypebeast', 'url': 'https://hypebeast.com/fashion/feed', 'lang': 'en'},
    {'name': 'Highsnobiety', 'url': 'https://www.highsnobiety.com/feed/', 'lang': 'en'},
    {'name': 'Fashionista', 'url': 'https://fashionista.com/.rss', 'lang': 'en'},
    {'name': 'WWD', 'url': 'https://wwd.com/feed/', 'lang': 'en'},
    {'name': 'The Cut', 'url': 'https://www.thecut.com/rss/index.xml', 'lang': 'en'},
    {'name': 'Complex', 'url': 'https://www.complex.com/feeds/style', 'lang': 'en'},
    {'name': 'Sneaker News', 'url': 'https://sneakernews.com/feed/', 'lang': 'en'},
    {'name': 'Nice Kicks', 'url': 'https://www.nicekicks.com/feed/', 'lang': 'en'},
    {'name': 'Kicks On Fire', 'url': 'https://www.kicksonfire.com/feed/', 'lang': 'en'},
    {'name': 'Robb Report', 'url': 'https://robbreport.com/feed/', 'lang': 'en'},
    {'name': "Harper's Bazaar", 'url': 'https://www.harpersbazaar.com/feed/rss/', 'lang': 'en'},
    {'name': 'Elle Global', 'url': 'https://www.elle.com/rss/all.xml', 'lang': 'en'},
    {'name': 'NYT Fashion', 'url': 'https://rss.nytimes.com/services/xml/rss/nyt/FashionandStyle.xml', 'lang': 'en'},
    {'name': 'Guardian Fashion', 'url': 'https://www.theguardian.com/fashion/rss', 'lang': 'en'},
    {'name': 'Dazed', 'url': 'https://www.dazeddigital.com/rss', 'lang': 'en'},
    {'name': 'i-D Magazine', 'url': 'https://i-d.vice.com/en_us/rss', 'lang': 'en'},
    {'name': 'SSENSE', 'url': 'https://www.ssense.com/en-us/feed', 'lang': 'en'},
    {'name': 'Grailed', 'url': 'https://www.grailed.com/drycleanonly/feed', 'lang': 'en'},
]

# –†–ê–°–®–ò–†–ï–ù–ù–´–ô –°–ü–ò–°–û–ö –ë–†–ï–ù–î–û–í
BRANDS = [
    'Gucci', 'Prada', 'Dior', 'Chanel', 'Louis Vuitton', 'Balenciaga',
    'Versace', 'Hermes', 'Valentino', 'Fendi', 'Dolce & Gabbana',
    'Bottega Veneta', 'Loewe', 'Off-White', 'Balmain', 'Givenchy',
    'Burberry', 'Tom Ford', 'Alexander McQueen', 'Saint Laurent',
    'Celine', 'JW Anderson', 'Vetements', 'Comme des Gar√ßons',
    'Maison Margiela', 'Acne Studios', 'Issey Miyake', 'Kenzo',
    'Moschino', 'Raf Simons', 'Rick Owens', 'Yves Saint Laurent',
    'Miu Miu', 'Moncler', 'Stone Island', 'Palm Angels',
    'Supreme', 'Palace', 'Stussy', 'Bape', 'Kith', 'Noah',
    'Aime Leon Dore', 'Carhartt WIP', 'Brain Dead', 'Awake NY',
    'Fear of God', 'Essentials', 'Rhude', 'Amiri', 'A-Cold-Wall',
    'Nike', 'Jordan', 'Adidas', 'New Balance', 'Converse',
]

# –≠–º–æ–¥–∑–∏ –¥–ª—è –±—Ä–µ–Ω–¥–æ–≤
BRAND_EMOJIS = {
    'Gucci': 'üêç', 'Prada': 'üî∫', 'Dior': 'üåπ', 'Chanel': 'üëë',
    'Louis Vuitton': 'üß≥', 'Balenciaga': 'üëü', 'Versace': 'üåû',
    'Hermes': 'üü†', 'Valentino': 'üî¥', 'Fendi': 'üü°',
    'Raf Simons': 'üé®', 'Rick Owens': '‚ö´', 'Yves Saint Laurent': 'üíÑ',
    'Supreme': 'üî¥', 'Palace': 'üî∑', 'Bape': 'üêí', 'Stussy': 'üèÑ',
    'Nike': 'üëü', 'Jordan': 'üÖ∞Ô∏è', 'Adidas': '‚ùå', 'Off-White': 'üü®',
    'Stone Island': 'üß≠', 'Moncler': 'ü¶¢', 'Bottega Veneta': 'üü¢',
    'Loewe': 'üêò', 'Givenchy': '‚öúÔ∏è', 'Burberry': 'üß•', 'Tom Ford': 'üï∂Ô∏è',
    'Alexander McQueen': 'üíÄ', 'Celine': '‚ö°', 'Vetements': 'üîµ',
    'Maison Margiela': 'ü•º', 'Acne Studios': 'üåÄ', 'Comme des Gar√ßons': '‚ù§Ô∏è',
    'default': 'üëó'
}

class AdvancedAITranslator:
    def __init__(self):
        self.cache = {}
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })

    def translate_with_deepl(self, text):
        """–ü–µ—Ä–µ–≤–æ–¥ —á–µ—Ä–µ–∑ DeepL"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–µ–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π API DeepL
            url = "https://api-free.deepl.com/v2/translate"
            params = {
                'auth_key': 'free',
                'text': text,
                'target_lang': 'RU',
                'source_lang': 'EN'
            }
            response = self.session.post(url, data=params, timeout=10)
            if response.status_code == 200:
                result = response.json()
                return result['translations'][0]['text']
        except Exception as e:
            logger.warning(f"DeepL translation failed: {e}")
        return None

    def translate_with_google(self, text):
        """–ü–µ—Ä–µ–≤–æ–¥ —á–µ—Ä–µ–∑ Google Translate"""
        try:
            url = "https://translate.googleapis.com/translate_a/single"
            params = {
                'client': 'gtx',
                'sl': 'en',
                'tl': 'ru',
                'dt': 't',
                'q': text
            }
            response = self.session.get(url, params=params, timeout=10)
            if response.status_code == 200:
                result = response.json()
                return ''.join([item[0] for item in result[0] if item[0]])
        except Exception as e:
            logger.warning(f"Google translation failed: {e}")
        return None

    def translate_with_libre(self, text):
        """–ü–µ—Ä–µ–≤–æ–¥ —á–µ—Ä–µ–∑ LibreTranslate"""
        try:
            url = "https://libretranslate.de/translate"
            data = {
                'q': text,
                'source': 'en',
                'target': 'ru',
                'format': 'text'
            }
            response = self.session.post(url, json=data, timeout=15)
            if response.status_code == 200:
                result = response.json()
                return result['translatedText']
        except Exception as e:
            logger.warning(f"LibreTranslate failed: {e}")
        return None

    def smart_translate(self, text):
        """–£–º–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ª—É—á—à–µ–≥–æ –¥–æ—Å—Ç—É–ø–Ω–æ–≥–æ —Å–µ—Ä–≤–∏—Å–∞"""
        if not text or len(text.strip()) < 10:
            return text

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        cache_key = hashlib.md5(text.encode()).hexdigest()
        if cache_key in self.cache:
            return self.cache[cache_key]

        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–∏ –≤ –ø–æ—Ä—è–¥–∫–µ –∫–∞—á–µ—Å—Ç–≤–∞
        translators = [
            self.translate_with_deepl,
            self.translate_with_google,
            self.translate_with_libre
        ]

        translated = None
        for translator in translators:
            translated = translator(text)
            if translated and len(translated) > len(text) * 0.3:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –ø–µ—Ä–µ–≤–æ–¥ –∞–¥–µ–∫–≤–∞—Ç–Ω—ã–π
                break

        # –ï—Å–ª–∏ –≤—Å–µ –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–∏ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback
        if not translated:
            translated = self.fallback_translate(text)

        # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        if translated:
            self.cache[cache_key] = translated
            return translated

        return text

    def fallback_translate(self, text):
        """–†–µ–∑–µ—Ä–≤–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∞–≤–∏–ª"""
        translations = {
            'collection': '–∫–æ–ª–ª–µ–∫—Ü–∏—è', 'sneakers': '–∫—Ä–æ—Å—Å–æ–≤–∫–∏', 'handbag': '—Å—É–º–∫–∞',
            'accessories': '–∞–∫—Å–µ—Å—Å—É–∞—Ä—ã', 'runway': '–ø–æ–∫–∞–∑', 'designer': '–¥–∏–∑–∞–π–Ω–µ—Ä',
            'luxury': '–ª—é–∫—Å', 'limited': '–ª–∏–º–∏—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π', 'exclusive': '—ç–∫—Å–∫–ª—é–∑–∏–≤–Ω—ã–π',
            'collaboration': '–∫–æ–ª–ª–∞–±–æ—Ä–∞—Ü–∏—è', 'release': '—Ä–µ–ª–∏–∑', 'announced': '–∞–Ω–æ–Ω—Å–∏—Ä–æ–≤–∞–ª',
            'launched': '–∑–∞–ø—É—Å—Ç–∏–ª', 'new': '–Ω–æ–≤—ã–π', 'innovative': '–∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π',
            'revolutionary': '—Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π', 'capsule': '–∫–∞–ø—Å—É–ª–∞', 'campaign': '–∫–∞–º–ø–∞–Ω–∏—è',
            'show': '—à–æ—É', 'fashion': '–º–æ–¥–∞', 'style': '—Å—Ç–∏–ª—å', 'trend': '—Ç—Ä–µ–Ω–¥',
            'premium': '–ø—Ä–µ–º–∏—É–º', 'quality': '–∫–∞—á–µ—Å—Ç–≤–æ', 'craftsmanship': '–º–∞—Å—Ç–µ—Ä—Å—Ç–≤–æ'
        }

        translated = text
        for en, ru in translations.items():
            translated = re.sub(rf'\b{en}\b', ru, translated, flags=re.IGNORECASE)
        
        return translated

    def generate_unique_expert_comment(self, brand, content):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        content_lower = content.lower()
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–µ–º—É –Ω–æ–≤–æ—Å—Ç–∏
        if any(word in content_lower for word in ['–∫–æ–ª–ª–∞–±–æ—Ä–∞—Ü–∏—è', 'collaboration', 'collab']):
            theme = 'collaboration'
            templates = [
                f"ü§ù {styler.bold('–°–¢–†–ê–¢–ï–ì–ò–ß–ï–°–ö–ò–ô –ê–õ–¨–Ø–ù–°')}: {brand} –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –∫—Ä–µ–∞—Ç–∏–≤–Ω—ã–µ –≤—Å–µ–ª–µ–Ω–Ω—ã–µ, —Å–æ–∑–¥–∞–≤–∞—è —É–Ω–∏–∫–∞–ª—å–Ω—ã–π —Å–∏–Ω—Ç–µ–∑ —Å—Ç–∏–ª–µ–π.",
                f"üé≠ {styler.bold('–¢–í–û–†–ß–ï–°–ö–ò–ô –î–ò–ê–õ–û–ì')}: –≠—Ç–∞ –∫–æ–ª–ª–∞–±–æ—Ä–∞—Ü–∏—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∫–∞–∫ {brand} –ø–µ—Ä–µ–æ—Å–º—ã—Å–ª–∏–≤–∞–µ—Ç –≥—Ä–∞–Ω–∏—Ü—ã –º–æ–¥—ã —á–µ—Ä–µ–∑ –¥–∏–∞–ª–æ–≥ —Å –Ω–æ–≤—ã–º –ø–∞—Ä—Ç–Ω–µ—Ä–æ–º.",
                f"‚ö° {styler.bold('–°–ò–ù–ï–†–ì–ò–Ø –¢–ê–õ–ê–ù–¢–û–í')}: –°–æ–≤–º–µ—Å—Ç–Ω—ã–π –ø—Ä–æ–µ–∫—Ç {brand} —Ä–æ–∂–¥–∞–µ—Ç –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–µ —ç—Å—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ—à–µ–Ω–∏—è, –æ–±—ä–µ–¥–∏–Ω—è—è –ª—É—á—à–µ–µ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –º–∏—Ä–æ–≤.",
            ]
        elif any(word in content_lower for word in ['–∞—Ä—Ö–∏–≤', 'vintage', '—Ä–µ—Ç—Ä–æ', '–∞—Ä—Ö–∏–≤–Ω—ã–π']):
            theme = 'archive'
            templates = [
                f"üèõÔ∏è {styler.bold('–ò–°–¢–û–†–ò–ß–ï–°–ö–û–ï –ù–ê–°–õ–ï–î–ò–ï')}: {brand} –≤–æ–∑—Ä–æ–∂–¥–∞–µ—Ç –∞—Ä—Ö–∏–≤–Ω—ã–µ –Ω–∞—Ö–æ–¥–∫–∏, –ø–µ—Ä–µ–æ—Å–º—ã—Å–ª–∏–≤–∞—è –∫–ª–∞—Å—Å–∏–∫—É —á–µ—Ä–µ–∑ –ø—Ä–∏–∑–º—É —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ—Å—Ç–∏.",
                f"üìú {styler.bold'–ù–û–°–¢–ê–õ–¨–ì–ò–Ø –° –ü–†–ò–¶–ï–õ–û–ú –ù–ê –ë–£–î–£–©–ï–ï')}: –û–±—Ä–∞—â–∞—è—Å—å –∫ –∞—Ä—Ö–∏–≤–∞–º, {brand} –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç timeless-–ø–æ–¥—Ö–æ–¥ –∫ –¥–∏–∑–∞–π–Ω—É.",
                f"üíé {styler.bold('–í–ï–ß–ù–´–ï –¶–ï–ù–ù–û–°–¢–ò')}: –ê—Ä—Ö–∏–≤–Ω–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è {brand} –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç - –Ω–∞—Å—Ç–æ—è—â–∞—è —Ä–æ—Å–∫–æ—à—å –Ω–µ –ø–æ–¥–≤–ª–∞—Å—Ç–Ω–∞ –≤—Ä–µ–º–µ–Ω–∏.",
            ]
        elif any(word in content_lower for word in ['—É—Å—Ç–æ–π—á–∏–≤', 'sustainable', '—ç–∫–æ–ª–æ–≥–∏—á', '—ç–∫–æ']):
            theme = 'sustainable'
            templates = [
                f"üå± {styler.bold('–û–°–û–ó–ù–ê–ù–ù–´–ô –ü–û–î–•–û–î')}: {brand} –∑–∞–¥–∞–µ—Ç –Ω–æ–≤—ã–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã –≤ sustainable-–º–æ–¥–µ, —Å–æ—á–µ—Ç–∞—è —Ä–æ—Å–∫–æ—à—å –∏ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å.",
                f"‚ôªÔ∏è {styler.bold('–≠–ö–û-–†–ï–í–û–õ–Æ–¶–ò–Ø')}: –ö–æ–ª–ª–µ–∫—Ü–∏—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç commitment {brand} –∫ —É—Å—Ç–æ–π—á–∏–≤–æ–º—É —Ä–∞–∑–≤–∏—Ç–∏—é –∏ –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–º –º–∞—Ç–µ—Ä–∏–∞–ª–∞–º.",
                f"üåç {styler.bold('–ú–û–î–ê –ë–£–î–£–©–ï–ì–û')}: {brand} –ø–µ—Ä–µ–æ—Å–º—ã—Å–ª–∏–≤–∞–µ—Ç –ª—é–∫—Å —á–µ—Ä–µ–∑ –ø—Ä–∏–∑–º—É —ç–∫–æ–ª–æ–≥–∏—á–Ω–æ—Å—Ç–∏ –∏ –æ—Å–æ–∑–Ω–∞–Ω–Ω–æ–≥–æ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è.",
            ]
        elif any(word in content_lower for word in ['–∫—Ä–æ—Å—Å–æ–≤–∫', 'sneaker', '–æ–±—É–≤—å']):
            theme = 'sneakers'
            templates = [
                f"üëü {styler.bold('–ö–£–õ–¨–¢–£–†–ù–´–ô –§–ï–ù–û–ú–ï–ù')}: –ù–æ–≤—ã–µ –∫—Ä–æ—Å—Å–æ–≤–∫–∏ {brand} –æ–±–µ—â–∞—é—Ç —Å—Ç–∞—Ç—å must-have —Å–µ–∑–æ–Ω–∞, –æ–±—ä–µ–¥–∏–Ω—è—è –∫–æ–º—Ñ–æ—Ä—Ç –∏ —Å—Ç–∏–ª—å.",
                f"üî• {styler.bold('–•–ê–ô–ü-–ú–ê–®–ò–ù–ê')}: {brand} –∑–∞–ø—É—Å–∫–∞–µ—Ç –æ—á–µ—Ä–µ–¥–Ω—É—é –≤–æ–ª–Ω—É –∞–∂–∏–æ—Ç–∞–∂–∞ –≤ –∫—Ä–æ—Å—Å–æ–≤–æ—á–Ω–æ–π –∏–Ω–¥—É—Å—Ç—Ä–∏–∏.",
                f"üéØ {styler.bold('–¢–û–ß–ù–´–ô –í–´–°–¢–†–ï–õ')}: –ö–æ–ª–ª–µ–∫—Ü–∏—è –æ–±—É–≤–∏ {brand} –∏–¥–µ–∞–ª—å–Ω–æ –ø–æ–ø–∞–¥–∞–µ—Ç –≤ –∑–∞–ø—Ä–æ—Å—ã —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—è.",
            ]
        else:
            theme = 'collection'
            templates = [
                f"üé® {styler.bold('–¢–í–û–†–ß–ï–°–ö–ò–ô –ü–†–û–†–´–í')}: {brand} –ø–µ—Ä–µ–æ—Å–º—ã—Å–ª–∏–≤–∞–µ—Ç –∫–∞–Ω–æ–Ω—ã —Ä–æ—Å–∫–æ—à–∏, –ø—Ä–µ–¥–ª–∞–≥–∞—è —Å–≤–µ–∂–∏–π –≤–∑–≥–ª—è–¥ –Ω–∞ –ø—Ä–∏–≤—ã—á–Ω—ã–µ —Å–∏–ª—É—ç—Ç—ã.",
                f"üí´ {styler.bold('–ò–ù–ù–û–í–ê–¶–ò–Ø –í –î–ï–¢–ê–õ–Ø–•')}: –í –Ω–æ–≤–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏ {brand} –ø—Ä–æ—Å–ª–µ–∂–∏–≤–∞–µ—Ç—Å—è —Å–º–µ–ª—ã–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç —Å –º–∞—Ç–µ—Ä–∏–∞–ª–∞–º–∏ –∏ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–µ–π.",
                f"üîÆ {styler.bold('–¢–†–ï–ù–î–°–ï–¢–¢–ï–†')}: {brand} –∑–∞–¥–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä —Ä–∞–∑–≤–∏—Ç–∏—è –∏–Ω–¥—É—Å—Ç—Ä–∏–∏, –ø—Ä–µ–¥–≤–æ—Å—Ö–∏—â–∞—è –∑–∞–ø—Ä–æ—Å—ã –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è.",
                f"üåü {styler.bold('–ö–£–õ–¨–¢–£–†–ù–´–ô –§–ï–ù–û–ú–ï–ù')}: –†–µ–ª–∏–∑ {brand} –≤—ã—Ö–æ–¥–∏—Ç –∑–∞ —Ä–∞–º–∫–∏ –º–æ–¥—ã, —Å—Ç–∞–Ω–æ–≤—è—Å—å –∞—Ä—Ç-–≤—ã—Å–∫–∞–∑—ã–≤–∞–Ω–∏–µ–º.",
                f"üöÄ {styler.bold('–¢–ï–•–ù–û–õ–û–ì–ò–ß–ï–°–ö–ò–ô –ü–†–û–†–´–í')}: {brand} –≤–Ω–µ–¥—Ä—è–µ—Ç –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è, –º–µ–Ω—è—é—â–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –æ —Ä–æ—Å–∫–æ—à–∏.",
            ]

        # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ —Ñ–∞–∫—Ç—ã –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–µ–º—ã
        random_facts = {
            'collaboration': [
                "–≠–∫—Å–ø–µ—Ä—Ç—ã –æ—Ç–º–µ—á–∞—é—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫—É—é –≤–∞–∂–Ω–æ—Å—Ç—å —ç—Ç–æ–≥–æ –ø–∞—Ä—Ç–Ω–µ—Ä—Å—Ç–≤–∞ –¥–ª—è –æ–±–æ–∏—Ö –±—Ä–µ–Ω–¥–æ–≤.",
                "–û–∂–∏–¥–∞–µ—Ç—Å—è, —á—Ç–æ –∫–æ–ª–ª–∞–±–æ—Ä–∞—Ü–∏—è —Å—Ç–∞–Ω–µ—Ç –æ–¥–Ω–æ–π –∏–∑ —Å–∞–º—ã—Ö –æ–±—Å—É–∂–¥–∞–µ–º—ã—Ö –≤ —ç—Ç–æ–º —Å–µ–∑–æ–Ω–µ.",
                "–ò–Ω—Å–∞–π–¥–µ—Ä—ã –ø—Ä–æ–≥–Ω–æ–∑–∏—é—Ç —Ä–µ–∫–æ—Ä–¥–Ω—ã–π —Å–ø—Ä–æ—Å –Ω–∞ –ª–∏–º–∏—Ç–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é."
            ],
            'archive': [
                "–ê—Ä—Ö–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ø–æ–ª—É—á–∞—é—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∞–ø–≥—Ä–µ–π–¥—ã, —Å–æ—Ö—Ä–∞–Ω—è—è –¥—É—Ö –æ—Ä–∏–≥–∏–Ω–∞–ª–∞.",
                "–ö–æ–ª–ª–µ–∫—Ü–∏–æ–Ω–µ—Ä—ã —É–∂–µ –ø—Ä–æ—è–≤–ª—è—é—Ç –ø–æ–≤—ã—à–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä–µ—Å –∫ —Ä–µ–ª–∏–∑—É.",
                "–ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –æ—Ç—Å—ã–ª–∫–∏ —Å–æ—á–µ—Ç–∞—é—Ç—Å—è —Å –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–º–∏ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ —Ç–µ—Ö–Ω–∏–∫–∞–º–∏."
            ],
            'sustainable': [
                "–ë—Ä–µ–Ω–¥ –∏–Ω–≤–µ—Å—Ç–∏—Ä—É–µ—Ç –≤ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —ç–∫–æ–ª–æ–≥–∏—á–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ —Å–ª–µ–¥—É—é—â–µ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è.",
                "–£—Å—Ç–æ–π—á–∏–≤—ã–π –ø–æ–¥—Ö–æ–¥ —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –∫–ª—é—á–µ–≤—ã–º —ç–ª–µ–º–µ–Ω—Ç–æ–º –î–ù–ö –±—Ä–µ–Ω–¥–∞.",
                "–ö–æ–ª–ª–µ–∫—Ü–∏—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Å–∞–º—ã–º —Å—Ç—Ä–æ–≥–∏–º —ç–∫–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º."
            ],
            'sneakers': [
                "–¢–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏ –≤ –ø–æ–¥–æ—à–≤–µ –∏ –º–∞—Ç–µ—Ä–∏–∞–ª–∞—Ö –≤–ø–µ—á–∞—Ç–ª—è—é—Ç —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–≤.",
                "–û–∂–∏–¥–∞–µ—Ç—Å—è, —á—Ç–æ —Ä–µ–ª–∏–∑ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç –Ω–æ–≤—ã–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã –≤ —Å–µ–≥–º–µ–Ω—Ç–µ –ø—Ä–µ–º–∏—É–º-–æ–±—É–≤–∏.",
                "–î–∏–∑–∞–π–Ω –∏–¥–µ–∞–ª—å–Ω–æ –±–∞–ª–∞–Ω—Å–∏—Ä—É–µ—Ç –º–µ–∂–¥—É —Å–ø–æ—Ä—Ç–∏–≤–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å—é –∏ —Å—Ç–∏–ª–µ–º."
            ],
            'collection': [
                "–í–Ω–∏–º–∞–Ω–∏–µ –∫ –¥–µ—Ç–∞–ª—è–º –∏ –∫–∞—á–µ—Å—Ç–≤–æ –∏—Å–ø–æ–ª–Ω–µ–Ω–∏—è –≤–ø–µ—á–∞—Ç–ª—è—é—Ç –¥–∞–∂–µ –∏—Å–∫—É—à–µ–Ω–Ω—ã—Ö –∫—Ä–∏—Ç–∏–∫–æ–≤.",
                "–ö–æ–ª–ª–µ–∫—Ü–∏—è –æ—Ç—Ä–∞–∂–∞–µ—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ç—Ä–µ–Ω–¥—ã, —Å–æ—Ö—Ä–∞–Ω—è—è —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –ø–æ—á–µ—Ä–∫ –±—Ä–µ–Ω–¥–∞.",
                "–û–∂–∏–¥–∞–µ—Ç—Å—è, —á—Ç–æ —Ä–µ–ª–∏–∑ –æ–∫–∞–∂–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –≤–ª–∏—è–Ω–∏–µ –Ω–∞ fashion-–∏–Ω–¥—É—Å—Ç—Ä–∏—é."
            ]
        }

        main_comment = random.choice(templates)
        additional_fact = random.choice(random_facts.get(theme, random_facts['collection']))
        
        return f"{main_comment} {additional_fact}"

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–∞
translator = AdvancedAITranslator()

def parse_rss_date(date_string):
    """–ü–∞—Ä—Å–∏—Ç –¥–∞—Ç—É –∏–∑ RSS –≤ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ"""
    if not date_string:
        return None
        
    date_formats = [
        '%a, %d %b %Y %H:%M:%S %Z',
        '%a, %d %b %Y %H:%M:%S %z',
        '%Y-%m-%dT%H:%M:%SZ',
        '%Y-%m-%d %H:%M:%S',
        '%d %b %Y %H:%M:%S'
    ]
    
    for fmt in date_formats:
        try:
            return datetime.strptime(date_string, fmt)
        except:
            continue
    
    try:
        parsed_time = feedparser._parse_date(date_string)
        if parsed_time:
            return datetime.fromtimestamp(time.mktime(parsed_time))
    except:
        pass
        
    return None

def is_recent_news(entry, max_hours_old=24):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –Ω–æ–≤–æ—Å—Ç—å —Å–≤–µ–∂–µ–π"""
    date_fields = ['published', 'updated', 'created', 'pubDate']
    news_date = None
    
    for field in date_fields:
        date_str = getattr(entry, field, None)
        if date_str:
            parsed_date = parse_rss_date(date_str)
            if parsed_date:
                news_date = parsed_date
                break
    
    if not news_date:
        return False
    
    now = datetime.now()
    time_diff = now - news_date
    hours_diff = time_diff.total_seconds() / 3600
    
    return hours_diff <= max_hours_old

def generate_news_hash(entry, brand):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π —Ö—ç—à –¥–ª—è –Ω–æ–≤–æ—Å—Ç–∏"""
    content = f"{entry.title}_{entry.link}_{brand}"
    return hashlib.md5(content.encode()).hexdigest()

def extract_high_quality_image(url):
    """–ê–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        }
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        image_selectors = [
            'meta[property="og:image"]',
            'meta[name="twitter:image"]',
            'meta[property="twitter:image:src"]',
            'meta[name="og:image"]',
            'link[rel="image_src"]',
            'article img',
            '.wp-post-image',
            '.article-image img',
            '.post-image img',
            '.entry-content img',
            '.content img',
            'figure img',
            '.hero-image img',
            '.main-image img',
            '.featured-image img',
            '[class*="image"] img',
            'img[src*="large"]',
            'img[src*="medium"]',
            'img[src*="main"]',
            'img[src*="featured"]',
            'img'
        ]
        
        candidates = []
        for selector in image_selectors:
            elements = soup.select(selector)
            for element in elements:
                if selector.startswith('meta'):
                    image_url = element.get('content', '')
                else:
                    image_url = element.get('src') or element.get('data-src') or element.get('data-lazy-src')
                
                if image_url and is_high_quality_image(image_url):
                    score = rate_image_quality(image_url, element)
                    candidates.append((image_url, score))
        
        if candidates:
            candidates.sort(key=lambda x: x[1], reverse=True)
            best_image = candidates[0][0]
            
            if best_image.startswith('//'):
                best_image = 'https:' + best_image
            elif best_image.startswith('/'):
                best_image = urljoin(url, best_image)
            
            logger.info(f"‚úÖ Found high-quality image")
            return best_image
            
    except Exception as e:
        logger.warning(f"Image extraction error: {e}")
    
    return None

def is_high_quality_image(url):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–º"""
    if not url.startswith(('http://', 'https://')):
        return False
    
    valid_extensions = {'.jpg', '.jpeg', '.png', '.webp'}
    if not any(ext in url.lower() for ext in valid_extensions):
        return False
    
    excluded_terms = ['icon', 'logo', 'thumbnail', 'small', 'avatar', 'sprite', 'pixel']
    if any(term in url.lower() for term in excluded_terms):
        return False
    
    return True

def rate_image_quality(url, element):
    """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
    score = 0
    
    if element.name == 'meta':
        score += 100
    
    width = element.get('width', '')
    height = element.get('height', '')
    if width and height:
        try:
            w = int(''.join(filter(str.isdigit, str(width))))
            h = int(''.join(filter(str.isdigit, str(height))))
            if w > 300 and h > 200:
                score += 50
            if w > 600 and h > 400:
                score += 30
        except:
            pass
    
    quality_indicators = ['large', 'xlarge', 'xxlarge', 'original', 'full', 'main', 'hero', 'featured']
    for indicator in quality_indicators:
        if indicator in url.lower():
            score += 20
    
    return score

def generate_unique_title(brand, content):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
    content_lower = content.lower()
    
    # –†–∞–∑–Ω—ã–µ —Å—Ç–∏–ª–∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
    style_templates = {
        'question': [
            f"{brand} –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é: —á—Ç–æ –∏–∑–≤–µ—Å—Ç–Ω–æ?",
            f"–ß—Ç–æ —Å–∫—Ä—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–π —Ä–µ–ª–∏–∑ {brand}?",
            f"{brand} –º–µ–Ω—è–µ—Ç –ø—Ä–∞–≤–∏–ª–∞ –∏–≥—Ä—ã: –≥–æ—Ç–æ–≤—ã –ª–∏ –≤—ã?",
        ],
        'news': [
            f"{brand} –∞–Ω–æ–Ω—Å–∏—Ä—É–µ—Ç —Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é",
            f"–≠–∫—Å–∫–ª—é–∑–∏–≤: {brand} —Ä–∞—Å–∫—Ä—ã–≤–∞–µ—Ç –¥–µ—Ç–∞–ª–∏ –Ω–æ–≤–æ–≥–æ —Ä–µ–ª–∏–∑–∞",
            f"–°–≤–µ–∂–∏–π –¥—Ä–æ–ø –æ—Ç {brand} —É–∂–µ –∑–¥–µ—Å—å",
        ],
        'creative': [
            f"{brand} √ó –ò—Å–∫—É—Å—Å—Ç–≤–æ: –Ω–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ –º–æ–¥—É",
            f"–†–µ–≤–æ–ª—é—Ü–∏—è —Å—Ç–∏–ª—è: {brand} –∑–∞–¥–∞–µ—Ç —Ç—Ä–µ–Ω–¥—ã",
            f"–ò–∑ –±—É–¥—É—â–µ–≥–æ: {brand} –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏",
        ],
        'minimal': [
            f"{brand} | –ù–æ–≤–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è",
            f"{brand}: —Å–≤–µ–∂–∏–π —Ä–µ–ª–∏–∑",
            f"{brand} –æ–±–Ω–æ–≤–ª—è–µ—Ç –∫–∞—Ç–∞–ª–æ–≥",
        ]
    }
    
    # –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–π —Å—Ç–∏–ª—å
    style = random.choice(list(style_templates.keys()))
    templates = style_templates[style]
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    if any(word in content_lower for word in ['–∫–æ–ª–ª–∞–±–æ—Ä–∞—Ü–∏—è', 'collaboration']):
        templates += [
            f"{brand} –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç—Å—è —Å –Ω–æ–≤—ã–º –ø–∞—Ä—Ç–Ω–µ—Ä–æ–º",
            f"–ö–æ–ª–ª–∞–±–æ—Ä–∞—Ü–∏—è –º–µ—á—Ç—ã: {brand} –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–≤–º–µ—Å—Ç–Ω—ã–π –ø—Ä–æ–µ–∫—Ç",
            f"{brand} √ó [–ë—Ä–µ–Ω–¥]: –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π –∞–ª—å—è–Ω—Å",
        ]
    elif any(word in content_lower for word in ['–∞—Ä—Ö–∏–≤', 'vintage']):
        templates += [
            f"{brand} –≤–æ–∑—Ä–æ–∂–¥–∞–µ—Ç –∞—Ä—Ö–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏",
            f"–ò–∑ –ø—Ä–æ—à–ª–æ–≥–æ –≤ –±—É–¥—É—â–µ–µ: {brand} –∏ –∫–ª–∞—Å—Å–∏–∫–∞",
            f"{brand} | –í–æ–∑–≤—Ä–∞—â–µ–Ω–∏–µ –ª–µ–≥–µ–Ω–¥",
        ]
    elif any(word in content_lower for word in ['—É—Å—Ç–æ–π—á–∏–≤', 'sustainable']):
        templates += [
            f"{brand} –∏ —ç–∫–æ–ª–æ–≥–∏—è: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥",
            f"–ó–µ–ª–µ–Ω–∞—è –º–æ–¥–∞: {brand} –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç sustainable-–∫–æ–ª–ª–µ–∫—Ü–∏—é",
            f"{brand} –∑–∞–±–æ—Ç–∏—Ç—Å—è –æ –ø–ª–∞–Ω–µ—Ç–µ",
        ]
    
    return random.choice(templates)

def create_unique_post(brand, content, image_url=None):
    """–°–æ–∑–¥–∞–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –ø–æ—Å—Ç —Å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    emoji = BRAND_EMOJIS.get(brand, BRAND_EMOJIS['default'])
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫
    title = generate_unique_title(brand, content)
    
    # –£–ª—É—á—à–∞–µ–º –∏ –ø–µ—Ä–µ–≤–æ–¥–∏–º –∫–æ–Ω—Ç–µ–Ω—Ç
    translated_content = translator.smart_translate(content)
    
    # –£–ª—É—á—à–∞–µ–º —Å—Ç–∏–ª—å –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    styled_content = enhance_content_style(translated_content, brand)
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π
    expert_comment = translator.generate_unique_expert_comment(brand, content)
    
    # –†–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –ø–æ—Å—Ç–æ–≤
    post_formats = [
        # –§–æ—Ä–º–∞—Ç 1: –ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π
        lambda: f"{emoji} {styler.create_header(title)}\n\n"
                f"üìñ {styled_content}\n\n"
                f"üíé {expert_comment}\n\n"
                f"{'‚îÄ' * 30}\n\n"
                f"üí¨ {styler.italic('–û–±—Å—É–∂–¥–∞–µ–º –≤ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è—Ö!')}",
        
        # –§–æ—Ä–º–∞—Ç 2: –° —Ü–∏—Ç–∞—Ç–æ–π
        lambda: f"{emoji} {styler.create_header(title)}\n\n"
                f"üìñ {styled_content}\n\n"
                f"‚ú® {styler.create_quote(expert_comment)}\n\n"
                f"{'„Éª' * 20}\n\n"
                f"üéØ {styler.italic('–í–∞—à–µ –º–Ω–µ–Ω–∏–µ?')}",
        
        # –§–æ—Ä–º–∞—Ç 3: –ú–∏–Ω–∏–º–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π
        lambda: f"{emoji} {styler.brand}\n\n"
                f"{styler.bold(title)}\n\n"
                f"{styled_content}\n\n"
                f"üåü {expert_comment}\n\n"
                f"{'„Éª' * 15}",
        
        # –§–æ—Ä–º–∞—Ç 4: –î–µ—Ç–∞–ª—å–Ω—ã–π
        lambda: f"{emoji} {styler.create_header(title, 'üöÄ')}\n\n"
                f"üì∞ {styled_content}\n\n"
                f"üí° {styler.bold('–≠–ö–°–ü–ï–†–¢–ù–û–ï –ú–ù–ï–ù–ò–ï:')}\n"
                f"{expert_comment}\n\n"
                f"{'‚ïê' * 35}\n\n"
                f"üí¨ {styler.italic('–ñ–¥–µ–º –≤–∞—à–∏ –º—ã—Å–ª–∏ –Ω–∏–∂–µ!')}"
    ]
    
    return random.choice(post_formats)()

def enhance_content_style(text, brand):
    """–£–ª—É—á—à–∞–µ—Ç —Å—Ç–∏–ª—å –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
    # –í—ã–¥–µ–ª—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
    important_keywords = [
        '—ç–∫—Å–∫–ª—é–∑–∏–≤–Ω', '–ª–∏–º–∏—Ç–∏—Ä–æ–≤–∞–Ω', '–∫–æ–ª–ª–∞–±–æ—Ä–∞—Ü–∏—è', '—Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω',
        '–∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω', '–∫—É–ª—å—Ç–æ–≤', '–¥–µ–±—é—Ç', '–ø—Ä–µ–º—å–µ—Ä', '–∞–Ω–æ–Ω—Å',
        '—Ä–µ–ª–∏–∑', '–∫–æ–ª–ª–µ–∫—Ü–∏—è', '–∫–∞–ø—Å—É–ª–∞', '–∞—Ä—Ö–∏–≤', '–≤–∏–Ω—Ç–∞–∂',
        '–ø—Ä–µ–º–∏—É–º', '–ª—é–∫—Å', '—Ä–æ—Å–∫–æ—à', '—É–Ω–∏–∫–∞–ª—å–Ω', '–æ—Å–æ–±—ã–π'
    ]
    
    for keyword in important_keywords:
        if keyword in text.lower():
            pattern = re.compile(re.escape(keyword), re.IGNORECASE)
            text = pattern.sub(styler.bold(r'\g<0>'), text)
    
    # –í—ã–¥–µ–ª—è–µ–º –±—Ä–µ–Ω–¥
    if brand in text:
        text = text.replace(brand, styler.bold(brand))
    
    # –î–æ–±–∞–≤–ª—è–µ–º —ç–º–æ–¥–∑–∏
    if any(word in text.lower() for word in ['–∫—Ä–æ—Å—Å–æ–≤–∫–∏', 'sneakers']):
        text = "üëü " + text
    elif any(word in text.lower() for word in ['—Å—É–º–∫', 'bag', 'handbag']):
        text = "üëú " + text
    elif any(word in text.lower() for word in ['–æ–¥–µ–∂–¥', 'collection']):
        text = "üëó " + text
    
    return text

def send_telegram_post(post, image_url=None):
    """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –ø–æ—Å—Ç –≤ Telegram"""
    try:
        if image_url:
            headers = {'User-Agent': 'Mozilla/5.0'}
            image_response = requests.get(image_url, headers=headers, timeout=10)
            if image_response.status_code == 200 and len(image_response.content) > 5000:
                url = f'https://api.telegram.org/bot{BOT_TOKEN}/sendPhoto'
                data = {
                    'chat_id': CHANNEL,
                    'caption': post,
                    'parse_mode': 'HTML'
                }
                files = {'photo': ('image.jpg', image_response.content, 'image/jpeg')}
                response = requests.post(url, data=data, files=files, timeout=30)
                if response.status_code == 200:
                    logger.info("‚úÖ Post sent successfully with image")
                    return True
        
        # Fallback: –æ—Ç–ø—Ä–∞–≤–∫–∞ –±–µ–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        url = f'https://api.telegram.org/bot{BOT_TOKEN}/sendMessage'
        data = {
            'chat_id': CHANNEL,
            'text': post,
            'parse_mode': 'HTML',
            'disable_web_page_preview': True
        }
        response = requests.post(url, json=data, timeout=30)
        return response.status_code == 200
        
    except Exception as e:
        logger.error(f"‚ùå Telegram send error: {e}")
        return False

def find_and_send_single_news():
    """–ò—â–µ—Ç –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –û–î–ù–£ —É–Ω–∏–∫–∞–ª—å–Ω—É—é –Ω–æ–≤–æ—Å—Ç—å"""
    random.shuffle(SOURCES)
    
    logger.info("üîç Searching for ONE unique fresh news...")
    
    for source in SOURCES:
        try:
            logger.info(f"Checking {source['name']}...")
            feed = feedparser.parse(source['url'])
            
            if not feed.entries:
                continue
                
            # –ò—â–µ–º —Å–≤–µ–∂–∏–µ –∑–∞–ø–∏—Å–∏
            fresh_entries = []
            for entry in feed.entries[:10]:
                if is_recent_news(entry, max_hours_old=24):
                    fresh_entries.append(entry)
            
            if not fresh_entries:
                continue
                
            logger.info(f"‚úÖ Found {len(fresh_entries)} fresh news in {source['name']}")
            random.shuffle(fresh_entries)
            
            for entry in fresh_entries:
                title = getattr(entry, 'title', '')
                description = getattr(entry, 'description', '')
                link = getattr(entry, 'link', '')
                
                if not title:
                    continue
                
                # –ò—â–µ–º –±—Ä–µ–Ω–¥—ã –≤ –∫–æ–Ω—Ç–µ–Ω—Ç–µ
                full_content = f"{title} {description}".lower()
                
                for brand in BRANDS:
                    if brand.lower() in full_content:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –æ—Ç–ø—Ä–∞–≤–ª—è–ª–∏ –ª–∏ —É–∂–µ —ç—Ç—É –Ω–æ–≤–æ—Å—Ç—å
                        news_hash = generate_news_hash(entry, brand)
                        if is_news_sent(news_hash):
                            logger.info(f"‚è≠Ô∏è News already sent: {brand} - {title[:50]}...")
                            continue
                        
                        logger.info(f"üéØ Processing unique news: {brand}")
                        
                        try:
                            # –ò—â–µ–º –∫–∞—Ä—Ç–∏–Ω–∫—É
                            image_url = extract_high_quality_image(link)
                            
                            # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
                            original_content = f"{title}. {description}"
                            
                            # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –ø–æ—Å—Ç
                            post = create_unique_post(brand, original_content, image_url)
                            
                            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –ø–æ—Å—Ç
                            if send_telegram_post(post, image_url):
                                # –ü–æ–º–µ—á–∞–µ–º –∫–∞–∫ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—É—é
                                mark_news_sent(news_hash, brand, title)
                                logger.info(f"üéâ Successfully sent UNIQUE news about {brand}")
                                return True  # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –æ–¥–Ω—É –Ω–æ–≤–æ—Å—Ç—å!
                            else:
                                logger.error(f"‚ùå Failed to send post about {brand}")
                                
                        except Exception as e:
                            logger.error(f"üîß Error processing {brand}: {str(e)}")
                        
                        break  # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–π –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±—Ä–µ–Ω–¥–∞
                        
        except Exception as e:
            logger.error(f"‚ùå Error with source {source['name']}: {str(e)}")
            continue
            
    return False

def send_unique_curated_post():
    """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∫—É—Ä–∏—Ä—É–µ–º—ã–π –ø–æ—Å—Ç"""
    logger.info("üé® Creating unique curated post...")
    
    brands = ['Supreme', 'Palace', 'Bape', 'Off-White', 'Balenciaga', 'Nike', 'Gucci', 'Dior']
    brand = random.choice(brands)
    
    curated_themes = [
        f"{brand} –∞–Ω–æ–Ω—Å–∏—Ä—É–µ—Ç –≤—ã–ø—É—Å–∫ –Ω–æ–≤–æ–π –∫–∞–ø—Å—É–ª—å–Ω–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏, –≤–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–Ω–æ–π –∞—Ä—Ö–∏–≤–Ω—ã–º–∏ –Ω–∞—Ö–æ–¥–∫–∞–º–∏ –∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º —É–ª–∏—á–Ω—ã–º –∏—Å–∫—É—Å—Å—Ç–≤–æ–º.",
        f"{brand} –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é, —Å–æ–∑–¥–∞–Ω–Ω—É—é –≤ –∫–æ–ª–ª–∞–±–æ—Ä–∞—Ü–∏–∏ —Å –∏–∑–≤–µ—Å—Ç–Ω—ã–º —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º —Ö—É–¥–æ–∂–Ω–∏–∫–æ–º.",
        f"–ù–æ–≤—ã–π –¥—Ä–æ–ø –æ—Ç {brand} —Å–æ—á–µ—Ç–∞–µ—Ç —ç–ª–µ–º–µ–Ω—Ç—ã —É–ª–∏—á–Ω–æ–≥–æ —Å—Ç–∏–ª—è –∏ –≤—ã—Å–æ–∫–æ–π –º–æ–¥—ã.",
        f"{brand} –∑–∞–ø—É—Å–∫–∞–µ—Ç sustainable –∫–æ–ª–ª–µ–∫—Ü–∏—é —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø–µ—Ä–µ—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤.",
        f"–ê—Ä—Ö–∏–≤–Ω–∞—è –Ω–∞—Ö–æ–¥–∫–∞: {brand} –≤–æ–∑—Ä–æ–∂–¥–∞–µ—Ç –∫—É–ª—å—Ç–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –∞–ø–≥—Ä–µ–π–¥–∞–º–∏.",
    ]
    
    content = random.choice(curated_themes)
    post = create_unique_post(brand, content)
    
    if send_telegram_post(post):
        logger.info("‚úÖ Unique curated post sent successfully!")
        return True
    
    return False

if __name__ == "__main__":
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
    init_database()
    cleanup_old_news(days=7)
    
    logger.info("üöÄ Starting SINGLE NEWS BOT - One unique post per run")
    start_time = time.time()
    
    # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –∏ –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –æ–¥–Ω—É —É–Ω–∏–∫–∞–ª—å–Ω—É—é –Ω–æ–≤–æ—Å—Ç—å
    success = find_and_send_single_news()
    
    # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —É–Ω–∏–∫–∞–ª—å–Ω–æ–π –Ω–æ–≤–æ—Å—Ç–∏, –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –∫—É—Ä–∏—Ä—É–µ–º—ã–π –ø–æ—Å—Ç
    if not success:
        logger.info("üìù No unique news found, creating curated content...")
        send_unique_curated_post()
    
    execution_time = time.time() - start_time
    logger.info(f"‚è±Ô∏è Execution time: {execution_time:.2f} seconds")
    logger.info("‚úÖ Single news bot finished!")

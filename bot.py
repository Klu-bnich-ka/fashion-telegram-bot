"""
Fashion News Bot - Final Stable Version with Content Filtering
Author: Gemini AI
"""

import os
import re
import time
import hashlib
import sqlite3
import logging
import requests
import feedparser
import random
from typing import List, Optional, Tuple
from dataclasses import dataclass
from urllib.parse import urljoin
from bs4 import BeautifulSoup
from deep_translator import GoogleTranslator
from fake_useragent import UserAgent

# ================= CONFIGURATION & LOGGING =================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger("Bot")

BOT_TOKEN = os.environ.get('BOT_TOKEN')
CHANNEL = os.environ.get('CHANNEL')
DB_NAME = 'news.db'

# –°–ø–∏—Å–æ–∫ —Å–∞–º—ã—Ö –Ω–∞–¥–µ–∂–Ω—ã—Ö RSS –ª–µ–Ω—Ç –¥–ª—è –º–æ–¥—ã
RSS_SOURCES = [
    {'name': 'Vogue', 'url': 'https://www.vogue.com/feed/rss'},
    {'name': 'Fashionista', 'url': 'https://fashionista.com/.rss/full/'},
    {'name': 'Hypebeast', 'url': 'https://hypebeast.com/fashion/feed'},
    {'name': 'Guardian Fashion', 'url': 'https://www.theguardian.com/fashion/rss'}
]

# –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –Ω–æ–≤–æ—Å—Ç–µ–π (—Ç–æ–ª—å–∫–æ –ø—Ä–æ –¥–æ–º–∞ –º–æ–¥—ã, –¥—Ä–æ–ø—ã, –∫–æ–ª–ª–∞–±—ã)
FASHION_KEYWORDS = [
    'fashion house', 'collaboration', 'collab', 'clothing', 
    'drop', 'collection', 'brand', 'designer', 'runway', 
    'couture', 'ready-to-wear', 'capsule', 'sneaker', 'apparel'
]

@dataclass
class Article:
    title: str
    url: str
    content: str
    images: List[str]
    source: str

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
if not BOT_TOKEN or not CHANNEL:
    logger.critical("‚ùå FATAL: BOT_TOKEN or CHANNEL not found in env vars.")
    exit(1) 

# ================= DATABASE LAYER =================
class Database:
    def __init__(self):
        self.conn = sqlite3.connect(DB_NAME)
        self.cursor = self.conn.cursor()
        self.cursor.execute('''
            CREATE TABLE IF NOT EXISTS history (
                hash TEXT PRIMARY KEY,
                title TEXT,
                date TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        self.conn.commit()

    def exists(self, url: str) -> bool:
        h = hashlib.md5(url.encode()).hexdigest()
        res = self.cursor.execute('SELECT 1 FROM history WHERE hash = ?', (h,)).fetchone()
        return res is not None

    def add(self, url: str, title: str):
        h = hashlib.md5(url.encode()).hexdigest()
        try:
            self.cursor.execute('INSERT OR IGNORE INTO history (hash, title) VALUES (?, ?)', (h, title))
            self.conn.commit()
        except Exception as e:
            logger.error(f"DB add error: {e}")

# ================= TOOLS: SANITIZER & TRANSLATOR =================
class TextSanitizer:
    @staticmethod
    def clean(text: str) -> str:
        if not text: return ""
        # 1. –£–¥–∞–ª—è–µ–º HTML —Ç–µ–≥–∏ –∏ –ø–æ–ª—É—á–∞–µ–º —á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç
        text = BeautifulSoup(text, "lxml").get_text(separator=' ')
        # 2. –£–±–∏—Ä–∞–µ–º –º—É—Å–æ—Ä–Ω—ã–µ —Ñ—Ä–∞–∑—ã
        bad_phrases = ['Read more', 'Source:', 'Photo:', 'Courtesy of', 'Click here', 
                       'Subscribe', 'Advertisement', 'Image Credit', 'Shop Now']
        for phrase in bad_phrases:
            text = re.sub(phrase, '', text, flags=re.IGNORECASE)
        # 3. –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r'\s+', ' ', text).strip()
        return text

class TranslatorService:
    def __init__(self):
        self.translator = GoogleTranslator(source='auto', target='ru')

    def translate(self, text: str) -> str:
        try:
            if not text: return ""
            if len(text) > 4000: text = text[:4000]
            time.sleep(1) 
            return self.translator.translate(text)
        except Exception as e:
            logger.error(f"Translation error: {e}")
            return text

# ================= CONTENT EXTRACTOR =================
class Extractor:
    def __init__(self):
        self.ua = UserAgent()
        self.session = requests.Session()
        self.sanitizer = TextSanitizer()

    def _get_article_soup(self, url: str) -> Optional[BeautifulSoup]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏ –æ—á–∏—Å—Ç–∫–∞ HTML —Å—Ç–∞—Ç—å–∏"""
        try:
            headers = {'User-Agent': self.ua.random, 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9'}
            resp = self.session.get(url, headers=headers, timeout=15)
            if resp.status_code != 200:
                logger.warning(f"‚ö†Ô∏è Failed to load content (HTTP {resp.status_code}) from {url}")
                return None
            
            soup = BeautifulSoup(resp.content, 'lxml')
            
            # –£–¥–∞–ª—è–µ–º –º—É—Å–æ—Ä–Ω—ã–µ –±–ª–æ–∫–∏ –¥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞
            for el in soup.find_all(['script', 'style', 'nav', 'footer', 'aside', 'iframe', 'header', '.ad', '.sidebar']):
                if el: el.decompose()
            
            return soup
        except Exception as e:
            logger.error(f"Network or Soup error: {e}")
            return None

    def get_full_content(self, url: str) -> Tuple[Optional[str], List[str]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç (—Å–æ–∫—Ä–∞—â–µ–Ω–æ –¥–æ 3 –∞–±–∑–∞—Ü–µ–≤) –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ —Å—Ç–∞—Ç—å–∏."""
        soup = self._get_article_soup(url)
        if not soup: return None, []
        
        # 1. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞ —Å—Ç–∞—Ç—å–∏
        article_body = soup.find('article') or soup.find('main') or soup.body

        # 2. –ü–æ–∏—Å–∫ —Ç–µ–∫—Å—Ç–∞ (–±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 3 —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞)
        full_text = ""
        if article_body:
            paragraphs = article_body.find_all('p')
            # <--- –ò–ó–ú–ï–ù–ï–ù–ò–ï 1: –°–æ–∫—Ä–∞—â–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –¥–æ 3 –∞–±–∑–∞—Ü–µ–≤ --->
            text_parts = [self.sanitizer.clean(p.get_text()) for p in paragraphs if len(p.get_text()) > 50][:3]
            full_text = "\n\n".join(text_parts)

        # 3. –£–°–ò–õ–ï–ù–ù–´–ô –ü–û–ò–°–ö –ö–ê–†–¢–ò–ù–û–ö
        images = []
        
        # –ò—â–µ–º –≤ <picture> –∏ <figure>
        for tag in article_body.select('picture img, figure img, img[data-src], img[srcset]'):
            src = tag.get('data-src') or tag.get('srcset') or tag.get('src')
            if not src: continue
            
            if ' ' in src and ',' in src: 
                src = src.split(',')[0].strip().split(' ')[0]
            
            if src.startswith('//'): src = 'https:' + src
            if not src.startswith('http'): src = urljoin(url, src)

            # –§–∏–ª—å—Ç—Ä –º—É—Å–æ—Ä–∞ –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞
            if any(x in src.lower() for x in ['logo', 'icon', 'avatar', 'svg', 'thumb', 'small']):
                continue
            if src.endswith(('.jpg', '.jpeg', '.png', '.webp')) and src not in images:
                images.append(src)

        unique_images = list(dict.fromkeys(images))
        logger.info(f"üñºÔ∏è Found {len(unique_images)} unique images for {url}")
        
        return full_text, unique_images[:3]

# ================= TELEGRAM SENDER =================
class TelegramSender:
    def __init__(self):
        self.api = f"https://api.telegram.org/bot{BOT_TOKEN}"

    def send(self, article: Article) -> bool:
        # 1. –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–¥–ø–∏—Å–∏
        caption = f"<b>{article.title}</b>\n\n{article.content}"
        
        # <--- –ò–ó–ú–ï–ù–ï–ù–ò–ï 2: –°–æ–∫—Ä–∞—â–µ–Ω–∏–µ –ø–æ–¥–ø–∏—Å–∏ –≤ Telegram --->
        MAX_CAPTION_LENGTH = 700 
        if len(caption) > MAX_CAPTION_LENGTH: 
            caption = caption[:(MAX_CAPTION_LENGTH - 4)] + "..."
        
        # 2. –õ–æ–≥–∏–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏
        try:
            # –ê. –ï—Å–ª–∏ –µ—Å—Ç—å –∫–∞—Ä—Ç–∏–Ω–∫–∏ (–û—Ç–ø—Ä–∞–≤–∫–∞ –∞–ª—å–±–æ–º–∞)
            if article.images:
                media = []
                for i, img in enumerate(article.images):
                    item = {'type': 'photo', 'media': img}
                    if i == 0: # –ü–æ–¥–ø–∏—Å—å —Ç–æ–ª—å–∫–æ –∫ –ø–µ—Ä–≤–æ–º—É —ç–ª–µ–º–µ–Ω—Ç—É
                        item['caption'] = caption
                        item['parse_mode'] = 'HTML'
                    media.append(item)
                
                r = requests.post(f"{self.api}/sendMediaGroup", json={'chat_id': CHANNEL, 'media': media})
                if r.status_code == 200: return True
                
                logger.warning(f"Failed to send media group. Trying text fallback. Error: {r.text}")
            
            # –ë. –§–æ–ª–±–µ–∫: –û—Ç–ø—Ä–∞–≤–∫–∞ —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–∞
            data = {'chat_id': CHANNEL, 'text': caption, 'parse_mode': 'HTML', 'disable_web_page_preview': True}
            r = requests.post(f"{self.api}/sendMessage", json=data)
            return r.status_code == 200

        except Exception as e:
            logger.error(f"Telegram send critical error: {e}")
            return False

# ================= MAIN CONTROLLER =================

def is_relevant(title: str) -> bool:
    """<--- –ò–ó–ú–ï–ù–ï–ù–ò–ï 3: –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º --->"""
    check_text = title.lower()
    
    # –ï—Å–ª–∏ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ, —Å—á–∏—Ç–∞–µ–º –Ω–æ–≤–æ—Å—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π
    return any(k in check_text for k in FASHION_KEYWORDS)


def run():
    logger.info("üöÄ Bot started (Final Stable Mode)")

    db = Database()
    extractor = Extractor()
    sender = TelegramSender()
    translator_service = TranslatorService()
    
    random.shuffle(RSS_SOURCES)
    
    news_sent = 0
    MAX_NEWS_PER_RUN = 1 
    
    for source in RSS_SOURCES:
        if news_sent >= MAX_NEWS_PER_RUN: break

        logger.info(f"üì° Checking {source['name']}...")
        try:
            feed = feedparser.parse(source['url'])
            if not feed.entries:
                logger.warning(f"Empty feed for {source['name']}")
                continue

            for entry in feed.entries[:5]: # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–æ–ª—å—à–µ –Ω–æ–≤–æ—Å—Ç–µ–π, —á—Ç–æ–±—ã –Ω–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é
                url = entry.link
                title = entry.title
                
                if db.exists(url): continue
                
                # --- –®–∞–≥ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ ---
                if not is_relevant(title):
                    logger.info(f"Title '{title}' is not relevant to drops/collabs. Skipping.")
                    continue
                
                logger.info(f"Found relevant news: {title}")
                
                # 1. –î–æ—Å—Ç–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç —Å —Å–∞–π—Ç–∞
                site_text, site_images = extractor.get_full_content(url)
                
                content_en = site_text or getattr(entry, 'summary', '')
                
                if len(content_en) < 150: 
                    logger.info("Content too short, skipping")
                    continue

                # 2. –ü–µ—Ä–µ–≤–æ–¥–∏–º
                logger.info("Translating...")
                title_ru = translator_service.translate(title)
                content_ru = translator_service.translate(content_en)
                
                # 3. –û—Ç–ø—Ä–∞–≤–ª—è–µ–º
                article = Article(title_ru, url, content_ru, site_images, source['name'])
                if sender.send(article):
                    logger.info("‚úÖ Sent successfully!")
                    db.add(url, title)
                    news_sent += 1
                    break # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–º—É –∑–∞–ø—É—Å–∫—É
                else:
                    logger.error("Failed to send article data.")
                
                time.sleep(5)

        except Exception as e:
            logger.error(f"Error processing source {source['name']}: {e}")

    logger.info("üí§ Cycle finished.")

if __name__ == "__main__":
    run()

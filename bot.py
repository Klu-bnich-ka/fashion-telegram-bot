import requests
import os
import re
import random
from bs4 import BeautifulSoup
import feedparser
from datetime import datetime, timedelta
import time
import logging
import hashlib
from urllib.parse import urljoin
import sqlite3
from googletrans import Translator
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import sent_tokenize, word_tokenize
import string

# –°–∫–∞—á–∏–≤–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è nltk
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
BOT_TOKEN = os.environ['BOT_TOKEN']
CHANNEL = os.environ['CHANNEL']

# 3 –æ—Å–Ω–æ–≤–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞
SOURCES = [
    {
        'name': 'Hypebeast', 
        'url': 'https://hypebeast.com/fashion/feed',
        'base_url': 'https://hypebeast.com'
    },
    {
        'name': 'Highsnobiety', 
        'url': 'https://www.highsnobiety.com/feed/',
        'base_url': 'https://www.highsnobiety.com'
    },
    {
        'name': 'Sneaker News',
        'url': 'https://sneakernews.com/feed/',
        'base_url': 'https://sneakernews.com'
    }
]

class ContentProcessor:
    def __init__(self):
        self.translator = Translator()
        self.stop_words = set(stopwords.words('english'))
        
    def extract_key_points(self, text, max_sentences=3):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = sent_tokenize(text)
        
        # –û—Ü–µ–Ω–∏–≤–∞–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        scored_sentences = []
        for i, sentence in enumerate(sentences):
            score = self.score_sentence(sentence, i, len(sentences))
            scored_sentences.append((sentence, score))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
        scored_sentences.sort(key=lambda x: x[1], reverse=True)
        
        # –ë–µ—Ä–µ–º —Ç–æ–ø –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        top_sentences = [s[0] for s in scored_sentences[:max_sentences]]
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø–æ—Ä—è–¥–∫—É –≤ —Ç–µ–∫—Å—Ç–µ
        final_sentences = []
        for original_sentence in sentences:
            if original_sentence in top_sentences:
                final_sentences.append(original_sentence)
        
        return ' '.join(final_sentences)
    
    def score_sentence(self, sentence, position, total_sentences):
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è"""
        score = 0
        
        # –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤ –Ω–∞—á–∞–ª–µ –æ–±—ã—á–Ω–æ –≤–∞–∂–Ω–µ–µ
        score += (1 - position / total_sentences) * 2
        
        # –î–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (—Å—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –ª—É—á—à–µ)
        words = word_tokenize(sentence)
        if 8 <= len(words) <= 25:
            score += 2
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        important_keywords = [
            'collaboration', 'release', 'limited', 'exclusive', 'new',
            'collection', 'drop', 'launch', 'announce', 'available',
            'first', 'special', 'edition', 'capsule', 'sneaker'
        ]
        
        for keyword in important_keywords:
            if keyword in sentence.lower():
                score += 3
        
        # –ë—Ä–µ–Ω–¥—ã –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏
        brands_in_sentence = any(brand.lower() in sentence.lower() for brand in [
            'Nike', 'Jordan', 'Adidas', 'Supreme', 'Bape', 'Gucci'
        ])
        if brands_in_sentence:
            score += 2
        
        return score
    
    def clean_and_improve_text(self, text):
        """–û—á–∏—â–∞–µ—Ç –∏ —É–ª—É—á—à–∞–µ—Ç —Ç–µ–∫—Å—Ç"""
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'\n+', ' ', text)
        
        # –£–¥–∞–ª—è–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ñ—Ä–∞–∑—ã
        technical_phrases = [
            'read more', 'read full article', 'click here', 'continue reading',
            'source:', 'image credit:', 'photo via', 'courtesy of'
        ]
        
        for phrase in technical_phrases:
            text = re.sub(phrase, '', text, flags=re.IGNORECASE)
        
        # –£–¥–∞–ª—è–µ–º —Å—Å—ã–ª–∫–∏
        text = re.sub(r'http\S+', '', text)
        
        # –£–ª—É—á—à–∞–µ–º –Ω–∞—á–∞–ª–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        text = self.improve_sentence_structure(text)
        
        return text.strip()
    
    def improve_sentence_structure(self, text):
        """–£–ª—É—á—à–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π"""
        # –î–µ–ª–∞–µ–º –ø–µ—Ä–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –±–æ–ª–µ–µ impactful
        sentences = sent_tokenize(text)
        if sentences:
            first_sentence = sentences[0]
            # –£–±–∏—Ä–∞–µ–º –≤–≤–æ–¥–Ω—ã–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
            first_sentence = re.sub(r'^(according to|reports indicate that|it has been announced that)\s+', '', first_sentence, flags=re.IGNORECASE)
            sentences[0] = first_sentence.capitalize()
        
        return ' '.join(sentences)
    
    def smart_translate(self, text):
        """–£–º–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ —Å —É–ª—É—á—à–µ–Ω–∏–µ–º –∫–∞—á–µ—Å—Ç–≤–∞"""
        try:
            if len(text) > 4000:
                text = text[:4000]
            
            translated = self.translator.translate(text, dest='ru')
            
            # –£–ª—É—á—à–∞–µ–º —Ä—É—Å—Å–∫–∏–π —Ç–µ–∫—Å—Ç
            improved_russian = self.improve_russian_text(translated.text)
            return improved_russian
            
        except Exception as e:
            logger.warning(f"Translation failed: {e}")
            return text
    
    def improve_russian_text(self, text):
        """–£–ª—É—á—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
        # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º —á–∞—Å—Ç—ã–µ –æ—à–∏–±–∫–∏ –ø–µ—Ä–µ–≤–æ–¥–∞
        improvements = {
            '—Ä–µ–ª–∏–∑': '—Ä–µ–ª–∏–∑',
            '–∫–æ–ª–ª–∞–±–æ—Ä–∞—Ü–∏—è': '–∫–æ–ª–ª–∞–±–æ—Ä–∞—Ü–∏—è',
            '–∫–æ–ª–ª–µ–∫—Ü–∏—è': '–∫–æ–ª–ª–µ–∫—Ü–∏—è',
            '–∫—Ä–æ—Å—Å–æ–≤–∫–∏': '–∫—Ä–æ—Å—Å–æ–≤–∫–∏',
            '–ª–∏–º–∏—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π': '–ª–∏–º–∏—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π',
            '—ç–∫—Å–∫–ª—é–∑–∏–≤–Ω—ã–π': '—ç–∫—Å–∫–ª—é–∑–∏–≤–Ω—ã–π',
            '–¥–æ—Å—Ç—É–ø–µ–Ω': '–¥–æ—Å—Ç—É–ø–µ–Ω',
            '–∞–Ω–æ–Ω—Å–∏—Ä–æ–≤–∞–ª': '–∞–Ω–æ–Ω—Å–∏—Ä–æ–≤–∞–ª',
            '–∑–∞–ø—É—Å—Ç–∏–ª': '–∑–∞–ø—É—Å—Ç–∏–ª'
        }
        
        for eng, ru in improvements.items():
            text = text.replace(eng, ru)
        
        # –î–µ–ª–∞–µ–º —Ç–µ–∫—Å—Ç –±–æ–ª–µ–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º
        text = text.replace(' ,', ',')
        text = text.replace(' .', '.')
        text = re.sub(r'\s+', ' ', text)
        
        return text

class SmartContentExtractor:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        self.processor = ContentProcessor()
    
    def extract_quality_content(self, url):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        try:
            response = self.session.get(url, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
            for element in soup.find_all(['script', 'style', 'nav', 'footer', 'aside', 'form']):
                element.decompose()
            
            # –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç–∞—Ç—å–∏
            article_content = self.find_article_content(soup)
            
            if not article_content:
                return None, []
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç
            raw_text = article_content.get_text()
            clean_text = self.processor.clean_and_improve_text(raw_text)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã
            key_points = self.processor.extract_key_points(clean_text)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            images = self.extract_quality_images(soup, url)
            
            return key_points, images
            
        except Exception as e:
            logger.error(f"Error extracting content from {url}: {e}")
            return None, []
    
    def find_article_content(self, soup):
        """–ù–∞—Ö–æ–¥–∏—Ç –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç–∞—Ç—å–∏"""
        content_selectors = [
            'article .post-content',
            'article .entry-content',
            'article .article-content',
            'article .content',
            '.post-content',
            '.entry-content',
            '.article-content',
            '.content',
            'article'
        ]
        
        for selector in content_selectors:
            element = soup.select_one(selector)
            if element and len(element.get_text(strip=True)) > 200:
                return element
        
        return soup.find('body')
    
    def extract_quality_images(self, soup, base_url):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–æ–ª—å–∫–æ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        images = []
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –≥–ª–∞–≤–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        priority_selectors = [
            '.wp-post-image',
            '.article-image img',
            '.post-image img',
            '.featured-image img',
            '.hero-image img',
            'figure img',
            '.entry-content img:first-of-type',
            '.content img:first-of-type'
        ]
        
        # –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        for selector in priority_selectors:
            imgs = soup.select(selector)
            for img in imgs[:2]:  # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 2
                src = self.get_image_src(img)
                if src and self.is_quality_image(src):
                    full_url = self.make_absolute_url(src, base_url)
                    if full_url:
                        images.append(full_url)
        
        # –ï—Å–ª–∏ –Ω–µ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã—Ö, –∏—â–µ–º –ª—é–±—ã–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ
        if not images:
            all_imgs = soup.find_all('img')
            for img in all_imgs[:3]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                src = self.get_image_src(img)
                if src and self.is_quality_image(src):
                    full_url = self.make_absolute_url(src, base_url)
                    if full_url:
                        images.append(full_url)
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
        return list(dict.fromkeys(images))[:3]  # –ú–∞–∫—Å–∏–º—É–º 3 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    
    def get_image_src(self, img_element):
        """–ü–æ–ª—É—á–∞–µ—Ç URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ —ç–ª–µ–º–µ–Ω—Ç–∞"""
        return (img_element.get('src') or 
                img_element.get('data-src') or 
                img_element.get('data-lazy-src'))
    
    def make_absolute_url(self, url, base_url):
        """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π URL –≤ –∞–±—Å–æ–ª—é—Ç–Ω—ã–π"""
        if url.startswith('//'):
            return 'https:' + url
        elif url.startswith('/'):
            return urljoin(base_url, url)
        elif url.startswith(('http://', 'https://')):
            return url
        return None
    
    def is_quality_image(self, url):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–º"""
        excluded_terms = [
            'logo', 'icon', 'avatar', 'thumbnail', 'pixel', 'spinner',
            'advertisement', 'banner', 'widget', 'placeholder'
        ]
        
        if any(term in url.lower() for term in excluded_terms):
            return False
        
        valid_extensions = ['.jpg', '.jpeg', '.png', '.webp']
        if not any(ext in url.lower() for ext in valid_extensions):
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –≤ URL (–ø—Ä–∏–∑–Ω–∞–∫ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è)
        size_indicators = ['large', 'xlarge', 'xxlarge', 'original', 'full', 'main']
        if any(indicator in url.lower() for indicator in size_indicators):
            return True
        
        return True

class PostCreator:
    def __init__(self):
        self.processor = ContentProcessor()
    
    def create_clean_post(self, title, content, source, images_count=0):
        """–°–æ–∑–¥–∞–µ—Ç —á–∏—Å—Ç—ã–π –∏ –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–π –ø–æ—Å—Ç"""
        # –£–ª—É—á—à–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
        improved_title = self.improve_title(title)
        
        # –£–ª—É—á—à–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
        improved_content = self.improve_content(content)
        
        # –°–æ–∑–¥–∞–µ–º –ø–æ—Å—Ç
        post = f"<b>{improved_title}</b>\n\n"
        post += f"{improved_content}\n\n"
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö
        if images_count > 0:
            post += f"üñºÔ∏è –í –º–∞—Ç–µ—Ä–∏–∞–ª–µ: {images_count} —Ñ–æ—Ç–æ\n\n"
        
        post += f"üì∞ {source}"
        
        return post
    
    def improve_title(self, title):
        """–£–ª—É—á—à–∞–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫"""
        # –£–±–∏—Ä–∞–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        title = re.sub(r'\s*-\s*[^-]*$', '', title)  # –£–±–∏—Ä–∞–µ–º "- Source Name"
        title = re.sub(r'\s*\|.*$', '', title)  # –£–±–∏—Ä–∞–µ–º "| Section"
        
        # –î–µ–ª–∞–µ–º –ø–µ—Ä–≤—É—é –±—É–∫–≤—É –∑–∞–≥–ª–∞–≤–Ω–æ–π
        if title:
            title = title[0].upper() + title[1:]
        
        return title.strip()
    
    def improve_content(self, content):
        """–£–ª—É—á—à–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ"""
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = sent_tokenize(content)
        
        if not sentences:
            return content
        
        # –í—ã–¥–µ–ª—è–µ–º –∫–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã –∂–∏—Ä–Ω—ã–º
        improved_sentences = []
        for sentence in sentences:
            improved_sentence = self.highlight_key_points(sentence)
            improved_sentences.append(improved_sentence)
        
        return ' '.join(improved_sentences)
    
    def highlight_key_points(self, sentence):
        """–í—ã–¥–µ–ª—è–µ—Ç –∫–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã –∂–∏—Ä–Ω—ã–º"""
        # –ö–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è
        key_phrases = [
            r'–∫–æ–ª–ª–∞–±–æ—Ä–∞—Ü–∏—è\w*',
            r'–ª–∏–º–∏—Ç–∏—Ä–æ–≤–∞–Ω–Ω\w*',
            r'—ç–∫—Å–∫–ª—é–∑–∏–≤–Ω\w*',
            r'—Ä–µ–ª–∏–∑\w*',
            r'–Ω–æ–≤—ã–π –º–æ–¥–µ–ª—å',
            r'–≤–ø–µ—Ä–≤—ã–µ',
            r'–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π —Ç–∏—Ä–∞–∂',
            r'—Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –≤—ã–ø—É—Å–∫',
            r'–∫–∞–ø—Å—É–ª—å–Ω–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è'
        ]
        
        result = sentence
        for phrase in key_phrases:
            matches = re.finditer(phrase, result, re.IGNORECASE)
            for match in matches:
                original = match.group()
                highlighted = f"<b>{original}</b>"
                result = result.replace(original, highlighted)
        
        return result

class DatabaseManager:
    def __init__(self):
        self.init_database()
    
    def init_database(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
        conn = sqlite3.connect('news.db')
        cursor = conn.cursor()
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS sent_news (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                url_hash TEXT UNIQUE,
                source TEXT,
                title TEXT,
                sent_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        conn.commit()
        conn.close()
    
    def is_news_sent(self, url):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –±—ã–ª–∞ –ª–∏ –Ω–æ–≤–æ—Å—Ç—å –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∞"""
        url_hash = hashlib.md5(url.encode()).hexdigest()
        conn = sqlite3.connect('news.db')
        cursor = conn.cursor()
        cursor.execute('SELECT 1 FROM sent_news WHERE url_hash = ?', (url_hash,))
        result = cursor.fetchone() is not None
        conn.close()
        return result
    
    def mark_news_sent(self, url, source, title):
        """–ü–æ–º–µ—á–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç—å –∫–∞–∫ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—É—é"""
        url_hash = hashlib.md5(url.encode()).hexdigest()
        conn = sqlite3.connect('news.db')
        cursor = conn.cursor()
        try:
            cursor.execute(
                'INSERT INTO sent_news (url_hash, source, title) VALUES (?, ?, ?)',
                (url_hash, source, title[:200])
            )
            conn.commit()
        except sqlite3.IntegrityError:
            pass
        conn.close()

class TelegramPublisher:
    def __init__(self, token, channel):
        self.token = token
        self.channel = channel
        self.session = requests.Session()
    
    def send_photo_group(self, caption, photo_urls):
        """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –≥—Ä—É–ø–ø—É —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π —Å –ø–æ–¥–ø–∏—Å—å—é"""
        if not photo_urls:
            return self.send_message(caption)
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –ø–µ—Ä–≤—É—é —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—é —Å –ø–æ–¥–ø–∏—Å—å—é
        first_photo = photo_urls[0]
        
        try:
            # –°–∫–∞—á–∏–≤–∞–µ–º –ø–µ—Ä–≤—É—é —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—é
            response = self.session.get(first_photo, timeout=10)
            if response.status_code != 200:
                return self.send_message(caption)
            
            files = {'photo': ('image.jpg', response.content, 'image/jpeg')}
            data = {
                'chat_id': self.channel,
                'caption': caption,
                'parse_mode': 'HTML'
            }
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –ø–µ—Ä–≤—É—é —Ñ–æ—Ç–æ —Å –ø–æ–¥–ø–∏—Å—å—é
            url = f'https://api.telegram.org/bot{self.token}/sendPhoto'
            response = self.session.post(url, files=files, data=data, timeout=30)
            
            if response.status_code == 200 and len(photo_urls) > 1:
                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Ñ–æ—Ç–æ
                for photo_url in photo_urls[1:3]:  # –ú–∞–∫—Å–∏–º—É–º 3 —Ñ–æ—Ç–æ
                    try:
                        photo_response = self.session.get(photo_url, timeout=10)
                        if photo_response.status_code == 200:
                            files = {'photo': ('image.jpg', photo_response.content, 'image/jpeg')}
                            data = {'chat_id': self.channel}
                            self.session.post(url, files=files, data=data, timeout=30)
                            time.sleep(1)
                    except:
                        continue
            
            return True
            
        except Exception as e:
            logger.error(f"Error sending photos: {e}")
            return self.send_message(caption)
    
    def send_message(self, text):
        """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ"""
        url = f'https://api.telegram.org/bot{self.token}/sendMessage'
        data = {
            'chat_id': self.channel,
            'text': text,
            'parse_mode': 'HTML',
            'disable_web_page_preview': False
        }
        
        try:
            response = self.session.post(url, json=data, timeout=30)
            return response.status_code == 200
        except Exception as e:
            logger.error(f"Error sending message: {e}")
            return False

class FashionNewsBot:
    def __init__(self):
        self.db = DatabaseManager()
        self.extractor = SmartContentExtractor()
        self.publisher = TelegramPublisher(BOT_TOKEN, CHANNEL)
        self.post_creator = PostCreator()
        self.translator = ContentProcessor()
    
    def check_sources(self):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–∞ –Ω–æ–≤—ã–µ –Ω–æ–≤–æ—Å—Ç–∏"""
        all_news = []
        
        for source in SOURCES:
            try:
                logger.info(f"üîç Checking {source['name']}...")
                news_items = self.parse_feed(source)
                all_news.extend(news_items)
                time.sleep(2)
            except Exception as e:
                logger.error(f"Error parsing {source['name']}: {e}")
                continue
        
        return all_news
    
    def parse_feed(self, source):
        """–ü–∞—Ä—Å–∏—Ç RSS —Ñ–∏–¥ –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        news_items = []
        
        try:
            feed = feedparser.parse(source['url'])
            
            for entry in feed.entries[:15]:  # –ë–µ—Ä–µ–º 15 –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –∑–∞–ø–∏—Å–µ–π
                if self.is_recent(entry) and self.is_fashion_related(entry):
                    news_item = {
                        'title': entry.title,
                        'url': entry.link,
                        'source': source['name'],
                        'published': getattr(entry, 'published', ''),
                        'summary': getattr(entry, 'summary', '')[:300]
                    }
                    news_items.append(news_item)
                    
        except Exception as e:
            logger.error(f"Error parsing feed {source['name']}: {e}")
        
        return news_items
    
    def is_recent(self, entry, max_hours=24):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å–≤–µ–∂–∞—è –ª–∏ –Ω–æ–≤–æ—Å—Ç—å"""
        try:
            date_str = getattr(entry, 'published', '')
            if not date_str:
                return True
                
            formats = [
                '%a, %d %b %Y %H:%M:%S %Z',
                '%a, %d %b %Y %H:%M:%S %z',
                '%Y-%m-%dT%H:%M:%SZ'
            ]
            
            for fmt in formats:
                try:
                    news_date = datetime.strptime(date_str, fmt)
                    time_diff = datetime.now() - news_date
                    return time_diff.total_seconds() / 3600 <= max_hours
                except:
                    continue
                    
            return True
        except:
            return True
    
    def is_fashion_related(self, entry):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –ª–∏ –Ω–æ–≤–æ—Å—Ç—å –∫ –º–æ–¥–µ"""
        content = f"{entry.title} {getattr(entry, 'summary', '')}".lower()
        
        fashion_keywords = [
            'sneaker', 'collection', 'collaboration', 'release', 'drop',
            'fashion', 'streetwear', 'luxury', 'designer', 'boot',
            'jacket', 'hoodie', 'shoe', 'apparel', 'capsule'
        ]
        
        return any(keyword in content for keyword in fashion_keywords)
    
    def process_news(self, news_item):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç—å –∏ —Å–æ–∑–¥–∞–µ—Ç –ø–æ—Å—Ç"""
        if self.db.is_news_sent(news_item['url']):
            return None
        
        logger.info(f"üìù Processing: {news_item['title']}")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
        content, images = self.extractor.extract_quality_content(news_item['url'])
        
        if not content:
            content = news_item['summary']
        
        # –ü–µ—Ä–µ–≤–æ–¥–∏–º –∏ —É–ª—É—á—à–∞–µ–º
        translated_title = self.translator.smart_translate(news_item['title'])
        translated_content = self.translator.smart_translate(content)
        
        # –°–æ–∑–¥–∞–µ–º —á–∏—Å—Ç—ã–π –ø–æ—Å—Ç
        post = self.post_creator.create_clean_post(
            translated_title, 
            translated_content, 
            news_item['source'],
            len(images)
        )
        
        # –ü–æ–º–µ—á–∞–µ–º –∫–∞–∫ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—É—é
        self.db.mark_news_sent(news_item['url'], news_item['source'], news_item['title'])
        
        return post, images
    
    def run(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –±–æ—Ç–∞"""
        logger.info("üöÄ Starting Smart Fashion News Bot")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        all_news = self.check_sources()
        logger.info(f"üì∞ Found {len(all_news)} new news items")
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏ –ø—É–±–ª–∏–∫—É–µ–º –∫–∞–∂–¥—É—é –Ω–æ–≤–æ—Å—Ç—å
        success_count = 0
        for news_item in all_news:
            try:
                result = self.process_news(news_item)
                if result:
                    post, images = result
                    
                    # –ü—É–±–ª–∏–∫—É–µ–º –ø–æ—Å—Ç
                    success = self.publisher.send_photo_group(post, images)
                    
                    if success:
                        success_count += 1
                        logger.info(f"‚úÖ Published: {news_item['title'][:50]}...")
                    else:
                        logger.error(f"‚ùå Failed to publish: {news_item['title'][:50]}...")
                    
                    # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –ø–æ—Å—Ç–∞–º–∏
                    if success_count < 3:  # –ú–∞–∫—Å–∏–º—É–º 3 –ø–æ—Å—Ç–∞ –∑–∞ —Ä–∞–∑
                        time.sleep(10)
                    else:
                        break
                    
            except Exception as e:
                logger.error(f"‚ùå Error processing news: {e}")
                continue
        
        logger.info(f"üéâ Published {success_count} news items")

if __name__ == "__main__":
    bot = FashionNewsBot()
    bot.run()
    logger.info("‚úÖ Bot finished!")

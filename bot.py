#!/usr/bin/env python3
# coding: utf-8
"""
Fashion News Bot ‚Äî –≤–µ—Ä—Å–∏—è –¥–ª—è GitHub Actions (HTML-aware + RSS + repo-state commit).
–ó–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –ø–æ cron (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∫–∞–∂–¥—ã–µ 30 –º–∏–Ω—É—Ç).
Secrets: BOT_TOKEN, CHANNEL (–∏–ª–∏ CHAT_ID). –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: DEEPL_KEY.
"""

import os
import json
import time
import hashlib
import logging
from datetime import datetime, timezone
from urllib.parse import urljoin, urlparse

import requests
from bs4 import BeautifulSoup
import feedparser

# --------- –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è ---------
logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")
logger = logging.getLogger("fashion-bot")

# --------- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è ---------
BOT_TOKEN = os.getenv("BOT_TOKEN")  # required
CHANNEL = os.getenv("CHANNEL")      # required (chat_id or @channelusername)
DEEPL_KEY = os.getenv("DEEPL_KEY")  # optional

# –ú–∞–∫—Å–∏–º—É–º –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ –∑–∞–ø—É—Å–∫
MAX_SEND = 3

# –ò—Å—Ç–æ—á–Ω–∏–∫–∏: RSS + –ª–∏—Å—Ç–∏–Ω–≥–æ–≤—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–ø–∞—Ä—Å–µ—Ä —É–º–µ–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –∏ –ø–æ RSS –∏ –ø–æ HTML)
SOURCES = [
    {
        "name": "Hypebeast",
        "rss": "https://hypebeast.com/fashion/feed",
        "list_url": "https://hypebeast.com/fashion",
        "base_url": "https://hypebeast.com",
    },
    {
        "name": "Highsnobiety",
        "rss": "https://www.highsnobiety.com/feed/",
        "list_url": "https://www.highsnobiety.com/page/1/",
        "base_url": "https://www.highsnobiety.com",
    },
    {
        "name": "SneakerNews",
        "rss": "https://sneakernews.com/feed/",
        "list_url": "https://sneakernews.com/",
        "base_url": "https://sneakernews.com",
    },
    # –ú–æ–∂–Ω–æ –ª–µ–≥–∫–æ –¥–æ–±–∞–≤–∏—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∑–¥–µ—Å—å
]

# –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è (—Å—Ç–∏–ª—å 1)
PRIORITY_KEYWORDS = [
    "collaboration", "release", "limited", "exclusive", "new", "collection", "drop",
    "launch", "announce", "available", "first", "special", "edition", "capsule",
    "sneaker", "runway", "fashion week", "fw", "ss", "show", "creative director",
]

# –§–∞–π–ª —Å–æ —Å–ø–∏—Å–∫–æ–º –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö —Ö—ç—à–µ–π (–±—É–¥–µ—Ç –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ –∏ –∫–æ–º–º–∏—Ç–∏—Ç—Å—è –æ–±—Ä–∞—Ç–Ω–æ)
SENT_FILE = "sent.json"

# HTTP —Å–µ—Å—Å–∏—è
SESSION = requests.Session()
SESSION.headers.update({
    "User-Agent": "Mozilla/5.0 (compatible; FashionNewsBot/1.0; +https://github.com/)"
})


# --------- –£—Ç–∏–ª–∏—Ç—ã ---------
def load_sent():
    try:
        if os.path.exists(SENT_FILE):
            with open(SENT_FILE, "r", encoding="utf-8") as f:
                data = json.load(f)
                if isinstance(data, dict) and "sent" in data:
                    return set(data["sent"])
        return set()
    except Exception as e:
        logger.warning("Failed load sent.json: %s", e)
        return set()


def save_sent(sent_set):
    try:
        data = {"sent": sorted(list(sent_set))}
        with open(SENT_FILE, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error("Failed to write sent.json: %s", e)


def hash_url(url):
    return hashlib.sha256(url.encode("utf-8")).hexdigest()


def short_text(text, limit=600):
    if not text:
        return ""
    text = " ".join(text.split())
    return text if len(text) <= limit else text[:limit].rsplit(" ", 1)[0] + "..."


# --------- –ü–∞—Ä—Å–∏–Ω–≥: RSS –∏ HTML list pages ---------
def fetch_rss_items(rss_url, source_name, max_items=10):
    items = []
    try:
        feed = feedparser.parse(rss_url)
        for entry in feed.entries[:max_items]:
            title = getattr(entry, "title", "")
            link = getattr(entry, "link", "")
            summary = getattr(entry, "summary", "") or getattr(entry, "description", "")
            published = getattr(entry, "published", "") or getattr(entry, "updated", "")
            if link:
                items.append({
                    "title": title,
                    "url": link,
                    "summary": summary,
                    "source": source_name,
                    "published": published
                })
    except Exception as e:
        logger.debug("RSS fetch failed for %s: %s", rss_url, e)
    return items


def fetch_html_list(list_url, base_url, source_name, max_items=12):
    """–î–µ–ª–∞–µ—Ç –ª—ë–≥–∫–∏–π –ø–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã-—Å–ø–∏—Å–∫–∞ —Å—Ç–∞—Ç–µ–π, –±–µ—Ä—ë—Ç —Å—Å—ã–ª–∫–∏ –∏ –∑–∞–≥–æ–ª–æ–≤–∫–∏.
       –î–µ–ª–∞–µ–º –æ–±—â–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º: –±–µ—Ä–µ–º –≤—Å–µ <a> —Å href, —Ñ–∏–ª—å—Ç—Ä—É–µ–º –Ω–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ.
    """
    items = []
    try:
        r = SESSION.get(list_url, timeout=12)
        r.raise_for_status()
        soup = BeautifulSoup(r.content, "html.parser")
        anchors = soup.find_all("a", href=True)
        seen = set()
        for a in anchors:
            href = a["href"]
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º
            if href.startswith("/"):
                href = urljoin(base_url, href)
            if not href.startswith("http"):
                continue
            # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ: —Ç–æ–ª—å–∫–æ –≤–Ω—É—Ç—Ä–∏ –¥–æ–º–µ–Ω–∞ base_url
            if urlparse(href).netloc not in (urlparse(base_url).netloc,):
                continue
            if href in seen:
                continue
            seen.add(href)
            title = (a.get_text() or "").strip()
            if not title:
                # –∏–Ω–æ–≥–¥–∞ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –≤ img alt
                img = a.find("img", alt=True)
                title = img["alt"].strip() if img else ""
            if not title:
                continue
            items.append({
                "title": title,
                "url": href,
                "summary": "",
                "source": source_name,
                "published": ""
            })
            if len(items) >= max_items:
                break
    except Exception as e:
        logger.debug("HTML list fetch failed for %s: %s", list_url, e)
    return items


# --------- –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Å—Ç–∞—Ç—å–∏ (—É–º–µ—Ä–µ–Ω–Ω—ã–π HTML-–ø–∞—Ä—Å–∏–Ω–≥) ---------
def extract_article_content(url, base_url):
    try:
        r = SESSION.get(url, timeout=12)
        r.raise_for_status()
        soup = BeautifulSoup(r.content, "html.parser")

        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–µ–µ
        for tag in soup(["script", "style", "nav", "footer", "aside", "form", "noscript"]):
            tag.decompose()

        # –ü–æ–ø—ã—Ç–∫–∏ –Ω–∞–π—Ç–∏ –æ—Å–Ω–æ–≤–Ω–æ–π –±–ª–æ–∫: –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—ë–Ω–Ω—ã—Ö —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–≤
        selectors = [
            "article",
            ".post-content",
            ".entry-content",
            ".article-content",
            ".post-body",
            ".content"
        ]
        main = None
        for sel in selectors:
            main = soup.select_one(sel)
            if main and len(main.get_text(strip=True)) > 150:
                break
        if not main:
            main = soup.find("body")

        text = short_text(main.get_text(separator=" ", strip=True), limit=800)

        # –ö–∞—Ä—Ç–∏–Ω–∫–∏: –∏—â–µ–º –±–æ–ª—å—à–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ —Å—Ç–∞—Ç—å–µ
        images = []
        # –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
        img_selectors = [
            "figure img",
            "img.featured",
            ".featured-image img",
            ".article-image img",
            ".post-image img",
            ".hero img",
            "img"
        ]
        for sel in img_selectors:
            for img in main.select(sel):
                src = img.get("data-src") or img.get("src") or img.get("data-lazy-src")
                if not src:
                    continue
                if src.startswith("//"):
                    src = "https:" + src
                if src.startswith("/"):
                    src = urljoin(base_url, src)
                if any(x in src.lower() for x in ("logo", "icon", "sprite", "thumb")):
                    continue
                if src not in images:
                    images.append(src)
                if len(images) >= 3:
                    break
            if images:
                break

        return text, images[:3]
    except Exception as e:
        logger.debug("Failed to extract article %s: %s", url, e)
        return "", []


# --------- –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è (—Å—Ç–∏–ª—å 1) ---------
def score_item(item):
    title = (item.get("title") or "").lower()
    summary = (item.get("summary") or "").lower()
    text = f"{title} {summary}"

    score = 0
    # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –ø–æ–≤—ã—à–∞—é—Ç –æ—Ü–µ–Ω–∫—É
    for kw in PRIORITY_KEYWORDS:
        if kw in text:
            score += 5
    # –∫–æ—Ä–æ—Ç–∫–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ —á–∞—Å—Ç–æ –≤–∞–∂–Ω–µ–µ
    if len(title.split()) <= 8:
        score += 1
    # —Å–ª–æ–≤–∞ "exclusive" / "limited" –¥–∞—é—Ç –±–æ–Ω—É—Å
    if "exclusive" in text or "limited" in text:
        score += 3
    # –±—Ä–µ–Ω–¥—ã (–ø—Ä–∏–º–µ—Ä–Ω—ã–π –Ω–∞–±–æ—Ä)
    for brand in ("nike", "adidas", "gucci", "supreme", "jordan", "balenciaga", "prada"):
        if brand in text:
            score += 2
    return score


# --------- –ü–µ—Ä–µ–≤–æ–¥ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ —á–µ—Ä–µ–∑ DeepL) ---------
def deepl_translate(text, target_lang="RU"):
    if not DEEPL_KEY:
        return text
    try:
        resp = requests.post(
            "https://api-free.deepl.com/v2/translate",
            data={"auth_key": DEEPL_KEY, "text": text, "target_lang": target_lang}
        )
        resp.raise_for_status()
        j = resp.json()
        if "translations" in j and len(j["translations"]) > 0:
            return j["translations"][0].get("text", text)
    except Exception as e:
        logger.warning("DeepL translation failed: %s", e)
    return text


# --------- Telegram publish ---------
class TelegramPublisher:
    def __init__(self, token, channel):
        self.token = token
        self.channel = channel
        self.base = f"https://api.telegram.org/bot{self.token}"

    def send_message(self, text, disable_preview=False):
        url = f"{self.base}/sendMessage"
        payload = {
            "chat_id": self.channel,
            "text": text,
            "parse_mode": "HTML",
            "disable_web_page_preview": disable_preview
        }
        try:
            r = requests.post(url, json=payload, timeout=25)
            r.raise_for_status()
            return True
        except Exception as e:
            logger.error("Telegram send_message failed: %s", e)
            return False

    def send_photos_group(self, caption, photos):
        # –ï—Å–ª–∏ –Ω–µ—Ç —Ñ–æ—Ç–æ, –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –∫–∞–∫ —Ç–µ–∫—Å—Ç
        if not photos:
            return self.send_message(caption, disable_preview=False)

        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –ø–µ—Ä–≤—É—é —Ñ–æ—Ç–æ —Å –ø–æ–¥–ø–∏—Å—å—é
        first = photos[0]
        try:
            resp = requests.get(first, timeout=15)
            resp.raise_for_status()
            files = {"photo": ("image.jpg", resp.content, "image/jpeg")}
            data = {"chat_id": self.channel, "caption": caption, "parse_mode": "HTML"}
            send_url = f"{self.base}/sendPhoto"
            r = requests.post(send_url, files=files, data=data, timeout=30)
            r.raise_for_status()
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ –º–∞–ª–µ–Ω—å–∫–∏–º–∏ —Ñ–æ—Ç–æ (–µ—Å–ª–∏ –µ—Å—Ç—å)
            for p in photos[1:3]:
                try:
                    r2 = requests.get(p, timeout=12)
                    r2.raise_for_status()
                    files = {"photo": ("image.jpg", r2.content, "image/jpeg")}
                    data = {"chat_id": self.channel}
                    requests.post(send_url, files=files, data=data, timeout=25)
                    time.sleep(1)
                except Exception:
                    continue
            return True
        except Exception as e:
            logger.error("Failed to send photos: %s", e)
            return self.send_message(caption)


# --------- –û—Å–Ω–æ–≤–Ω–æ–π –ø–æ—Ç–æ–∫ ---------
def collect_candidates():
    candidates = []
    for src in SOURCES:
        name = src["name"]
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º RSS
        if src.get("rss"):
            ritems = fetch_rss_items(src["rss"], name, max_items=12)
            if ritems:
                for it in ritems:
                    it["base_url"] = src.get("base_url")
                candidates.extend(ritems)
                continue  # RSS –¥–∞–ª —Å–ø–∏—Å–æ–∫ ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
        # RSS –ø—É—Å—Ç –∏–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç ‚Äî –ø–∞—Ä—Å–∏–º –ª–∏—Å—Ç–∏–Ω–≥
        lit = fetch_html_list(src["list_url"], src["base_url"], name, max_items=12)
        candidates.extend(lit)
    return candidates


def pick_best(candidates, sent_set, max_count=MAX_SEND):
    # –û—Ü–µ–Ω–∏–≤–∞–µ–º –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ score –∏ –ø–æ source priority (–ø–æ—Ä—è–¥–æ–∫ –≤ SOURCES)
    source_priority = {s["name"]: i for i, s in enumerate(SOURCES)}
    for c in candidates:
        c["_score"] = score_item(c)
        c["_priority"] = source_priority.get(c.get("source"), 99)
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º: 1) –ø–æ score desc, 2) –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–∞ asc, 3) –ø–æ —Å–≤–µ–∂–µ—Å—Ç–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)
    candidates.sort(key=lambda x: (-x["_score"], x["_priority"]))
    selected = []
    used_event_signatures = set()  # –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–π —Å—Ö–æ–∂–∏—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
    for c in candidates:
        if len(selected) >= max_count:
            break
        url = c.get("url")
        if not url:
            continue
        h = hash_url(url)
        if h in sent_set:
            continue
        # event signature: –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ + –ø–µ—Ä–≤—ã–µ 20 —Å–∏–º–≤–æ–ª–æ–≤ URL path
        title_sig = (c.get("title") or "").lower().strip()
        path = urlparse(url).path
        sig = title_sig[:80] + "|" + path[:40]
        if any(title_sig in s or s in title_sig for s in used_event_signatures):
            # –ø–æ—Ö–æ–∂–∞—è –Ω–æ–≤–æ—Å—Ç—å —É–∂–µ –≤—ã–±—Ä–∞–Ω–∞ ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –¥—É–±–ª—è
            continue
        used_event_signatures.add(title_sig)
        selected.append(c)
    return selected


def build_post(item):
    title = item.get("title") or ""
    source = item.get("source") or ""
    url = item.get("url") or ""
    base_url = item.get("base_url") or urlparse(url).scheme + "://" + urlparse(url).netloc
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –∏ –∫–∞—Ä—Ç–∏–Ω–∫–∏ (—É–º–µ—Ä–µ–Ω–Ω–æ)
    content, images = extract_article_content(url, base_url)
    if not content:
        content = item.get("summary") or ""

    # –ü–µ—Ä–µ–≤–æ–¥ (–µ—Å–ª–∏ –µ—Å—Ç—å –∫–ª—é—á)
    if DEEPL_KEY:
        title_ru = deepl_translate(title, target_lang="RU")
        content_ru = deepl_translate(content, target_lang="RU")
    else:
        title_ru = title
        content_ru = content

    # –°–æ–∑–¥–∞—ë–º HTML-–ø–æ–¥–ø–∏—Å—å
    excerpt = short_text(content_ru or content, limit=600)
    post = f"<b>{title_ru}</b>\n\n{excerpt}\n\nüì∞ –ò—Å—Ç–æ—á–Ω–∏–∫: {source}\nüîó {url}"

    return post, images


def commit_sent_file_if_changed():
    """–≠—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç –Ω–µ –∫–æ–º–º–∏—Ç–∏—Ç ‚Äî –∫–æ–º–º–∏—Ç –¥–µ–ª–∞–µ—Ç—Å—è –≤ workflow –ø–æ—Å–ª–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è.
       –ù–æ –æ—Å—Ç–∞–≤–ª—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é –∫–∞–∫ –∑–∞–≥–ª—É—à–∫—É (–º–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å)."""
    pass


def main():
    if not BOT_TOKEN or not CHANNEL:
        logger.error("BOT_TOKEN and CHANNEL environment variables must be set.")
        return

    logger.info("Starting collector")
    sent = load_sent()
    candidates = collect_candidates()
    logger.info("Candidates collected: %d", len(candidates))

    selected = pick_best(candidates, sent, max_count=MAX_SEND)
    logger.info("Selected to send: %d", len(selected))

    publisher = TelegramPublisher(BOT_TOKEN, CHANNEL)
    sent_now = set()

    for item in selected:
        try:
            post, images = build_post(item)
            ok = publisher.send_photos_group(post, images)
            if ok:
                logger.info("Published: %s", item.get("title"))
                sent_now.add(hash_url(item.get("url")))
                # –Ω–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –ø—É–±–ª–∏–∫–∞—Ü–∏—è–º–∏
                time.sleep(2)
            else:
                logger.error("Failed publishing: %s", item.get("title"))
        except Exception as e:
            logger.exception("Error processing item: %s", e)

    if sent_now:
        logger.info("Saving sent list (%d new)", len(sent_now))
        sent.update(sent_now)
        save_sent(sent)
    else:
        logger.info("No new items sent")

    logger.info("Done.")


if __name__ == "__main__":
    main()
